{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f97bd10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV files merged and saved as total1-0925.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_csvs_in_directory(directory_path, output_filename):\n",
    "    # 初始化一个空的DataFrame，用于存储所有的数据\n",
    "    df_total = pd.DataFrame()\n",
    "    \n",
    "    # 列出目录中的所有子文件夹\n",
    "    subdirectories = [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n",
    "    \n",
    "    # 遍历每个子文件夹\n",
    "    for subdirectory in subdirectories:\n",
    "        subdirectory_path = os.path.join(directory_path, subdirectory)\n",
    "        \n",
    "        # 列出子文件夹中的所有CSV文件\n",
    "        files = [f for f in os.listdir(subdirectory_path) if f.endswith('.csv')]\n",
    "        \n",
    "        # 确保至少有一个CSV文件\n",
    "        if not files:\n",
    "            print(f\"No CSV files found in the subdirectory: {subdirectory}\")\n",
    "            continue\n",
    "        \n",
    "        # 读取并合并子文件夹中的所有CSV文件\n",
    "        for file in files:\n",
    "            df = pd.read_csv(os.path.join(subdirectory_path, file), encoding='utf-8')\n",
    "            df_total = pd.concat([df_total, df], ignore_index=True)\n",
    "        \n",
    "    # 保存合并后的CSV文件\n",
    "    df_total.to_csv(os.path.join(directory_path, output_filename), index=False, encoding='utf-8')\n",
    "    print(f\"All CSV files merged and saved as {output_filename}.\")\n",
    "\n",
    "# 使用方法\n",
    "directory_path = '/Users/laihuiqian/Documents/weibo_0925/'  # 替换为你的文件夹路径\n",
    "output_filename = 'total1-0925.csv'\n",
    "merge_csvs_in_directory(directory_path, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "836adc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            微博id                                               微博正文 头条文章url  \\\n",
      "0      Mp30eEgpM  【2022·年度爱用物🏆彩妆篇】超长25min完整版！20大品类！100+件爱用精选！一路常...     NaN   \n",
      "1      Nixct2Ig9  《云之羽》原班造型团队🎬      🌟#虞书欣云为衫妆造还原#㊙️虞书欣本剧御用化妆师@侍慧...     NaN   \n",
      "2      Nivdh5y52  谁懂！！😭欣欣把云为衫的剧组衣服借我穿了！！！#虞书欣云为衫妆造还原##微博变美手册# #仿...     NaN   \n",
      "3      NhtIJgB2D      回家啦！饭后散步中~特别舒服的初秋上海！💨🍂#一枝南南[超话]#  [组图共6张] 原图      NaN   \n",
      "4      NhgghmWtm  9月第一天！！水逆退散！！来接好运🍀🎁————固定栏目「pr礼物」上线～评论区许愿池开放✌🏻...     NaN   \n",
      "...          ...                                                ...     ...   \n",
      "13000  MEXvCuYmC  𝙉𝙞𝙘𝙚 𝙩𝙤 𝘾 𝙮𝙤𝙪现场做实验的一天🔅#美妆生活##motd##好物分享#  [组图共...     NaN   \n",
      "13001  MCzrien8B  𝙄𝙉𝙏𝙊𝙉𝙀 𝙀𝘿𝙄𝙏𝙄𝙊𝙉_心慕与你色彩编辑部实习编辑上线💗#2023的她们##动静皆风尚...     NaN   \n",
      "13002  MBYOJ6L6n  最近天气太好啦～到处都是春日的信号🌿☁️#2023的她们##美妆生活##动静皆风尚#  [组...     NaN   \n",
      "13003  MATl5faty  啾咪！ʙʟᴜᴇ ʙʟᴜᴇ皮闪现💙今天打卡了科颜氏「超CHILL」保湿街区！超多的沉浸式体验场...     NaN   \n",
      "13004  MAj4Wr1M9  《关于相册里的囤图这件事》✎ 迟到的➌月ᴘʟᴏɢ请查收 ✓#2023的她们##日常碎片plo...     NaN   \n",
      "\n",
      "                                                 原始图片url  \\\n",
      "0                                                      无   \n",
      "1                                                      无   \n",
      "2                                                      无   \n",
      "3      http://ww4.sinaimg.cn/large/74b3fa89ly1hhimoti...   \n",
      "4      http://ww3.sinaimg.cn/large/74b3fa89gy1hhgz7oj...   \n",
      "...                                                  ...   \n",
      "13000  http://ww3.sinaimg.cn/large/dc3d6f16ly1hdm4sag...   \n",
      "13001  http://ww4.sinaimg.cn/large/dc3d6f16ly1hczgjlq...   \n",
      "13002  http://ww4.sinaimg.cn/large/dc3d6f16ly1hczgm4q...   \n",
      "13003  http://ww1.sinaimg.cn/large/dc3d6f16ly1hcr7kmt...   \n",
      "13004  http://ww4.sinaimg.cn/large/dc3d6f16ly1hcmkk0o...   \n",
      "\n",
      "                                                 微博视频url 发布位置  \\\n",
      "0      https://f.video.weibocdn.com/o0/XYtcJp3Glx082t...    无   \n",
      "1      https://f.video.weibocdn.com/o0/KlpRcnUHlx088t...    无   \n",
      "2      https://f.video.weibocdn.com/o0/ewBDq0eYlx088t...    无   \n",
      "3                                                      无    无   \n",
      "4                                                      无    无   \n",
      "...                                                  ...  ...   \n",
      "13000                                                  无    无   \n",
      "13001                                                  无    无   \n",
      "13002                                                  无    无   \n",
      "13003                                                  无   全文   \n",
      "13004                                                  无    无   \n",
      "\n",
      "                   发布时间    发布工具    点赞数   转发数   评论数  \n",
      "0      2023-01-19 18:53   微博视频号   7131   733  1834  \n",
      "1      2023-09-09 21:38   微博视频号   8188   802  1379  \n",
      "2      2023-09-09 16:35   微博视频号  39015  2717  4380  \n",
      "3      2023-09-02 22:56  一枝南南超话   2496    30   335  \n",
      "4      2023-09-01 12:40  一枝南南超话   2194   182  2427  \n",
      "...                 ...     ...    ...   ...   ...  \n",
      "13000  2023-05-04 10:19       无   8669   587  1130  \n",
      "13001  2023-04-18 17:22       无   6769   381   685  \n",
      "13002  2023-04-14 20:08       无  24709  1095  2675  \n",
      "13003  2023-04-07 16:21       无   9291   483  1341  \n",
      "13004  2023-04-03 20:02       无   6846   394  1002  \n",
      "\n",
      "[13005 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 指定文件路径\n",
    "file_path = '/Users/laihuiqian/Documents/weibo_0925/total1-0925.csv'\n",
    "\n",
    "# 读取.csv文件到DataFrame\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# 打印DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06b75e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        【2022·年度爱用物🏆彩妆篇】超长25min完整版！20大品类！100+件爱用精选！一路常...\n",
      "1        《云之羽》原班造型团队🎬      🌟#虞书欣云为衫妆造还原#㊙️虞书欣本剧御用化妆师@侍慧...\n",
      "2        谁懂！！😭欣欣把云为衫的剧组衣服借我穿了！！！#虞书欣云为衫妆造还原##微博变美手册# #仿...\n",
      "3            回家啦！饭后散步中~特别舒服的初秋上海！💨🍂#一枝南南[超话]#  [组图共6张] 原图 \n",
      "4        9月第一天！！水逆退散！！来接好运🍀🎁————固定栏目「pr礼物」上线～评论区许愿池开放✌🏻...\n",
      "                               ...                        \n",
      "13000    𝙉𝙞𝙘𝙚 𝙩𝙤 𝘾 𝙮𝙤𝙪现场做实验的一天🔅#美妆生活##motd##好物分享#  [组图共...\n",
      "13001    𝙄𝙉𝙏𝙊𝙉𝙀 𝙀𝘿𝙄𝙏𝙄𝙊𝙉_心慕与你色彩编辑部实习编辑上线💗#2023的她们##动静皆风尚...\n",
      "13002    最近天气太好啦～到处都是春日的信号🌿☁️#2023的她们##美妆生活##动静皆风尚#  [组...\n",
      "13003    啾咪！ʙʟᴜᴇ ʙʟᴜᴇ皮闪现💙今天打卡了科颜氏「超CHILL」保湿街区！超多的沉浸式体验场...\n",
      "13004    《关于相册里的囤图这件事》✎ 迟到的➌月ᴘʟᴏɢ请查收 ✓#2023的她们##日常碎片plo...\n",
      "Name: 微博正文, Length: 13005, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 指定文件路径\n",
    "file_path = '/Users/laihuiqian/Documents/weibo_0925/total1-0925.csv'\n",
    "\n",
    "# 读取.csv文件到DataFrame\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# 提取“微博正文”这一列\n",
    "weibo_content = df[\"微博正文\"]\n",
    "\n",
    "# 打印“微博正文”\n",
    "print(weibo_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d247ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# 周小仙yoo的微博视频', '# 周小仙yoo的微博', '#周小仙yoo的微博视频', '#周小仙yoo的微博', '周小仙yoo', '# 张进ZJZJ的微博视频', '# 张进ZJZJ的微博', '#张进ZJZJ的微博视频', '#张进ZJZJ的微博', '张进ZJZJ', '# 陈大皮儿的微博视频', '# 陈大皮儿的微博', '#陈大皮儿的微博视频', '#陈大皮儿的微博', '陈大皮儿', '# 马锐的微博视频', '# 马锐的微博', '#马锐的微博视频', '#马锐的微博', '马锐', '# Kiko成晨的微博视频', '# Kiko成晨的微博', '#Kiko成晨的微博视频', '#Kiko成晨的微博', 'Kiko成晨', '# 彤彤_sakura的微博视频', '# 彤彤_sakura的微博', '#彤彤_sakura的微博视频', '#彤彤_sakura的微博', '彤彤_sakura', '# 手边巴黎urruolan的微博视频', '# 手边巴黎urruolan的微博', '#手边巴黎urruolan的微博视频', '#手边巴黎urruolan的微博', '手边巴黎urruolan', '# 叫我大表哥好吗好的的微博视频', '# 叫我大表哥好吗好的的微博', '#叫我大表哥好吗好的的微博视频', '#叫我大表哥好吗好的的微博', '叫我大表哥好吗好的', '# 牛明昱的微博视频', '# 牛明昱的微博', '#牛明昱的微博视频', '#牛明昱的微博', '牛明昱', '# LookLana的微博视频', '# LookLana的微博', '#LookLana的微博视频', '#LookLana的微博', 'LookLana', '# 孙一玮Well的微博视频', '# 孙一玮Well的微博', '#孙一玮Well的微博视频', '#孙一玮Well的微博', '孙一玮Well', '# Hedy北北的微博视频', '# Hedy北北的微博', '#Hedy北北的微博视频', '#Hedy北北的微博', 'Hedy北北', '# Ruby幼熙的微博视频', '# Ruby幼熙的微博', '#Ruby幼熙的微博视频', '#Ruby幼熙的微博', 'Ruby幼熙', '# 蒲一雯iwen的微博视频', '# 蒲一雯iwen的微博', '#蒲一雯iwen的微博视频', '#蒲一雯iwen的微博', '蒲一雯iwen', '# 美少女毛容易的微博视频', '# 美少女毛容易的微博', '#美少女毛容易的微博视频', '#美少女毛容易的微博', '美少女毛容易', '# Joey土播鼠的微博视频', '# Joey土播鼠的微博', '#Joey土播鼠的微博视频', '#Joey土播鼠的微博', 'Joey土播鼠', '# 章解放_的微博视频', '# 章解放_的微博', '#章解放_的微博视频', '#章解放_的微博', '章解放_', '# 一枝南南的微博视频', '# 一枝南南的微博', '#一枝南南的微博视频', '#一枝南南的微博', '一枝南南', '# 潘白雪s的微博视频', '# 潘白雪s的微博', '#潘白雪s的微博视频', '#潘白雪s的微博', '潘白雪s', '# 一番fane的微博视频', '# 一番fane的微博', '#一番fane的微博视频', '#一番fane的微博', '一番fane', '# 美少女Lisa酱的微博视频', '# 美少女Lisa酱的微博', '#美少女Lisa酱的微博视频', '#美少女Lisa酱的微博', '美少女Lisa酱', '# 潘南竹呀的微博视频', '# 潘南竹呀的微博', '#潘南竹呀的微博视频', '#潘南竹呀的微博', '潘南竹呀', '# 小考拉Lake的微博视频', '# 小考拉Lake的微博', '#小考拉Lake的微博视频', '#小考拉Lake的微博', '小考拉Lake', '# 周姊伊的微博视频', '# 周姊伊的微博', '#周姊伊的微博视频', '#周姊伊的微博', '周姊伊', '# 种草达人绵绵酱的微博视频', '# 种草达人绵绵酱的微博', '#种草达人绵绵酱的微博视频', '#种草达人绵绵酱的微博', '种草达人绵绵酱', '# Yuki_优酱的微博视频', '# Yuki_优酱的微博', '#Yuki_优酱的微博视频', '#Yuki_优酱的微博', 'Yuki_优酱', '# 兰阿雯的微博视频', '# 兰阿雯的微博', '#兰阿雯的微博视频', '#兰阿雯的微博', '兰阿雯', '# 优莉Uli的微博视频', '# 优莉Uli的微博', '#优莉Uli的微博视频', '#优莉Uli的微博', '优莉Uli', '# Echo桃小小的微博视频', '# Echo桃小小的微博', '#Echo桃小小的微博视频', '#Echo桃小小的微博', 'Echo桃小小', '# 呦呦仔的微博视频', '# 呦呦仔的微博', '#呦呦仔的微博视频', '#呦呦仔的微博', '呦呦仔', '# 宋素雯的微博视频', '# 宋素雯的微博', '#宋素雯的微博视频', '#宋素雯的微博', '宋素雯', '# 月野皮皮的微博视频', '# 月野皮皮的微博', '#月野皮皮的微博视频', '#月野皮皮的微博', '月野皮皮', '# 美硕的成分测评的微博视频', '# 美硕的成分测评的微博', '#美硕的成分测评的微博视频', '#美硕的成分测评的微博', '美硕的成分测评', '# 桃子百莉的微博视频', '# 桃子百莉的微博', '#桃子百莉的微博视频', '#桃子百莉的微博', '桃子百莉', '# 苍口小梨涡的微博视频', '# 苍口小梨涡的微博', '#苍口小梨涡的微博视频', '#苍口小梨涡的微博', '苍口小梨涡', '# Fairy小默的微博视频', '# Fairy小默的微博', '#Fairy小默的微博视频', '#Fairy小默的微博', 'Fairy小默', '# 莓灵酱的微博视频', '# 莓灵酱的微博', '#莓灵酱的微博视频', '#莓灵酱的微博', '莓灵酱', '# 你Rui哥的微博视频', '# 你Rui哥的微博', '#你Rui哥的微博视频', '#你Rui哥的微博', '你Rui哥', '# 刘魔王大人-的微博视频', '# 刘魔王大人-的微博', '#刘魔王大人-的微博视频', '#刘魔王大人-的微博', '刘魔王大人-', '# 蟹阿文AWEN的微博视频', '# 蟹阿文AWEN的微博', '#蟹阿文AWEN的微博视频', '#蟹阿文AWEN的微博', '蟹阿文AWEN', '# 陈端端儿的微博视频', '# 陈端端儿的微博', '#陈端端儿的微博视频', '#陈端端儿的微博', '陈端端儿', '# 迟池Chichi的微博视频', '# 迟池Chichi的微博', '#迟池Chichi的微博视频', '#迟池Chichi的微博', '迟池Chichi', '# 叫我桃maymay的微博视频', '# 叫我桃maymay的微博', '#叫我桃maymay的微博视频', '#叫我桃maymay的微博', '叫我桃maymay', '# 种草颜究生的微博视频', '# 种草颜究生的微博', '#种草颜究生的微博视频', '#种草颜究生的微博', '种草颜究生']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "blogger_names = [\n",
    "\"周小仙yoo\", \"张进ZJZJ\", \"陈大皮儿\", \"马锐\", \n",
    "\"Kiko成晨\", \"彤彤_sakura\",\"手边巴黎urruolan\", \"叫我大表哥好吗好的\", \n",
    "\"牛明昱\", \"LookLana\",\"孙一玮Well\", \"Hedy北北\", \n",
    "\"Ruby幼熙\",\"蒲一雯iwen\", \"美少女毛容易\", \"Joey土播鼠\", \n",
    "\"章解放_\", \"一枝南南\", \"潘白雪s\",\"一番fane\",\n",
    "\"美少女Lisa酱\",\"潘南竹呀\",\"小考拉Lake\", \"周姊伊\",\n",
    "\"种草达人绵绵酱\", \"Yuki_优酱\", \"兰阿雯\", \"优莉Uli\",\n",
    "\"Echo桃小小\", \"呦呦仔\", \"宋素雯\", \"月野皮皮\", \n",
    "\"美硕的成分测评\", \"桃子百莉\",\"苍口小梨涡\", \"Fairy小默\", \n",
    "\"莓灵酱\", \"你Rui哥\", \"刘魔王大人-\", \"蟹阿文AWEN\",\n",
    "\"陈端端儿\", \"迟池Chichi\", \"叫我桃maymay\", \"种草颜究生\"\n",
    "]\n",
    "\n",
    "self_references = []\n",
    "\n",
    "for name in blogger_names:\n",
    "    reference1 = f\"# {name}的微博视频\"\n",
    "    reference2 = f\"# {name}的微博\"\n",
    "    reference3 = f\"#{name}的微博视频\"\n",
    "    reference4 = f\"#{name}的微博\"\n",
    "    reference5 = name\n",
    "    self_references.append(reference1)\n",
    "    self_references.append(reference2)\n",
    "    self_references.append(reference3)\n",
    "    self_references.append(reference4)\n",
    "    self_references.append(reference5)\n",
    "\n",
    "blogger_names_variants = [\n",
    "    \"周小仙yoo\", \"周小仙\", \"yoo\",\n",
    "    \"张进ZJZJ\", \"张进\", \"ZJZJ\", \"zjzj\",\n",
    "    \"陈大皮儿\",\"陈大皮\"\n",
    "    \"马锐\",\n",
    "    \n",
    "    \"Kiko成晨\", \"Kiko\", \"成晨\",\n",
    "    \"彤彤_sakura\", \"彤彤\", \"sakura\",\n",
    "    \"手边巴黎urruolan\", \"手边巴黎\", \"urruolan\",\n",
    "    \"叫我大表哥好吗好的\",\n",
    "    \n",
    "    \"牛明昱\",\n",
    "    \"LookLana\",\n",
    "    \"孙一玮Well\", \"孙一玮\", \"Well\",\"well\",\n",
    "    \"Hedy北北\", \"Hedy\", \"北北\",\"hedy\",\n",
    "    \n",
    "    \"Ruby幼熙\", \"Ruby\", \"幼熙\",\"ruby\",\n",
    "    \"蒲一雯iwen\", \"蒲一雯\", \"iwen\",\n",
    "    \"美少女毛容易\", \"毛容易\",\n",
    "    \"Joey土播鼠\", \"Joey\",\"joey\",\n",
    "    \n",
    "    \"章解放_\", \"章解放\",\n",
    "    \"一枝南南\", \"南南\", \"一枝\",\n",
    "    \"潘白雪s\", \"潘白雪\",\n",
    "    \"一番fane\",\"fane\",\"一番\",\n",
    "    \n",
    "    \"美少女Lisa酱\",\"Lisa酱\",\n",
    "    \"潘南竹呀\",\"潘南竹\",\"南竹\",\n",
    "    \"小考拉Lake\", \"小考拉\", \"Lake\",\"lake\",\n",
    "    \"周姊伊\",\n",
    "    \n",
    "    \"种草达人绵绵酱\", \"绵绵酱\",\n",
    "    \"Yuki_优酱\", \"Yuki\", \"优酱\",\n",
    "    \"兰阿雯\",\n",
    "    \"优莉Uli\", \"优莉\", \"Uli\",\"uli\",\n",
    "    \n",
    "    \"Echo桃小小\", \"Echo\", \"桃小小\",\"echo\",\n",
    "    \"呦呦仔\",\n",
    "    \"宋素雯\",\n",
    "    \"月野皮皮\", \"皮皮\", \"月野\",\n",
    "    \n",
    "    \"美硕的成分测评\", \"美硕\",\n",
    "    \"桃子百莉\", \"百莉\",\n",
    "    \"苍口小梨涡\",\n",
    "    \"Fairy小默\", \"Fairy\", \"小默\",\"fairy\",\n",
    "    \n",
    "    \"莓灵酱\",\n",
    "    \"你Rui哥\",\n",
    "    \"刘魔王大人-\",\n",
    "    \"蟹阿文AWEN\", \"蟹阿文\", \"AWEN\",\"awen\",\n",
    "    \n",
    "    \"陈端端儿\",\n",
    "    \"迟池Chichi\", \"迟池\", \"Chichi\",\"chi\",\"Chi\",\n",
    "    \"叫我桃maymay\", \"maymay\", \"may\",\"May\",\n",
    "    \"种草颜究生\"\n",
    "]\n",
    "\n",
    "\n",
    "print(self_references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a7dc29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#潘白雪s的微博', 'Well', '#孙一玮Well的微博视频', '# 你Rui哥的微博视频', '南南', '# LookLana的微博视频', '# Kiko成晨的微博视频', '彤彤_sakura', '#潘白雪s的微博视频', '#美硕的成分测评的微博', '南竹', 'Uli', '#周姊伊的微博视频', '一番fane', '桃子百莉', '#Kiko成晨的微博', '#Hedy北北的微博', '#种草颜究生的微博', '# 一番fane的微博视频', '小考拉Lake', 'Hedy北北', '# 马锐的微博', '# 叫我大表哥好吗好的的微博', '#叫我桃maymay的微博', '美少女Lisa酱', 'ruby', '#蒲一雯iwen的微博', '# 一番fane的微博', 'maymay', '# 迟池Chichi的微博', '叫我大表哥好吗好的', '# 潘白雪s的微博视频', '# Echo桃小小的微博', '# 宋素雯的微博', '小默', '# 叫我桃maymay的微博', 'Lake', '美硕的成分测评', '# Yuki_优酱的微博', '#Fairy小默的微博', '# 莓灵酱的微博视频', 'well', '#章解放_的微博', '# 你Rui哥的微博', '#种草颜究生的微博视频', '# 美少女毛容易的微博视频', 'fane', '蟹阿文', '#牛明昱的微博', 'Echo桃小小', '#小考拉Lake的微博', '#月野皮皮的微博视频', '#优莉Uli的微博视频', '# 呦呦仔的微博视频', '#美硕的成分测评的微博视频', '# 陈端端儿的微博视频', '桃小小', 'May', '宋素雯', '# 美少女毛容易的微博', '# 张进ZJZJ的微博视频', '#周小仙yoo的微博视频', '张进ZJZJ', '陈大皮儿', '# 牛明昱的微博', '# Hedy北北的微博', '百莉', '#Echo桃小小的微博视频', '# 呦呦仔的微博', '#手边巴黎urruolan的微博视频', '蟹阿文AWEN', 'Lisa酱', '成晨', '# 张进ZJZJ的微博', 'ZJZJ', '#月野皮皮的微博', '#莓灵酱的微博视频', '孙一玮', '# 陈大皮儿的微博视频', '# 周姊伊的微博', '# 美硕的成分测评的微博', 'iwen', '迟池Chichi', 'Fairy', 'echo', '潘白雪s', '你Rui哥', '#小考拉Lake的微博视频', '# 苍口小梨涡的微博', '苍口小梨涡', 'zjzj', '# 彤彤_sakura的微博', '# 宋素雯的微博视频', '#种草达人绵绵酱的微博视频', '潘白雪', '#兰阿雯的微博', '# 潘南竹呀的微博', '#你Rui哥的微博视频', '# 迟池Chichi的微博视频', '蒲一雯', '#Yuki_优酱的微博', '#刘魔王大人-的微博', '皮皮', 'fairy', '#呦呦仔的微博视频', '#Hedy北北的微博视频', '#一番fane的微博视频', '# 手边巴黎urruolan的微博', '# 美少女Lisa酱的微博', '#种草达人绵绵酱的微博', 'Fairy小默', '#优莉Uli的微博', '#章解放_的微博视频', '#潘南竹呀的微博视频', '# 彤彤_sakura的微博视频', '# Hedy北北的微博视频', '# 章解放_的微博视频', '# 周姊伊的微博视频', '# Fairy小默的微博', '# 一枝南南的微博视频', 'Joey', '#叫我大表哥好吗好的的微博', '# 叫我大表哥好吗好的的微博视频', 'Hedy', '#彤彤_sakura的微博视频', 'Yuki_优酱', '# 潘南竹呀的微博视频', '#彤彤_sakura的微博', 'urruolan', '章解放', '# Ruby幼熙的微博', 'Echo', '#你Rui哥的微博', '# LookLana的微博', '# 牛明昱的微博视频', '#美少女毛容易的微博', '#陈大皮儿的微博视频', '张进', '蒲一雯iwen', '# 种草达人绵绵酱的微博', '一番', '周小仙yoo', '一枝', '# 月野皮皮的微博视频', '#牛明昱的微博视频', 'sakura', '优酱', '#Echo桃小小的微博', '# Joey土播鼠的微博', '优莉Uli', '#刘魔王大人-的微博视频', '#蟹阿文AWEN的微博', '# Fairy小默的微博视频', '# Echo桃小小的微博视频', '#苍口小梨涡的微博视频', '# 莓灵酱的微博', '北北', '# 刘魔王大人-的微博', '#周姊伊的微博', '#陈端端儿的微博', '手边巴黎', '# 孙一玮Well的微博', '#一番fane的微博', '#桃子百莉的微博', '#Ruby幼熙的微博视频', '小考拉', '# 月野皮皮的微博', '# 优莉Uli的微博', '种草颜究生', '# 周小仙yoo的微博视频', 'Kiko', '#潘南竹呀的微博', '毛容易', '马锐', 'Chichi', '#周小仙yoo的微博', '#美少女Lisa酱的微博', 'Chi', '迟池', '#Yuki_优酱的微博视频', '牛明昱', '# 优莉Uli的微博视频', '#张进ZJZJ的微博', '一枝南南', '#陈端端儿的微博视频', '# 叫我桃maymay的微博视频', '#Ruby幼熙的微博', '# 周小仙yoo的微博', '#呦呦仔的微博', '#LookLana的微博视频', '# 小考拉Lake的微博', '#桃子百莉的微博视频', 'Ruby幼熙', '#陈大皮儿的微博', '# 蟹阿文AWEN的微博视频', '# 蟹阿文AWEN的微博', '# 种草颜究生的微博视频', '彤彤', '# 苍口小梨涡的微博视频', '#Joey土播鼠的微博视频', '刘魔王大人-', '#Joey土播鼠的微博', '潘南竹呀', '#孙一玮Well的微博', '陈端端儿', '# Ruby幼熙的微博视频', '月野皮皮', '# 手边巴黎urruolan的微博视频', '# 小考拉Lake的微博视频', '# 马锐的微博视频', '#一枝南南的微博视频', '绵绵酱', 'Yuki', '# 章解放_的微博', '#手边巴黎urruolan的微博', '兰阿雯', '#张进ZJZJ的微博视频', '#迟池Chichi的微博视频', 'hedy', '#马锐的微博', '# 一枝南南的微博', '# 桃子百莉的微博视频', '莓灵酱', 'uli', 'AWEN', '潘南竹', 'lake', '# 美少女Lisa酱的微博视频', '#兰阿雯的微博视频', '叫我桃maymay', '章解放_', '#莓灵酱的微博', '#叫我桃maymay的微博视频', '幼熙', '手边巴黎urruolan', '#一枝南南的微博', '#宋素雯的微博视频', 'awen', 'Joey土播鼠', '# 兰阿雯的微博', '孙一玮Well', '# 孙一玮Well的微博视频', '# Joey土播鼠的微博视频', '周小仙', 'Kiko成晨', '# 陈大皮儿的微博', '# Yuki_优酱的微博视频', 'yoo', '# 种草达人绵绵酱的微博视频', '# 陈端端儿的微博', '#Fairy小默的微博视频', '# Kiko成晨的微博', '陈大皮马锐', 'chi', 'LookLana', '#美少女Lisa酱的微博视频', '周姊伊', '种草达人绵绵酱', '# 兰阿雯的微博视频', '# 刘魔王大人-的微博视频', '# 蒲一雯iwen的微博视频', 'joey', '#美少女毛容易的微博视频', '# 美硕的成分测评的微博视频', '呦呦仔', '# 桃子百莉的微博', '#蟹阿文AWEN的微博视频', '#宋素雯的微博', 'Ruby', '美少女毛容易', '#苍口小梨涡的微博', '月野', '#LookLana的微博', '优莉', '# 种草颜究生的微博', '# 蒲一雯iwen的微博', '#叫我大表哥好吗好的的微博视频', '#Kiko成晨的微博视频', '# 潘白雪s的微博', '美硕', 'may', '#迟池Chichi的微博', '#马锐的微博视频', '#蒲一雯iwen的微博视频']\n"
     ]
    }
   ],
   "source": [
    "combined_blogger_names = list(set(self_references + blogger_names_variants))\n",
    "print(combined_blogger_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a052ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['年度 爱用物 彩妆 篇 超长 min 完整版 品类 件 爱用 精选 一路 常绿 放心 食用 观看 指南 混干 微 瑕 适用 底妆 皮肤 炼成 不卡粉 淡颜 彩妆 心头 宝 一年 彩妆 爱 胞胎 口红 找 不同 一只 真的 纯 白开水 气质 色 变美 过程 神 辅助 化妆 刷 工具 发型 好物 护肤 篇 路上 咯 马上 南得 花钱 年度 爱 彩妆', '云之羽 原班 造型 团队 虞书 欣云 为衫妆造 还原 虞书欣 本剧 御用 化妆师 侍慧 Vicky 揭秘 衫 姐妆容 小心 机 剧组 封存 N年 戏服 原款 上身 大师级 底妆 秘籍 还原 美貌 欣欣 call 剧粉 圆梦 横店 快 一起 追云之羽 吼吼 仿妆 微博变 美 手册', '懂 欣欣 云为 衫 剧组 衣服 穿 虞书 欣云 为衫妆造 还原 微博变 美 手册 仿妆', '回家 饭后 散步 中 特别 舒服 初秋 上海', '月 第一天 水逆退 散 来接 好运 固定 栏目 pr 礼物 上线 评论 区 许愿池 开放 共享 快乐 ps 谢谢 品牌 霸霸们 爱 抽奖 详情', '出差 ootd 感受 提前 入秋', '沙漠 邂逅 玫瑰 沙漠 拍 水感 大片 OLAY 沙漠 水润 出镜 挑战 对话 七夕 纯 真 美力 定义', '露思 居家 慵懒 感 拍照 太出 片 纯享 图奉 直接 照抄 对话 七夕 纯 真 美力 定义', '菊宅 一段 美丽 慢 生活 美丽 源 计划', '南得 花钱 月 双 月 爱 用物 有人 画 仿妆 记得 韩妆 集邮者 我家 眼影 腮红 色号 Olive Young 还全 黄皮 白皮 试试 紫色 隔离 啊啊啊 敏门 弟子 夏天 面膜 精华 喷雾 好用 好看 韩妆 请 抽奖 详情', '周一 午好 收到', '首尔 购物 开箱 太 买 谁家 好人 逛街 寸 行李箱 装货 不够 戳 视频 数字 密码 馋 久 绝美 彩妆 香氛 淘到 新 衣服 piu 亮 首饰 穿戴 手艺 活 发家致富 赚 购物 南得 花钱 抽奖 详情', '半年度 工作 总结 晒 黢黑 努力 养白中 反复 横 跳 把姐 晒 晕', '或许 缺 壁纸 看看 鸿蒙 灵感 创造 官 OOTD 做成 多巴胺 壁纸 抽奖 详情', '日记 只办 一件 事 爱豆 同款 人生 四格 胶片版 拍 今日 份妆容 韩女 水光 肌 clean makeup 学成归来 南 师傅 朋友 拍照 抽奖 详情', '知道 从不 画饼 新加坡 番外 购物 特辑 环球 进货 咯 带回 一车 可爱 崽崽们 美博 天堂 药妆店 丝芙兰 库库 三筐 带回家 钱 真 花 代购 南得 花钱 抽奖 详情', '跨国 工 新加坡 sing 嘎坡 出国 差 闲暇 吃 吃 买买 吃 海鲜大餐 吃 xxxxxxl 海鲜大餐 甘蔗汁 好喝 玩 环球 影城 小心 胳膊 掐 淤青 逛 著名景点 丝芙兰 一模一样 口红 买 一筐 买 太 这期 放不下 购物 分享 抽奖 详情', '暴走 首尔', '跨国 差 彩妆 购物 分享 拆边 画 欧米茄 挖 一堆 绝 美韩 系小 彩妆 本期 预告 P 浅 春夏 必备 彩妆 显嫩 腮红 绝美 唇 釉 韩系 眼影 DIY 橘猫 气垫 P GRWM 撸 妆 韩系 血色 感妆容 生命力 淡颜 女孩 狂喜 ing 快乐 传递 屏幕 前 姐妹 南得 花钱 美出 圈 抽奖 详情', '月 第一个 跨国 工 无比 想念 国内 韩国', '粽意 粽意 端午 快乐 谁家 今天 早餐 吃 粽子 摇 位 分 点儿 东西 抽奖 详情', '属于 生日 小时 幸福 碎片 多图 快乐 感谢 一位 到来 平凡 一天 变 鲜活 温暖 下次 再见', 'WB 电影 之夜 后台 全记录 VLOG 感谢 微博霸 霸 邀请 点击 进入 第一 视角 体验 女明星 一天 恐怖 事件 奇遇 半夜 敲门声 走近 红毯 后台 遇到 老朋友 微博 电影 之夜 红毯 初体验 芜湖 盛典 观礼 带 来看 半个 娱乐圈 好像 有点 脉 收到 争取 探 报 抽奖 详情', 'Au revoir Paris 法国 巴黎', '澳门 一日 vlog 带 微博去 都市 画个 小碎 钻 con 湿热 地区 级底 妆 阿玛尼 权力 持妆 PLUS', '暴走 巴黎 街头', '月 爱 用物 赶个 尾巴 保姆 组合式 分享 洋洋洒洒 分钟 抗老 嫩肤 不谈 虚 有效 亮嫩 滑 祛痘 底妆 素颜好 皮 养成 微 油 越 美丽 爱用 单品 手 残党 好使 工具 口红 大户 进货 芭比 眼妆 自带 温柔 人设 香水 森林 中 玫瑰 味道 南得 花钱', '夏日 茶 棕 冷萃 冰 咖 微博 电影 之夜 红毯 微博 电影 之夜 冷感 大片 YSL', '补发 昨天 现场图 两位 仙人 仙女 电影 之夜 红毯 初体验 微博 电影 之夜', '柔光 莹莹 微博 电影 之夜', '快乐 收工', '高考 加油 金榜题名', '希腊 风 库存 dd', '小抄 看着办 今年 没 实感 直播 需 食用 大牌 值得 买首 瓶 卸妆油 冷门 功效 精华 近期 挖到 心头 爱 油 回购 多年 爱 用水 水乳大 全美 白 提亮 平衡 水油 抗老 紧致 补水 保湿 推荐 评论 区 抽奖 详情', '六一 快乐 冲刺 冲刺 安排 份 过个 快乐 谢谢 品牌 霸霸 抽奖 详情', '满意', '', '蚌壳 公主 图 写真 芭莎 珠宝 夜宴 第二年 第十四届 trends 高级 珠宝 年度 设计 大赏 澳门', '没见 分钟 爱 用物 精选 老登西 讲烦 来点 新 对象 无广 偏 干性 皮 友好 底妆大换 血水 光 奶油 肌 整个 秒 坠入 爱河 粉嫩 贵妇 肌 粉饼 冲妆效 姑且 忍忍 包装 叭 新晋 彩妆 宝藏 提升 骨相 粉 nun 气 血色 一个顶 俩 护肤 独苗 以油 养肤 亘古 不变 用护唇 牛波 短小精悍 更 过瘾 南得 花钱', '最近 好好看 忍不住 分享 分享 位 懂 抽奖 详情', '剑 歼灭 敌军', '放点 添添 节气 提前 祝 快乐 评论 区 位 抽奖 详情', '今天 妈妈 体验 一天 太 好玩 yznn 风格 复刻 妈 脸上 样子 做 猛药 护肤 没 得逞 换季 护肤 我妈 整活 主打 一个 h 日夜 养护 贵妇 求稳 懂 下次 海南 这套 背 回来 yznn 仿妆 首次 尝试 嫩 蜜桃 色系 亮片 灵魂 粗 眼线 恶魔 睫毛 哈哈哈 发型 造型 温柔 卷发 香风 母女 装 永远 珍珠 真 有点 日夜 双修 倍效 驻龄 赫莲娜 白 绷带 赫莲娜 亚太 免税', '过期 日常', '超级 红人节 幕后 红毯 黑色 挂 脖 礼服 优雅 珍珠 礼服 里面 穿 夏日 清凉 穿 搭 内衣 夜间 护肤 重要 场合 前 眼部 淡纹 tips 领奖 满载 星光 初心 不改 姐妹 见面会 一年一度 线下 团建 南 朋友 年 故事 未完待续 超级 红人节 A 醇 大佬 资生堂 针管 抽奖 详情', '开心 超级 红人节 见面会 超级 红人节', '超级 红人节', '今天 美丽 瞬间 红人节 最辣 姐 高清 镜头 红人妆 超级 红人节', '风来 听风 雨来 听雨 超级 红人节 红毯 高清 镜头 红人妆 红人节 最辣 姐', '报告 到达 传递 前两天 快乐', '', '芭蕾 风 韩系 DOLL 妆教 完整版 妆面 摘要 毛绒绒 下垂 式 眉毛 营造 无辜 感 DOLL 妆 灵魂 网感粉 紫色 系眼妆 哑光 截断 恶魔 睫毛 眼头 bngbng 提亮 色彩 平衡 淡化 腮红 唇部 色彩 太美 下次 还画 日 抛 脸 抽奖 详情', '想 Roomtour 一隅 生活区 爽 漂亮 东西 运动 角 身材 管理 秘诀 动感单车 拉伸 肌肉 家用 负重 工具 工作 间 tour 衣帽间 首次 公开 带 简单 搂 一眼 老铁 终于', 'Nan s Daliy PLOG 复古 相机 味儿 夏日 碎片', '第一季度 爱用物 完整版 新晋 电子 榨菜 分钟 管够 含 季度 爱 彩妆 护肤 捂紧 钱包 请 省流版 见 粉饼 走不动 路 毛病 犯 主打 一个 普照 黄 提亮 匀肤个 顶个 淡颜 韩妆而生 化妆 刷 标杆 一句 老话 提升 妆容 精致 度 混干 功效 稳定 发力 众 抗老 乳液 果酸 宗师 镇牌 之作 即将 停产 美白 身体 乳 哈喇子 讲 没 下次 一定 一月 一出 南得 花钱', '有福 同箱 会 做 产品 还会 搞 周边 品牌 霸霸们 壕 礼 一起 分享 键入 池 许愿池 开', '一个脚印', '电子 榨菜 加量 版 暴拆 女神 节 开箱 采购 明细 啵 啵 间 买 and 忘 买 买 忘 真的 会无语 新衣 新包 新 配饰 知道 爱看 漂亮 无罪 本颜值 控对底 妆 包容 度 爱用物 预定 来种 新草 下次 买 南得 花钱 抽奖 详情', '各国 校园 女二 系列 No 美式 混血 辣妹 妆 完整版 妆教 安排 妆容 要义 截断 向下 纹路 凸显 骨相 五官 立体 加粗 增强 原生 眉 流感 深 眼窝 圆钝 鼻 截断 巧 造 外双 浓密 睫毛 混血 美瞳 重点 扩唇 放大 倍 加强 丰满度 日 抛 脸 抽奖 详情', '超级 红人节 长沙 见 诚邀 长沙 茶 颜悦色 黑色 经典 费 大厨 小炒 黄 牛肉 激情 面基 月 号 下午 不见不散 超红 VLOG 现场 超级 红人节 想来 戳 超级 红人节', 'Nan s Daliy PLOG', '固定 栏目 pr 共享 本月 开箱 云 分享 评论 区 位 抽奖 详情', '湿 发冷 欲 烟熏 妆 姐学 重 五官 刻画 粗 眉毛 压住 烟熏 碎 钻 眼影 打底 干净 成功 一半 强调 轮廓 感 全包 眼线 放射状 睫毛 细小 闪点 冷欲 牛奶 色 腮红 眼下 提亮 姐味 开麦 不下 日 抛 脸 抽奖 详情', '', '这波 芭蕾 疯 沾 春光 正好 芭蕾 少女 妆 完整版 妆 教来 咯 中庭 偏长 姐妹 快来试 nun 妆 春姑娘 成型 基础 V A 字 仙子 毛 强调 睫毛 毛 流感 蜜桃 粉叠 浅粉色 腮红 太乖 眼影 睫毛 中 庭长 显嫩 必备 日 抛 脸 抽奖 详情', '别嘶哈 粉 玫瑰 贵气 晚宴 妆 完整版 妆教 我学 起来 重点 浅 玫瑰色 眼影 深 粉色 眼线 加点 魅 牛郎 真的 爆闪 重点 加深 山根 两侧 面部 立体 视觉 缩短 中庭 晚宴 画 见 夸 日 抛 脸 抽奖 详情 抽奖 详情', '救 清冷 美女 直发 真的 氛围 细软 塌 发质 漫不经心 凹 造型 划 重点 梳 负离子 直发 梳不伤 头发 一梳 到底 夹 厚 牢记 指宽 C 字 弧 头 包脸 高颅 顶 分 分钟 修 善 刘海 鬓角 定型 整体 效果 做 美女 真的 好累 变美不南', '有福 同箱 早鸟版 亏 完好 拍 照片 现已 拍 完 视频 意思 先 评论 区摇 位 抽奖 详情', '开学 季 生 一个多月 年 会 vlog 前方 聒噪 预警 音量 注意 调小 本次 年会 主题 白色 浪漫 点击 进入 活动 现场 团建 惯例 抢房 大战 老母鸡 下蛋 欢迎 收看 yznn 电 臀 表演赛 年会 抽奖 全程 尖叫 前 大奖 花落谁家 没 运气 实力 饭后 娱乐 套 圈圈 百元 交换 礼物 文化 送礼 佛系 打工 必备 奇葩 好物 圆满 落幕 感谢 Team NAN 南 朋友 支持 付出 一起 继续 努力 南仔 周末 干点 抽奖 详情', '年 回来 工作室 爆仓 赶紧 霸霸们 祝福 分给 大伙儿 兔兔 拜年 新年快乐 钱 无量 评论 区 位 锦鲤 附体 锦鲤 附体 抽奖 详情', '年度 爱用物 讲 烦 护肤 篇 min 完整版 卷 彩妆 共计 品类 眼熟 真爱 自用 绝不 掺广 关键词 敏肌 友好 超多 功效 品 自信 素颜好 皮肤 行 大眼 猛药党 抗皱 紧致 眼部 护理 混干皮 养 脸 日常 必备 乳霜 面膜 精致 全身 护理 从上到下 全包 润唇 护发 身体 工具 报告 年度 总结 完毕 啊啊啊 再也 不要 早 过年 拖 这么久 抱歉 鞠躬 南得 花钱', '除夕 快乐', 's VLOG 感谢 兰蔻 免税 邀请 海口 新春 旅行 我来 海口 最近 天气 阴 晴 定 春节 氛围 感拉满 本期 看点 兰蔻 免税 直播 日 主持 南 合体 明星 嘉宾 提前 拜年 咯 兰蔻臻 宠花开 启航', 'Nan s travel diary 东北 Vlog 长白山 延吉 欢乐 天 晚 开学 季 大放送 班 一天 不想 行程 预览 单板 滑雪 兴趣 组成 立 痛 快乐 许 半夏 雪地 疯狂 飙车 老江湖 快冻出 鼻涕 延吉 公主 一天 and 欢迎 收看 美食 频道 东北 烤 炖煮 集齐 干饭 点击 沉浸 式 体验 欢乐 假期 南仔 周末 干点 抽奖 详情', '东北 旅行 穿 搭 零下 OOTD 那得 必须 保暖 尽量 好看 时 时髦 管不了 注意 截屏 套 抗冻 不露 腿 LOOK 莫兰迪色 保命 羽绒服 宝蓝色 大鹅潮 girl 许 半夏 回乡 记 偶像剧 女主秒切 文艺 邻家 小姐姐 干练 风 都市 丽人 年前 入手 新衣 抽奖 详情', '直播 预告 滴滴 新年 第一次 直播 见面 大大的 重磅 月 日 cdf 海口 国际 免税 城 直播间 见 提前 路透 Yznn 宋轶 songyi UNIQ 李汶翰 梦幻 联动 我要 前排 追星 划 重点 重磅 惊喜 一定 蹲 兰蔻 免税 携手 cdf 中免 海南 多重 新春 豪礼 送 不停 超级 宠粉 兰蔻臻 宠花开 启航 豪华 阵容 华丽 场面 一起 美美 开年 戳 cdf 海口 国际 免税 城 微博 直播', '期 圣诞 新年 交换 礼物 特辑 一盆 快递 延误 电子 榨菜 一次 深感 相互 奔赴 意义 完整 期 不得不 说 我会 挑 诶 今年 陪 跑 姐妹 明年 再战 抽奖 详情', '新年新 pr 感谢 品牌 霸霸们 过去 一年 宠爱 南 朋友 一年 陪伴 评论 区 位 一起 延续 年 幸福 抽奖 详情', 'New Year 第一次 开箱 父女 版 大笑 女 爸爸 送 新年 惊喜 险遭 抢单 礼物 关键词 全天候 陪伴 时尚 百搭 科技 单品 可测 血氧 筛查 肺部 情况 健康 数据 爸爸 拿到 华为 鸿蒙 健康 套装 差点 客厅 沙发 坐 一个 坑 一点 没 骄傲 年 年 年 年 令人 心动 潮酷 礼物', '快乐 延续 新年快乐 见者 有乐', '平安 健康', 'PR 节日 盲盒 开箱 派发 礼物 咯 快乐 藏不住 圣诞 礼盒 and 高颜值 美衣 新货 老 熟人 完 分钟 会 说 圣诞老人 炫耀 吼吼', '新手入门 普拉提 装备 分享 年 爱好者 举手发言 常购 店铺 推荐 品牌 快 入门 必备 实穿 美的 运动 Bra 舒适 专业 可外 穿 社交 瑜伽 裤 安全 必备 瑜伽 袜 防滑 手套 冬季 必备 上衣 私心 分享 需入 普拉提 宣传 大使 强烈 安利 运动 身心 柔软', '干货 新手 有效 眼线 画 预习 目录 多眼 型 适用 内双 外双 肿 眼泡 准备 工作 防晕妆 实用工具 外 眼线 入门级 通用 平拉 眼线 眼线 进阶 版 放大 双眼 赶紧 练起来 反馈 新手 化妆 连续剧 变美不南 抽奖 详情', '韩系 ins 白开水 氧气 妆 这是 一个 完整 手把手 教程 下次 看到 好看 韩妆请 我全 细节 扒 妆 强调 眼 胜 氛围 色彩 高度 和谐 值得 学 拍照 记得 眼睛 半睁 吼吼 日 抛 脸 抽奖 详情', '月 收到 漂亮 感恩 品牌 霸霸们 马上 圣诞 评论 区 位 共享 快乐 吼吼 抽奖 详情', 'Perfume 自用 淡 香水 合集 皂角 阳光 淡 花香 爱好者 请 看看 香味 导览 两年 爱 白色 浪漫 千金 皂角香 邀请 尹峥 一同 品鉴 少年 香 汤唯感 优雅 坚韧 女神 香 香水 小白 盲入 第一瓶 淡香 写 已经 认识 香 字 南得 花钱 抽奖 详情', '圣诞 交换 礼物 名单 纠结 一个 周末 以表 诚意 追加 名额 期待 期待', '件 双十 开箱 辣 拆门 开 不发 明年 概括 生活 博主 漂亮 摆件 美妆博主 护肤 彩妆 新品 穿 搭 博主 新 衣服 新 靴子 快 快 电子 拌 饭 酱 南得 花钱 抽奖 详情', '南 朋友 安利 室 抽奖 详情', '头 包脸 慵懒 盘发 教程 问 w 次 工具 根 皮筋 双手 仅此而已 限时 分钟 人群 扁塌头 低颅 顶 想 后脑勺 变更 饱满 没 可惜 变美不南', '交换 礼物 每年 固定 仪式 感来 第四期 圣诞 新年 互换 礼物 栏目 老套 开场 感谢 南 朋友 一年 年 支持 陪伴 今年 老朋友 评论 区 留下 印象 最深 一件 事 邀请 位来 体验 交换 礼物 快乐 位老粉 位新 朋友 名单 天内 公布', 'Nan s Work Holiday 海 陆 空 无缝 衔接 姗姗来迟 迟迟 迟 VLOG 奉上 首次 荧幕 亮相 本期 看点 海 Aranya Travel 疯狂 工作 旅行 快乐 想 唱 嗨 嗨 嗨 程度 陆 Tasaki 线下 忙忙碌碌 Lu 空 潘婷 TVC 日 双脚 人生 新体验 滴 拖 更 忙碌 日子 里 已经 计划 一次 出逃 抽奖 详情', '赢 牛奶 吼吼 吼 一年一度 微博 超级 红人节 今年 长沙 想见 做 倒霉蛋 看过 团建 vlog 懂 月 号 找 玩儿 送 花花 想 创造 专属 长沙 新 回忆 湘 江 美食 推荐 打卡 开学 季 红人节 议程 开启 戳 超级 红人节 查看', '可爱 碎片 寻找 闪光 定格 闪光 瞬间', 'VLOG 仙女 阿绿 打工 日记 今儿 拍摄 幕后 花絮 实记 完全 妆教 看会算 厉害', '冷艳 钓系妆 叶舒华 Nude 仿妆 美妙绝伦 本性 不可 方物 仿妆 仿人 尽力 姐妹 这类 粉 橘色 半全包 眼线 真的 显脏 无限 拉长 眼睛 增加 精致 度绝 钓系 氛围 暖杏色 腮红 灵魂 点痣 美丽 细节 懂 绿 丝绒 玫瑰 来晚 日 抛 脸 抽奖 详情', '月 异瞳 限定版 玛丽 猫', '超红来 贴 贴 天气 有点 凉 想 温暖 贴 贴 快 贴 贴 主页 贴 贴 叭 超级 红人节 先来 贴 一个 日 抛 脸美博 超级 红人节', '如风 自由 如光 闪耀 现代版 欢天喜地 七仙女 宝藏 系列 应该 微博 发 一遍 一个 颜色 人设 重新 构思 绿 七仙女 系列 拍 现在 喜欢 一个 造型 第一次 穿 绿 裙子 没想到 复古 优雅 喜欢 嘞 开学 季']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "\n",
    "# 加载停用词\n",
    "def load_stopwords():\n",
    "    url = \"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\"\n",
    "    return pd.read_csv(url, header=None, sep=\"\\t\", quoting=3, names=['stopword'], encoding='utf-8').stopword.tolist()\n",
    "\n",
    "# 获取停用词列表\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# 读取数据\n",
    "# df = pd.read_csv('your_file.csv')\n",
    "\n",
    "# 检查并删除df['微博正文']中的重复数据\n",
    "df = df.drop_duplicates(subset=['微博正文'])\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 使用正则表达式去除“XX的微博视频”\n",
    "    text = re.sub(r'[^#]*的微博视频', '', text)\n",
    "    \n",
    "    # 去除特定无用词\n",
    "    useless_terms = [\"显示地图\", \"原图\", \"[超话]\", \"超话\"]\n",
    "    for term in useless_terms:\n",
    "        text = text.replace(term, '')\n",
    "    \n",
    "    # 去除自我指代\n",
    "    for name in combined_blogger_names:\n",
    "        text = text.replace(name, '')\n",
    "    \n",
    "    # 使用jieba进行分词\n",
    "    words = jieba.cut(text, cut_all=False)\n",
    "    \n",
    "    # 去除标点和特殊字符\n",
    "    words = [re.sub(r'[^a-zA-Z\\u4e00-\\u9fff]', '', word) for word in words if len(word) > 0]\n",
    "    \n",
    "    # 去除“组图”、“共”、“张”\n",
    "    words = [word for word in words if word not in [\"组图\", \"共\", \"张\"]]\n",
    "    \n",
    "    # 去除停用词\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    # 将连续的多个空格替换为一个空格\n",
    "    result = ' '.join(words)\n",
    "    result = re.sub(r'\\s+', ' ', result).strip()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 应用预处理函数\n",
    "texts_cut = [preprocess_text(text) for text in df['微博正文']]\n",
    "print(texts_cut[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf885aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "INFO:lda:n_documents: 12541\n",
      "INFO:lda:vocab_size: 1000\n",
      "INFO:lda:n_words: 187116\n",
      "INFO:lda:n_topics: 15\n",
      "INFO:lda:n_iter: 1000\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -1959623\n",
      "INFO:lda:<10> log likelihood: -1381774\n",
      "INFO:lda:<20> log likelihood: -1293726\n",
      "INFO:lda:<30> log likelihood: -1271693\n",
      "INFO:lda:<40> log likelihood: -1261497\n",
      "INFO:lda:<50> log likelihood: -1255610\n",
      "INFO:lda:<60> log likelihood: -1251802\n",
      "INFO:lda:<70> log likelihood: -1250536\n",
      "INFO:lda:<80> log likelihood: -1248438\n",
      "INFO:lda:<90> log likelihood: -1246166\n",
      "INFO:lda:<100> log likelihood: -1245735\n",
      "INFO:lda:<110> log likelihood: -1245301\n",
      "INFO:lda:<120> log likelihood: -1244341\n",
      "INFO:lda:<130> log likelihood: -1245104\n",
      "INFO:lda:<140> log likelihood: -1243468\n",
      "INFO:lda:<150> log likelihood: -1242522\n",
      "INFO:lda:<160> log likelihood: -1242648\n",
      "INFO:lda:<170> log likelihood: -1242029\n",
      "INFO:lda:<180> log likelihood: -1242869\n",
      "INFO:lda:<190> log likelihood: -1242572\n",
      "INFO:lda:<200> log likelihood: -1242139\n",
      "INFO:lda:<210> log likelihood: -1242459\n",
      "INFO:lda:<220> log likelihood: -1243076\n",
      "INFO:lda:<230> log likelihood: -1241676\n",
      "INFO:lda:<240> log likelihood: -1241723\n",
      "INFO:lda:<250> log likelihood: -1240819\n",
      "INFO:lda:<260> log likelihood: -1241861\n",
      "INFO:lda:<270> log likelihood: -1241650\n",
      "INFO:lda:<280> log likelihood: -1241766\n",
      "INFO:lda:<290> log likelihood: -1241706\n",
      "INFO:lda:<300> log likelihood: -1241685\n",
      "INFO:lda:<310> log likelihood: -1241743\n",
      "INFO:lda:<320> log likelihood: -1241565\n",
      "INFO:lda:<330> log likelihood: -1241600\n",
      "INFO:lda:<340> log likelihood: -1240780\n",
      "INFO:lda:<350> log likelihood: -1241456\n",
      "INFO:lda:<360> log likelihood: -1241018\n",
      "INFO:lda:<370> log likelihood: -1241107\n",
      "INFO:lda:<380> log likelihood: -1240370\n",
      "INFO:lda:<390> log likelihood: -1240186\n",
      "INFO:lda:<400> log likelihood: -1239884\n",
      "INFO:lda:<410> log likelihood: -1238807\n",
      "INFO:lda:<420> log likelihood: -1239491\n",
      "INFO:lda:<430> log likelihood: -1239490\n",
      "INFO:lda:<440> log likelihood: -1238917\n",
      "INFO:lda:<450> log likelihood: -1239606\n",
      "INFO:lda:<460> log likelihood: -1239518\n",
      "INFO:lda:<470> log likelihood: -1239100\n",
      "INFO:lda:<480> log likelihood: -1238830\n",
      "INFO:lda:<490> log likelihood: -1238990\n",
      "INFO:lda:<500> log likelihood: -1238553\n",
      "INFO:lda:<510> log likelihood: -1239095\n",
      "INFO:lda:<520> log likelihood: -1237333\n",
      "INFO:lda:<530> log likelihood: -1238011\n",
      "INFO:lda:<540> log likelihood: -1238197\n",
      "INFO:lda:<550> log likelihood: -1238221\n",
      "INFO:lda:<560> log likelihood: -1237394\n",
      "INFO:lda:<570> log likelihood: -1237282\n",
      "INFO:lda:<580> log likelihood: -1237426\n",
      "INFO:lda:<590> log likelihood: -1236523\n",
      "INFO:lda:<600> log likelihood: -1237307\n",
      "INFO:lda:<610> log likelihood: -1237749\n",
      "INFO:lda:<620> log likelihood: -1236560\n",
      "INFO:lda:<630> log likelihood: -1236160\n",
      "INFO:lda:<640> log likelihood: -1236000\n",
      "INFO:lda:<650> log likelihood: -1236117\n",
      "INFO:lda:<660> log likelihood: -1236179\n",
      "INFO:lda:<670> log likelihood: -1235485\n",
      "INFO:lda:<680> log likelihood: -1235568\n",
      "INFO:lda:<690> log likelihood: -1235312\n",
      "INFO:lda:<700> log likelihood: -1234107\n",
      "INFO:lda:<710> log likelihood: -1234628\n",
      "INFO:lda:<720> log likelihood: -1234374\n",
      "INFO:lda:<730> log likelihood: -1235095\n",
      "INFO:lda:<740> log likelihood: -1235464\n",
      "INFO:lda:<750> log likelihood: -1235135\n",
      "INFO:lda:<760> log likelihood: -1235331\n",
      "INFO:lda:<770> log likelihood: -1234460\n",
      "INFO:lda:<780> log likelihood: -1235376\n",
      "INFO:lda:<790> log likelihood: -1235422\n",
      "INFO:lda:<800> log likelihood: -1234808\n",
      "INFO:lda:<810> log likelihood: -1234980\n",
      "INFO:lda:<820> log likelihood: -1234152\n",
      "INFO:lda:<830> log likelihood: -1234942\n",
      "INFO:lda:<840> log likelihood: -1234353\n",
      "INFO:lda:<850> log likelihood: -1234918\n",
      "INFO:lda:<860> log likelihood: -1234786\n",
      "INFO:lda:<870> log likelihood: -1235041\n",
      "INFO:lda:<880> log likelihood: -1234609\n",
      "INFO:lda:<890> log likelihood: -1234387\n",
      "INFO:lda:<900> log likelihood: -1234853\n",
      "INFO:lda:<910> log likelihood: -1234752\n",
      "INFO:lda:<920> log likelihood: -1234579\n",
      "INFO:lda:<930> log likelihood: -1234638\n",
      "INFO:lda:<940> log likelihood: -1235190\n",
      "INFO:lda:<950> log likelihood: -1235005\n",
      "INFO:lda:<960> log likelihood: -1235055\n",
      "INFO:lda:<970> log likelihood: -1234318\n",
      "INFO:lda:<980> log likelihood: -1234362\n",
      "INFO:lda:<990> log likelihood: -1234501\n",
      "INFO:lda:<999> log likelihood: -1234035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model done\n",
      "Topic #0:\n",
      "vlog 真的 生活 快乐 开学 旅行 一起 超级 朋友 长沙\n",
      "Topic #1:\n",
      "妆容 美妆 化妆 真的 眼妆 今日 适合 教程 今天 重点\n",
      "Topic #2:\n",
      "分享 好物 头发 双十 大会 最近 种草 开箱 护发 推荐\n",
      "Topic #3:\n",
      "礼盒 礼物 快乐 七夕 情人节 心动 约会 品牌 收到 美妆\n",
      "Topic #4:\n",
      "京东 链接 网页 一起 天猫 手机 免税 真的 姐妹 直播\n",
      "Topic #5:\n",
      "时尚 上海 生活 巴黎 感受 系列 一起 好看 设计 微博\n",
      "Topic #6:\n",
      "香水 味道 玫瑰 真的 没有 感觉 喜欢 香气 起来 一个\n",
      "Topic #7:\n",
      "护肤 防晒 分享 皮肤 面膜 好物 精华 换季 身体 卸妆\n",
      "Topic #8:\n",
      "夏日 ootd 夏天 今天 真的 今日 搭配 拍照 氛围 挑战\n",
      "Topic #9:\n",
      "互动 粉丝 抽奖 详情 评论 宝贝 一起 微博 日常 上榜\n",
      "Topic #10:\n",
      "皮肤 精华 肌肤 修护 抗老 真的 护肤 成分 状态 质地\n",
      "Topic #11:\n",
      "口红 粉底液 底妆 分享 妆容 眼影 高级 腮红 真的 出门\n",
      "Topic #12:\n",
      "plog 日常 春天 生活 春夏 日记 出游 春季 春日 look\n",
      "Topic #13:\n",
      "一个 没有 真的 知道 觉得 很多 不要 现在 一直 全文\n",
      "Topic #14:\n",
      "新年 计划 完美 攻略 开春 好颜 过年 虎年 娱乐 今天\n",
      "\n",
      "Model Perplexity:  731.4502366654693\n",
      "Results saved\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# 3. 文本向量化\n",
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, stop_words=stopwords, max_df=0.5, min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(texts_cut)\n",
    "\n",
    "# 4. LDA模型训练\n",
    "n_topics = 15\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=1000, random_state=2)\n",
    "model.fit(tf)\n",
    "print('Model done')\n",
    "\n",
    "# 5. 打印每个主题的关键词\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# 6. 为每个文档分配主题\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = [topics.argmax() for topics in doc_topic]\n",
    "\n",
    "# 7. 计算困惑度\n",
    "log_likelihood = model.loglikelihood()\n",
    "n_total_words = tf.sum()\n",
    "perplexity = np.exp(-log_likelihood / n_total_words)\n",
    "print(\"Model Perplexity: \", perplexity)\n",
    "\n",
    "# 8. 保存结果\n",
    "df.to_excel('/Users/laihuiqian/Documents/weibo_0925/total2-0925_with_topics.xlsx', index=False)\n",
    "print('Results saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fb2aad3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['回家 饭后 散步 特别 舒服 初秋 上海', '第一天 水逆退 来接 好运 固定 栏目 pr 礼物 上线 评论 许愿池 开放 共享 快乐 ps 谢谢 品牌 霸霸们', '出差 ootd 感受 提前 入秋', '沙漠 邂逅 玫瑰', '露思 居家 慵懒 拍照 太出 纯享 图奉 直接 照抄', '菊宅 一段 美丽 生活', '周一 午好 收到', '半年度 工作 总结 黢黑 努力 养白中 反复 把姐', '或许 壁纸 看看', '暴走 首尔', '第一个 跨国 无比 想念 国内 韩国', '粽意 粽意 端午 快乐 谁家 今天 早餐 粽子 摇位 点儿 东西', '属于 生日 小时 幸福 碎片 多图 快乐 感谢 一位 到来 平凡 一天 鲜活 温暖 下次 再见', 'Au revoir Paris 法国 巴黎', '夏日 冷萃', '补发 昨天 现场图 两位 仙人 仙女 电影 之夜 红毯 初体验', '柔光 莹莹', '快乐 收工', '加油 金榜题名', '希腊 库存 dd', '六一 快乐 冲刺 冲刺 安排 过个 快乐 谢谢 品牌 霸霸', '蚌壳 公主 写真 芭莎 珠宝 夜宴 第二年 澳门', '最近 好好看 忍不住 分享 分享', '歼灭 敌军', '放点 添添 节气 提前 51 快乐 评论 区位', '过期 日常', '开心', '今天 美丽 瞬间', '风来 听风 雨来 听雨', '报告 到达 传递 前两天 快乐', 'Nan Daliy PLOG 复古 相机 味儿 夏日 碎片', '一个脚印', '诚邀 长沙 颜悦色 黑色 经典 大厨 小炒 牛肉 激情 面基 下午 不见不散 想来 超级 红人节', 'Nan Daliy PLOG', '固定 栏目 pr 共享 本月 开箱 分享 评论 区位', '有福 同箱 早鸟版 完好 照片 现已 视频 意思 评论 区摇位', '回来 工作室 爆仓 赶紧 霸霸们 祝福 分给 大伙儿 兔兔 拜年 新年快乐 无量 评论 区位 锦鲤 附体 锦鲤 附体', '除夕 快乐', '滴滴 新年 第一次 直播 见面 大大的 重磅 12 直播间 提前 路透 Yznn 宋轶 songyi UNIQ 李汶翰 梦幻 联动 我要 前排 追星 重点 重磅 惊喜 一定 携手 cdf 中免 海南 多重 新春 豪礼 不停 超级 宠粉 豪华 阵容 华丽 场面 一起 美美 开年 cdf 海口 国际 免税 微博 直播', '新年新 pr 感谢 品牌 霸霸们 过去 一年 宠爱 朋友 一年 陪伴 评论 区位 一起 延续 幸福', '快乐 延续 新年快乐 见者 有乐', '平安 健康', '收到 漂亮 感恩 品牌 霸霸们 马上 圣诞 评论 区位 共享 快乐 吼吼', '圣诞 交换 礼物 名单 纠结 一个 周末 以表 诚意 追加 名额 共人 期待 期待', '交换 礼物 每年 固定 仪式 感来 第四期 圣诞 新年 互换 礼物 栏目 老套 开场 感谢 朋友 一年 支持 陪伴 今年 老朋友 评论 留下 印象 最深 一件 邀请 位来 体验 交换 礼物 快乐 位老粉 位新 朋友 名单 天内 公布', '牛奶 吼吼 一年一度 微博 今年 倒霉蛋 看过 团建 vlog 12 玩儿 花花 创造 专属 长沙 回忆 美食 推荐 打卡 开学 红人节 议程 开启 超级 红人节 查看', '可爱 碎片', '11 异瞳 限定版 玛丽', '天气 有点 温暖 主页 先来 一个 脸美博 超级 红人节', '接受 一下 周末 新鲜 大笑 快乐 传递 biu', '秋日 庄园 游记 万物 秋意 正酣 天气 适合 庄园 散心 肆意 动容 眼霜 沁润 娇嫩 眼周 笑意 长随 我心 润物无声 精华 入妆 粉霜 奢养肤 时间 更迭 相伴 不畏 岁月 长河 恒绽 高光 一起 绽放 秋日 不衰 光彩 温柔 坚定 纵享 属于', '滴滴 本月 pr 公开 日到 感恩 品牌 霸霸 宠爱 姐妹 大饱眼福 样子 评论 抽位 心想事成 吼吼', '上周 一卷 库存 胶卷 看过 看过 求求 doge', '抬起 热烈 风中 相遇', 'you be faithful to yourself live earnestly and laugh freely 忠于 认真 放肆', '预告 pr 开箱 即将 送达 先上 先行 看中 评论 留言 视频 安排', '周末 愉快 预告 一下', '蓝白 配色 Arnaya 日记 第一次 阿那 没有 想到 节奏 舒适 北方 秋天 海风 阳光 温差 刚刚 下次 还来', '感受 心脏 跳动 分秒 热爱 感恩 世界 万物 一起 参与 公益 微博 做好事 网页 链接', '甘孜 泸定 地震 红帆 行动 发起 一起 公益 大小 行动 榜样 公益 大小 快来 一起 项目 助力 甘孜 泸定 地震 红帆 行动', '一年 开学 美丽 学姐 驾到 状态 唤醒 计划 假期 say 调整 作息 肌肤 解压 精简 护肤 一瓶 搞定 皮肤 需求 雅诗兰黛 首选 精华 专利 成分 律波 深入 肌底 调整 肌肤 昼夜节律 加速 修护 细胞 损伤 促生 胶原蛋白 透明质 酸钠 二裂 酵母 VE 保湿 抚纹 抗氧 满分 妆容 美好 面貌 启程 整装待发 早秋 校园 感穿 OOTD 面对面 唤醒 预约 主题 黄金 修护 维稳 肌肤 秘诀 时间 记得 我会 揭晓 修护 熬夜 皮肤 保持 上镜 状态 独家 秘诀 直播间 专属 福利 起来 解锁 链接 一定 要来 网页 链接', '诚邀 朋友 共享 感谢 品牌 霸霸们 厚爱 中秋 祝福 希望 中秋 快乐 心想事成 依旧 评论 区见', 'Nan Daliy PLOG', '奥比 品牌 霸霸们 周一 快乐 回来 看看 中过 评论 开放 许愿池 hiahia', '朋友 牛奶 本座 有事 宣布 上线 身份 闪光 生活 见证人 叉腰 一直 陪伴 南仔 好奇 可爱 日常 有意思 没意思 生活 值得 记录 时间 即日起 活动 规则 话题 发布 原创 视频 福利 海量 资源 12 现金 大奖 瓜分 了解 活动 开学 点击 题词 解锁 评论 分享 开心', '世界 偏见 脚下 第一次 橙色 截断 真的 好酷 发型 有没有 宝想学 干杯 春游 家族', '一期 爱心 泡泡 礼物 合集 谢谢 霸霸 厚爱 邀请 姐妹 一起 快乐 吼吼 记得 评论 区见', '一套 正经 写真 理发店 盗图 必究 别信', '一月 一度 品牌 霸霸 感恩 大会 满满当当 九宫格 朋友 评论 集合', '仲夏 物语 茶园 山庄 今日 份人设 假日 都市 丽人 心之所 现在 工作 时间 行之所 双人 成行 结伴 山花 烂漫 云卷云舒 小黑 精华 相伴 悠闲 护肤 变美 时刻 小白管 防晒 持妆 粉底液 拥抱 日光 底气 属于 热烈 自由', '写真 生日快乐', '超级 月亮 女士 合照', '品牌 霸霸们 永不 缺席 南仔 开箱 有福同享 老规矩 评论 集合', '网球 少女 夏日 限定版 cheer for our summer 活力 初次 尝试 网球 元气 一天 阳光 肌肤 透气 秘密 汗水 夏日 持妆 必备 拒绝 斑驳 闷痘 自信 尽情 释放 运动 魅力 夏日 满分 电量 满格', '写真 海边 限定 皮肤', 'Nan Daliy PLOG', '一周', '我先 声明 作业 618 准备 今年 我要 优质 羊毛 以图 易物', '库存 写真 组太娘 下次 一定 一点', 'Hey 辣妹 张忍 今天 花絮', '美丽 live 库存 一期 PLOG', 'Pr 礼盒 五一 囤货 感谢 品牌 老规矩 开学 评论 共享 快乐 但愿 快递 正常', '追星 成功 快乐 一天 知道 记得 几个 昵称', '标题 留给', '视频 真的 很绝', '春暖花开 节日 限定版 礼盒 Yznn 好能 一女 开学 福利 dddd 评论 区见', 'BAZAAR JEWELRY 感恩 芭莎 珠宝 解锁 红毯 主持 新技能 夜晚 VLOG TASAKIWECOUTURE', '海岛 日记 艾特 今年夏天 一起 看海', '出差 小南人 live 椰子 jpg 三亚', '浪漫 周末 几年 没有 认真 画画 这件 事静 下心 分钟 对话 自我', '开工 大吉 祝宝子们 许愿 福气 如虎添翼 心想事成 福气 满满 新年 PR 礼盒 第一 共享 快乐 牛奶 老规矩 评论 区见', 'Nan Daliy PLOG 放假', '成长 过程 勇于探索 带来 新奇 体验 潜伏 危险 小时候 妈妈 一边 厨房 一边 厨房 离远 一点 记得 小时候 爸爸妈妈 教过 安全 知识 加入 思源 方舟 安全 校服 小朋友 安全 校服 安全 教育 课堂 小朋友 成长 双重 守护 公益 扬帆 计划 微博共益 计划', '新年快乐 一年 大女主 春游 家族 剧情 丰满 顺利 小雪人 微笑 趣泡', '五一 人头 彩虹', '阳光 最好 滤镜 赶紧 带上 家里 毛孩子 露营', '可爱 日常', '最近 天气 终于 回暖 外出 郊游 踏青 一定 做好 防晒 通勤 防晒 户外 防晒 护肤 需要 进行 选择 毕竟 防晒 抗老 事后 花大钱 抗老 认真 涂好 防晒霜', '终身 浪漫', '马上 三八 女神 女性 特权 节日 姐妹 安排 起来 过节 这天 过生日 开心 这种 媒体 行业 工作 节奏快 难得 工作日 忙里偷闲 尽情 放松 今年 打算 在家 喝点 小酒 看点 电影 好好 洗个 放松 放松 不得不 安利 一下 舒肤佳 山茶花 泡沫 沐浴露 泡沫 绵密 山茶花 真的 hin 高级 hin 好闻 沉浸 沐浴 拿捏 最后 睡前 准备 一杯 牛奶 一种 身心 得到 滋养 感觉 姐妹 囤点 尽情 宠爱 好物 宝洁 新品 开启 大牌 宝洁 限时 秒杀 好物 网页 链接']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "\n",
    "# 加载停用词\n",
    "def load_stopwords():\n",
    "    url = \"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\"\n",
    "    return pd.read_csv(url, header=None, sep=\"\\t\", quoting=3, names=['stopword'], encoding='utf-8').stopword.tolist()\n",
    "\n",
    "# 获取停用词列表\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# 示例文本\n",
    "texts = df[\"微博正文\"]\n",
    "\n",
    "# 保留的特殊数字列表\n",
    "special_numbers = [\"11\", \"1111\", \"618\", \"315\", \"12\", \"1212\", \"51\"]\n",
    "\n",
    "# 文本预处理\n",
    "def preprocess_text(text):\n",
    "    # 去除URL\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # 去除表情符号和特殊字符\n",
    "    text = re.sub(r'[\\U00010000-\\U0010ffff]|[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]', '', text)\n",
    "    \n",
    "    # 使用正则表达式去除“组图共\\d张”\n",
    "    text = re.sub(r'组图共\\s*\\d+\\s*张', '', text)\n",
    "\n",
    "    \n",
    "    # 去除“..”和\"...\"\n",
    "    text = re.sub(r'\\.{2,}', '', text)\n",
    "    \n",
    "    # 去除标签和话题\n",
    "    text = re.sub(r'#.*?#', '', text)\n",
    "    \n",
    "    # 去除不在特殊数字列表中的数字（包括整数和小数）\n",
    "    text = re.sub(r'(?<!\\.)\\d+(\\.\\d+)?(?!\\.)', lambda x: x.group() if x.group() in special_numbers else '', text)\n",
    "    \n",
    "    # 去除多余空格和换行符\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    \n",
    "    # 使用正则表达式去除“XX的微博视频”\n",
    "    text = re.sub(r'[^#]*的微博视频', '', text)\n",
    "    \n",
    "    # 去除特定无用词\n",
    "    useless_terms = [\"显示地图\", \"原图\", \"[超话]\", \"超话\", \"详情\",\"\\[组图共\\d张\\]\",\"组图共\\d张\",\"组图\",\"网页\"]\n",
    "    \n",
    "    for term in useless_terms:\n",
    "        text = text.replace(term, '')\n",
    "        \n",
    "    \n",
    "    # 去除自我指代\n",
    "    for name in combined_blogger_names:\n",
    "        text = text.replace(name, '')\n",
    "    \n",
    "    # jieba分词\n",
    "    words = jieba.cut(text)\n",
    "    \n",
    "    # 去除停用词和单个字符\n",
    "    words = [word for word in words if word not in stopwords and len(word) > 1]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# 应用预处理函数\n",
    "texts_cut = [preprocess_text(text) for text in texts if preprocess_text(text) != '']\n",
    "print(texts_cut[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cb6ddd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 8338\n",
      "INFO:lda:vocab_size: 1000\n",
      "INFO:lda:n_words: 122219\n",
      "INFO:lda:n_topics: 15\n",
      "INFO:lda:n_iter: 1000\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -1289743\n",
      "INFO:lda:<10> log likelihood: -903777\n",
      "INFO:lda:<20> log likelihood: -858986\n",
      "INFO:lda:<30> log likelihood: -843011\n",
      "INFO:lda:<40> log likelihood: -834811\n",
      "INFO:lda:<50> log likelihood: -829481\n",
      "INFO:lda:<60> log likelihood: -826107\n",
      "INFO:lda:<70> log likelihood: -824035\n",
      "INFO:lda:<80> log likelihood: -822882\n",
      "INFO:lda:<90> log likelihood: -820896\n",
      "INFO:lda:<100> log likelihood: -819457\n",
      "INFO:lda:<110> log likelihood: -818418\n",
      "INFO:lda:<120> log likelihood: -818113\n",
      "INFO:lda:<130> log likelihood: -817553\n",
      "INFO:lda:<140> log likelihood: -816813\n",
      "INFO:lda:<150> log likelihood: -817155\n",
      "INFO:lda:<160> log likelihood: -816780\n",
      "INFO:lda:<170> log likelihood: -816363\n",
      "INFO:lda:<180> log likelihood: -816004\n",
      "INFO:lda:<190> log likelihood: -815301\n",
      "INFO:lda:<200> log likelihood: -815064\n",
      "INFO:lda:<210> log likelihood: -815325\n",
      "INFO:lda:<220> log likelihood: -815032\n",
      "INFO:lda:<230> log likelihood: -815396\n",
      "INFO:lda:<240> log likelihood: -815361\n",
      "INFO:lda:<250> log likelihood: -814512\n",
      "INFO:lda:<260> log likelihood: -814804\n",
      "INFO:lda:<270> log likelihood: -814656\n",
      "INFO:lda:<280> log likelihood: -814996\n",
      "INFO:lda:<290> log likelihood: -814804\n",
      "INFO:lda:<300> log likelihood: -814834\n",
      "INFO:lda:<310> log likelihood: -814415\n",
      "INFO:lda:<320> log likelihood: -814419\n",
      "INFO:lda:<330> log likelihood: -814505\n",
      "INFO:lda:<340> log likelihood: -814284\n",
      "INFO:lda:<350> log likelihood: -814768\n",
      "INFO:lda:<360> log likelihood: -814365\n",
      "INFO:lda:<370> log likelihood: -814847\n",
      "INFO:lda:<380> log likelihood: -814917\n",
      "INFO:lda:<390> log likelihood: -814385\n",
      "INFO:lda:<400> log likelihood: -813539\n",
      "INFO:lda:<410> log likelihood: -814441\n",
      "INFO:lda:<420> log likelihood: -813815\n",
      "INFO:lda:<430> log likelihood: -813581\n",
      "INFO:lda:<440> log likelihood: -813876\n",
      "INFO:lda:<450> log likelihood: -814051\n",
      "INFO:lda:<460> log likelihood: -814220\n",
      "INFO:lda:<470> log likelihood: -813481\n",
      "INFO:lda:<480> log likelihood: -813934\n",
      "INFO:lda:<490> log likelihood: -813788\n",
      "INFO:lda:<500> log likelihood: -814064\n",
      "INFO:lda:<510> log likelihood: -814083\n",
      "INFO:lda:<520> log likelihood: -814078\n",
      "INFO:lda:<530> log likelihood: -813905\n",
      "INFO:lda:<540> log likelihood: -813865\n",
      "INFO:lda:<550> log likelihood: -814794\n",
      "INFO:lda:<560> log likelihood: -814158\n",
      "INFO:lda:<570> log likelihood: -814168\n",
      "INFO:lda:<580> log likelihood: -813969\n",
      "INFO:lda:<590> log likelihood: -814252\n",
      "INFO:lda:<600> log likelihood: -814062\n",
      "INFO:lda:<610> log likelihood: -814190\n",
      "INFO:lda:<620> log likelihood: -814555\n",
      "INFO:lda:<630> log likelihood: -814199\n",
      "INFO:lda:<640> log likelihood: -814250\n",
      "INFO:lda:<650> log likelihood: -814128\n",
      "INFO:lda:<660> log likelihood: -814425\n",
      "INFO:lda:<670> log likelihood: -813771\n",
      "INFO:lda:<680> log likelihood: -814111\n",
      "INFO:lda:<690> log likelihood: -814701\n",
      "INFO:lda:<700> log likelihood: -813967\n",
      "INFO:lda:<710> log likelihood: -814046\n",
      "INFO:lda:<720> log likelihood: -814444\n",
      "INFO:lda:<730> log likelihood: -814334\n",
      "INFO:lda:<740> log likelihood: -814448\n",
      "INFO:lda:<750> log likelihood: -814044\n",
      "INFO:lda:<760> log likelihood: -813697\n",
      "INFO:lda:<770> log likelihood: -814079\n",
      "INFO:lda:<780> log likelihood: -814131\n",
      "INFO:lda:<790> log likelihood: -814347\n",
      "INFO:lda:<800> log likelihood: -813932\n",
      "INFO:lda:<810> log likelihood: -814002\n",
      "INFO:lda:<820> log likelihood: -813996\n",
      "INFO:lda:<830> log likelihood: -813776\n",
      "INFO:lda:<840> log likelihood: -813017\n",
      "INFO:lda:<850> log likelihood: -814100\n",
      "INFO:lda:<860> log likelihood: -814298\n",
      "INFO:lda:<870> log likelihood: -813912\n",
      "INFO:lda:<880> log likelihood: -813715\n",
      "INFO:lda:<890> log likelihood: -813759\n",
      "INFO:lda:<900> log likelihood: -813624\n",
      "INFO:lda:<910> log likelihood: -813866\n",
      "INFO:lda:<920> log likelihood: -813663\n",
      "INFO:lda:<930> log likelihood: -814375\n",
      "INFO:lda:<940> log likelihood: -814431\n",
      "INFO:lda:<950> log likelihood: -814251\n",
      "INFO:lda:<960> log likelihood: -814657\n",
      "INFO:lda:<970> log likelihood: -814195\n",
      "INFO:lda:<980> log likelihood: -814189\n",
      "INFO:lda:<990> log likelihood: -814520\n",
      "INFO:lda:<999> log likelihood: -814260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model done\n",
      "Topic #0:\n",
      "礼盒 礼物 新年 品牌 收到 微博 快乐 感谢 限定 官方\n",
      "Topic #1:\n",
      "粉底液 底妆 彩妆 适合 气垫 秋冬 定妆 喷雾 眼影 质地\n",
      "Topic #2:\n",
      "皮肤 肌肤 精华 修护 护肤 抗老 成分 质地 真的 状态\n",
      "Topic #3:\n",
      "生活 一个 工作 希望 努力 选择 快乐 很多 喜欢 非常\n",
      "Topic #4:\n",
      "妆容 真的 氛围 眼妆 搭配 今天 口红 分享 温柔 适合\n",
      "Topic #5:\n",
      "真的 感觉 非常 没有 觉得 一个 这种 喜欢 味道 不会\n",
      "Topic #6:\n",
      "精华 护肤 防晒 分享 好物 全文 面霜 面膜 卸妆 最近\n",
      "Topic #7:\n",
      "最近 真的 快乐 分享 生活 一下 好好 今天 近期 上海\n",
      "Topic #8:\n",
      "夏天 春天 夏日 今天 真的 拍照 全文 一起 化妆 浪漫\n",
      "Topic #9:\n",
      "链接 京东 网页 一起 福利 好物 姐妹 直播间 直播 准备\n",
      "Topic #10:\n",
      "一起 体验 感受 系列 时尚 三亚 现场 艺术 免税 打卡\n",
      "Topic #11:\n",
      "互动 粉丝 宝贝 评论 上榜 私信 多多 日常 福利 一起\n",
      "Topic #12:\n",
      "头发 身体 真的 护发 最近 护理 分享 精油 全文 姐妹\n",
      "Topic #13:\n",
      "一个 没有 知道 真的 不会 看到 一下 可能 全文 已经\n",
      "Topic #14:\n",
      "香水 玫瑰 味道 香气 温柔 清新 品牌 花香 木质 喜欢\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (8338) does not match length of index (12541)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9ec72dc64437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# 6. 为每个文档分配主题\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdoc_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_topic_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_topic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# 7. 计算困惑度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3162\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3163\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3240\u001b[0m         \"\"\"\n\u001b[1;32m   3241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3242\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3243\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3898\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3899\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3901\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \"\"\"\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    752\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (8338) does not match length of index (12541)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# 3. 文本向量化\n",
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, stop_words=stopwords, max_df=0.5, min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(texts_cut)\n",
    "\n",
    "# 4. LDA模型训练\n",
    "n_topics = 15\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=1000, random_state=2)\n",
    "model.fit(tf)\n",
    "print('Model done')\n",
    "\n",
    "# 5. 打印每个主题的关键词\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# 6. 为每个文档分配主题\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = [topics.argmax() for topics in doc_topic]\n",
    "\n",
    "# 7. 计算困惑度\n",
    "log_likelihood = model.loglikelihood()\n",
    "n_total_words = tf.sum()\n",
    "perplexity = np.exp(-log_likelihood / n_total_words)\n",
    "print(\"Model Perplexity: \", perplexity)\n",
    "\n",
    "# # 8. 保存结果\n",
    "# df.to_excel('/Users/laihuiqian/Documents/weibo_0925/total2-0925_with_topics.xlsx', index=False)\n",
    "# print('Results saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63820d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
