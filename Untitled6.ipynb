{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df52576e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'正文' column extracted and saved to /Users/laihuiqian/weibo/test/阿野Flora/2248502817_pure.txt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "csv_file_path = \"/Users/laihuiqian/weibo/test/阿野Flora/2248502817.csv\"\n",
    "output_txt_path = \"/Users/laihuiqian/weibo/test/阿野Flora/2248502817_pure.txt\"\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "\n",
    "# 检查\"正文\"列是否存在\n",
    "if \"正文\" in df.columns:\n",
    "    # 提取\"正文\"列并去除NaN值\n",
    "    content = df[\"正文\"].dropna()\n",
    "\n",
    "    # 保存到TXT文件\n",
    "    content.to_csv(output_txt_path, index=False, header=False, encoding='utf-8')\n",
    "    print(f\"'正文' column extracted and saved to {output_txt_path}.\")\n",
    "else:\n",
    "    print(f\"'正文' column not found in {csv_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "379db070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'正文' column extracted and saved to /Users/laihuiqian/weibo/test/可乙瘦/2609299062_pure.txt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "csv_file_path = \"/Users/laihuiqian/weibo/test/可乙瘦/2609299062.csv\"\n",
    "output_txt_path = \"/Users/laihuiqian/weibo/test/可乙瘦/2609299062_pure.txt\"\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "\n",
    "# 检查\"正文\"列是否存在\n",
    "if \"正文\" in df.columns:\n",
    "    # 提取\"正文\"列并去除NaN值\n",
    "    content = df[\"正文\"].dropna()\n",
    "\n",
    "    # 保存到TXT文件\n",
    "    content.to_csv(output_txt_path, index=False, header=False, encoding='utf-8')\n",
    "    print(f\"'正文' column extracted and saved to {output_txt_path}.\")\n",
    "else:\n",
    "    print(f\"'正文' column not found in {csv_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e907b9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'正文' column extracted and saved to /Users/laihuiqian/weibo/test/梁上進henderson/1764357042_pure.txt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "csv_file_path = \"/Users/laihuiqian/weibo/test/梁上進henderson/1764357042.csv\"\n",
    "output_txt_path = \"/Users/laihuiqian/weibo/test/梁上進henderson/1764357042_pure.txt\"\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "\n",
    "# 检查\"正文\"列是否存在\n",
    "if \"正文\" in df.columns:\n",
    "    # 提取\"正文\"列并去除NaN值\n",
    "    content = df[\"正文\"].dropna()\n",
    "\n",
    "    # 保存到TXT文件\n",
    "    content.to_csv(output_txt_path, index=False, header=False, encoding='utf-8')\n",
    "    print(f\"'正文' column extracted and saved to {output_txt_path}.\")\n",
    "else:\n",
    "    print(f\"'正文' column not found in {csv_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "703125a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'正文' column extracted and saved to /Users/laihuiqian/weibo/test/Annie_妮大人/5611646229_pure.txt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "csv_file_path = \"/Users/laihuiqian/weibo/test/Annie_妮大人/5611646229.csv\"\n",
    "output_txt_path = \"/Users/laihuiqian/weibo/test/Annie_妮大人/5611646229_pure.txt\"\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "\n",
    "# 检查\"正文\"列是否存在\n",
    "if \"正文\" in df.columns:\n",
    "    # 提取\"正文\"列并去除NaN值\n",
    "    content = df[\"正文\"].dropna()\n",
    "\n",
    "    # 保存到TXT文件\n",
    "    content.to_csv(output_txt_path, index=False, header=False, encoding='utf-8')\n",
    "    print(f\"'正文' column extracted and saved to {output_txt_path}.\")\n",
    "else:\n",
    "    print(f\"'正文' column not found in {csv_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17ef3c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       不能错过｜乔一的超详细抗老“攻略”\\n马上就要迈入30岁大关啦！年龄“进阶”了，护肤当然也要...\n",
      "1       ⚠️注意了‼️网购避雷指南💣#微博vlog大赛##寻找闪光的你# \\n网购真真假假太头疼\\n...\n",
      "2       又到了一年一度，月饼🥮卷颜值的时刻了，来看看你们被哪款迷到了。\\n中秋一起享受团团圆圆，我也...\n",
      "3       中秋佳节将至，阖家团圆的日子当然少不了美酒助兴！更适合咱们中国宝宝体质的@五粮液 带着#中秋...\n",
      "4                                                  咔嚓！📸🤳 \n",
      "                              ...                        \n",
      "2654    #万物皆可滑雪#看着4位【阿里巴巴冰雪潮流官】用来滑雪的皮划艇、充气火烈鸟和睡袋，手里的滑雪...\n",
      "2655    年度爱用分享|彩妆篇📝\\n给大家盘点了2021年的爱用彩妆\\n很多都已经给我用到铁皮啦\\n可...\n",
      "2656    关于臭美这件事，十个有九个喜欢折腾头发，好看是真好看，伤发也是真伤发！其中最恐怖的就是脱发了...\n",
      "2657    本以为小捷和徐正是甜甜的姐弟恋，没想到变成法制在线，真是给我气死了！小捷被徐正囚禁后离婚，即...\n",
      "2658    ☁️\\n凛冬散尽 星河长明 \\n新的一年 万事顺遂\\n👜：POLENE  PARIS#202...\n",
      "Length: 2659, dtype: object\n",
      "All values saved to /Users/laihuiqian/weibo/all_data/all_values.txt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "excel_file_path = \"/Users/laihuiqian/weibo/all_data/89-100-all-2023.xlsx\"\n",
    "output_txt_path = \"/Users/laihuiqian/weibo/all_data/all_values.txt\"\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# 将所有的列堆叠起来形成一个单一的列\n",
    "all_values = df.stack().reset_index(drop=True)\n",
    "\n",
    "# 打印所有的值\n",
    "print(all_values)\n",
    "\n",
    "# 保存到TXT文件\n",
    "all_values.to_csv(output_txt_path, index=False, header=False, encoding='utf-8')\n",
    "print(f\"All values saved to {output_txt_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0bfbe874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-6bcb3292acee>:17: UserWarning: Pandas requires version '1.4.3' or newer of 'xlsxwriter' (version '1.3.8' currently installed).\n",
      "  all_values_df.to_excel(output_excel_path, index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values saved to /Users/laihuiqian/weibo/all_data/1-20-all-2023-new.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "excel_file_path = \"/Users/laihuiqian/weibo/all_data/1-20-all-2023.xlsx\"\n",
    "output_excel_path = \"/Users/laihuiqian/weibo/all_data/1-20-all-2023-new.xlsx\"\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# 将所有的列堆叠起来形成一个单一的列\n",
    "all_values = df.stack().reset_index(drop=True)\n",
    "\n",
    "# 转换为DataFrame\n",
    "all_values_df = pd.DataFrame(all_values, columns=[\"Values\"])\n",
    "\n",
    "# 保存到新的Excel文件\n",
    "all_values_df.to_excel(output_excel_path, index=False)\n",
    "print(f\"All values saved to {output_excel_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3541b5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Values\n",
      "0  🥂 ✨ 𝗩𝗹𝗼𝗴  ‣  周记77 粥觉的生日特辑  ✨🥂                 ...\n",
      "1  mini vlog 🧏🏻♀️用了新买的水泥盘  尊的很好看！#微博变美手册##妆容# 一番f...\n",
      "2  【2022·年度爱用物🏆彩妆篇】超长25min完整版！20大品类！100+件爱用精选！一路常...\n",
      "3  又双叒和𝐄𝐒𝐓𝐄𝐄 𝐋𝐀𝐔𝐃𝐄𝐑&𝐜𝐝𝐟老朋友们合作啦宇恒尊的太可爱啦 最后能够现场听歌真的...\n",
      "4  MAKEUP|#泰式千金气质拿捏住了#宝贝们，泰版流星花园都看了吗，简直就是偶像剧的颜值天花...\n",
      "(5260, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "excel_file_path = \"/Users/laihuiqian/weibo/all_data/1-20-all-2023-new.xlsx\"\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel(excel_file_path, engine='openpyxl')\n",
    "\n",
    "# 展示头部内容\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d4d67eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    🥂 ✨ 𝗩𝗹𝗼𝗴  ‣  周记77 粥觉的生日特辑  ✨🥂                 ...\n",
      "1    mini vlog 🧏🏻♀️用了新买的水泥盘  尊的很好看！#微博变美手册##妆容# 一番f...\n",
      "2    【2022·年度爱用物🏆彩妆篇】超长25min完整版！20大品类！100+件爱用精选！一路常...\n",
      "3    又双叒和𝐄𝐒𝐓𝐄𝐄 𝐋𝐀𝐔𝐃𝐄𝐑&𝐜𝐝𝐟老朋友们合作啦宇恒尊的太可爱啦 最后能够现场听歌真的...\n",
      "4    MAKEUP|#泰式千金气质拿捏住了#宝贝们，泰版流星花园都看了吗，简直就是偶像剧的颜值天花...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 定义文件夹路径\n",
    "folder_path = \"/Users/laihuiqian/weibo/all_data/all_new_data\"\n",
    "\n",
    "# 初始化一个空的DataFrame，用于存储所有的数据\n",
    "all_data = []\n",
    "\n",
    "# 遍历文件夹中的所有Excel文件\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith('.xlsx'):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        # 读取Excel文件\n",
    "        df = pd.read_excel(file_path, engine='openpyxl')\n",
    "        \n",
    "        # 将所有的列堆叠起来形成一个单一的列\n",
    "        stacked_values = df.stack().reset_index(drop=True)\n",
    "        \n",
    "        # 追加到总的数据列表中\n",
    "        all_data.append(stacked_values)\n",
    "\n",
    "# 合并所有数据到一个DataFrame\n",
    "df_total = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# 重命名列名\n",
    "df_total.columns = ['Values']\n",
    "\n",
    "# 展示头部内容\n",
    "print(df_total.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1a05e8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28510,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print(df_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3ae18153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        🥂 ✨ 𝗩𝗹𝗼𝗴  ‣  周记77 粥觉的生日特辑  ✨🥂                 ...\n",
      "1        mini vlog 🧏🏻♀️用了新买的水泥盘  尊的很好看！#微博变美手册##妆容# 一番f...\n",
      "2        【2022·年度爱用物🏆彩妆篇】超长25min完整版！20大品类！100+件爱用精选！一路常...\n",
      "3        又双叒和𝐄𝐒𝐓𝐄𝐄 𝐋𝐀𝐔𝐃𝐄𝐑&𝐜𝐝𝐟老朋友们合作啦宇恒尊的太可爱啦 最后能够现场听歌真的...\n",
      "4        MAKEUP|#泰式千金气质拿捏住了#宝贝们，泰版流星花园都看了吗，简直就是偶像剧的颜值天花...\n",
      "                               ...                        \n",
      "28505    #王俊凯##吴磊##王鹤棣# 别人都在处处吻，只有王俊凯还在QQ爱哈哈哈哈！ 迷妹说娱乐的微...\n",
      "28506               爷爷也太宠福猪猪了吧！蝴蝶：真的没有人为我发声吗？ 迷妹说娱乐的微博视频  \n",
      "28507        #关晓彤##于谦# 有网友扒出关晓彤按辈分是于谦姑姑，你相信吗？ 迷妹说娱乐的微博视频  \n",
      "28508     #那英##马嘉祺# 那英演我听马嘉祺唱歌的反应，马嘉祺唱的真的太好听了 迷妹说娱乐的微博视频  \n",
      "28509       #张云龙##娱乐# 我真的笑晕了！看你们把龙哥气的哈哈哈哈哈！！！ 迷妹说娱乐的微博视频  \n",
      "Length: 28510, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print(df_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3566758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        🥂 ✨ 𝗩𝗹𝗼𝗴  ‣  周记77 粥觉的生日特辑  ✨🥂                 ...\n",
      "1        mini vlog 🧏🏻♀️用了新买的水泥盘  尊的很好看！#微博变美手册##妆容# 一番f...\n",
      "2        【2022·年度爱用物🏆彩妆篇】超长25min完整版！20大品类！100+件爱用精选！一路常...\n",
      "3        又双叒和𝐄𝐒𝐓𝐄𝐄 𝐋𝐀𝐔𝐃𝐄𝐑&𝐜𝐝𝐟老朋友们合作啦宇恒尊的太可爱啦 最后能够现场听歌真的...\n",
      "4        MAKEUP|#泰式千金气质拿捏住了#宝贝们，泰版流星花园都看了吗，简直就是偶像剧的颜值天花...\n",
      "                               ...                        \n",
      "28505    #王俊凯##吴磊##王鹤棣# 别人都在处处吻，只有王俊凯还在QQ爱哈哈哈哈！ 迷妹说娱乐的微...\n",
      "28506               爷爷也太宠福猪猪了吧！蝴蝶：真的没有人为我发声吗？ 迷妹说娱乐的微博视频  \n",
      "28507        #关晓彤##于谦# 有网友扒出关晓彤按辈分是于谦姑姑，你相信吗？ 迷妹说娱乐的微博视频  \n",
      "28508     #那英##马嘉祺# 那英演我听马嘉祺唱歌的反应，马嘉祺唱的真的太好听了 迷妹说娱乐的微博视频  \n",
      "28509       #张云龙##娱乐# 我真的笑晕了！看你们把龙哥气的哈哈哈哈哈！！！ 迷妹说娱乐的微博视频  \n",
      "Length: 28510, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "def open_dict(Dict):\n",
    "    path = '%s.txt' % Dict\n",
    "    dictionary = open(path, 'r', encoding='utf-8')\n",
    "    dict = []\n",
    "    for word in dictionary:\n",
    "        word = word.strip('\\n')\n",
    "        dict.append(word)\n",
    "    return dict\n",
    "\n",
    "clearwords=open_dict('clearwords')\n",
    "\n",
    "def chinese_word_cut(mytext):\n",
    "    tempcut = jieba.cut(str(mytext))\n",
    "    result = \" \".join(set(tempcut) - set(clearwords))\n",
    "    return result if result.strip() else np.nan\n",
    "\n",
    "print(df_total)\n",
    "df_total[\"content_cutted\"] = df_total.apply(chinese_word_cut)\n",
    "n_features = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5bbf118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        漫步     ： get 𝗹 见到 🩰 启动 words 今日 小粥 在 🏕 周记 𝗩 s ...\n",
      "1            vlog 了 mini 好看 的 ️ 妆容 新 买 水泥 🏻 ♀ 美 很 尊 微博变...\n",
      "2        品类 心头 混干 过程 🧾 在 可 🥹 马上 是 炼成 一路 100 咯 宝 爱 南得 这样...\n",
      "3          感谢   𝐜 到 🥰 奚妍 、 老朋友 [ 听歌 专业 在 𝐀 真的 也 双 和 宇恒尊...\n",
      "4        天花板 住     千金 出游 了 太美 日记 拿捏 学姐 真的 富家 宅家 整体 清新 💎...\n",
      "                               ...                        \n",
      "28505        在 王鹤 QQ 的 说 吴磊 王俊凯 处处 都 棣 爱 只有 迷妹 还 ， 娱乐 别...\n",
      "28506        ： 了 蝴蝶 真的 也 吧 吗 发声 的 说 人为 猪猪 太宠福 没有 迷妹 ？ 爷...\n",
      "28507        于 扒 出关 吗 是 按 相信 的 辈分 说 关晓彤 于谦 谦 姑姑 迷妹 ？ ， ...\n",
      "28508        马嘉祺唱 了 听 真的 马嘉祺 的 反应 说 那英演 太 那英 迷妹 ， 好听 娱乐...\n",
      "28509        晕 了 真的 的 说 看 张云龙 气 把 迷妹 哈哈哈 娱乐 你们 龙哥 笑 ！ 我 哈哈\n",
      "Length: 28510, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print(df_total[\"content_cutted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "939d4ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                 🥂 ✨ 𝗩𝗹𝗼𝗴  ‣  周记77 粥觉的生日特辑  ✨🥂                 ...\n",
      "1                 mini vlog 🧏🏻♀️用了新买的水泥盘  尊的很好看！#微博变美手册##妆容# 一番f...\n",
      "2                 【2022·年度爱用物🏆彩妆篇】超长25min完整版！20大品类！100+件爱用精选！一路常...\n",
      "3                 又双叒和𝐄𝐒𝐓𝐄𝐄 𝐋𝐀𝐔𝐃𝐄𝐑&𝐜𝐝𝐟老朋友们合作啦宇恒尊的太可爱啦 最后能够现场听歌真的...\n",
      "4                 MAKEUP|#泰式千金气质拿捏住了#宝贝们，泰版流星花园都看了吗，简直就是偶像剧的颜值天花...\n",
      "                                        ...                        \n",
      "28506                        爷爷也太宠福猪猪了吧！蝴蝶：真的没有人为我发声吗？ 迷妹说娱乐的微博视频  \n",
      "28507                 #关晓彤##于谦# 有网友扒出关晓彤按辈分是于谦姑姑，你相信吗？ 迷妹说娱乐的微博视频  \n",
      "28508              #那英##马嘉祺# 那英演我听马嘉祺唱歌的反应，马嘉祺唱的真的太好听了 迷妹说娱乐的微博视频  \n",
      "28509                #张云龙##娱乐# 我真的笑晕了！看你们把龙哥气的哈哈哈哈哈！！！ 迷妹说娱乐的微博视频  \n",
      "content_cutted    0        漫步     ： get 𝗹 见到 🩰 启动 words 今日 小粥 在 ...\n",
      "Length: 28511, dtype: object\n",
      "0        漫步     ： get 𝗹 见到 🩰 启动 words 今日 小粥 在 🏕 周记 𝗩 s ...\n",
      "1            vlog 了 mini 好看 的 ️ 妆容 新 买 水泥 🏻 ♀ 美 很 尊 微博变...\n",
      "2        品类 心头 混干 过程 🧾 在 可 🥹 马上 是 炼成 一路 100 咯 宝 爱 南得 这样...\n",
      "3          感谢   𝐜 到 🥰 奚妍 、 老朋友 [ 听歌 专业 在 𝐀 真的 也 双 和 宇恒尊...\n",
      "4        天花板 住     千金 出游 了 太美 日记 拿捏 学姐 真的 富家 宅家 整体 清新 💎...\n",
      "                               ...                        \n",
      "28505        在 王鹤 QQ 的 说 吴磊 王俊凯 处处 都 棣 爱 只有 迷妹 还 ， 娱乐 别...\n",
      "28506        ： 了 蝴蝶 真的 也 吧 吗 发声 的 说 人为 猪猪 太宠福 没有 迷妹 ？ 爷...\n",
      "28507        于 扒 出关 吗 是 按 相信 的 辈分 说 关晓彤 于谦 谦 姑姑 迷妹 ？ ， ...\n",
      "28508        马嘉祺唱 了 听 真的 马嘉祺 的 反应 说 那英演 太 那英 迷妹 ， 好听 娱乐...\n",
      "28509        晕 了 真的 的 说 看 张云龙 气 把 迷妹 哈哈哈 娱乐 你们 龙哥 笑 ！ 我 哈哈\n",
      "Length: 28508, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "df_total[\"content_cutted\"] = df_total[\"content_cutted\"].dropna()\n",
    "print(df_total)\n",
    "print(df_total[\"content_cutted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7af25525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "INFO:lda:n_documents: 28508\n",
      "INFO:lda:vocab_size: 5000\n",
      "INFO:lda:n_words: 706651\n",
      "INFO:lda:n_topics: 17\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -8386009\n",
      "INFO:lda:<10> log likelihood: -6343913\n",
      "INFO:lda:<20> log likelihood: -5994649\n",
      "INFO:lda:<30> log likelihood: -5876768\n",
      "INFO:lda:<40> log likelihood: -5817961\n",
      "INFO:lda:<50> log likelihood: -5786905\n",
      "INFO:lda:<60> log likelihood: -5763460\n",
      "INFO:lda:<70> log likelihood: -5747918\n",
      "INFO:lda:<80> log likelihood: -5734905\n",
      "INFO:lda:<90> log likelihood: -5725692\n",
      "INFO:lda:<99> log likelihood: -5718605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model done\n",
      "shape: (17, 5000)\n",
      "['00' '01' '02']\n",
      "[[1.96610436e-07 1.96610436e-07 1.96610436e-07]\n",
      " [2.16788068e-07 2.16788068e-07 2.16788068e-07]\n",
      " [1.53808293e-07 1.53808293e-07 1.53808293e-07]\n",
      " [3.65603978e-07 3.65603978e-07 3.65603978e-07]\n",
      " [2.73920070e-07 2.73920070e-07 2.73920070e-07]\n",
      " [3.74942222e-03 8.73350110e-04 2.56792152e-07]\n",
      " [2.41551728e-07 2.41551728e-07 2.41551728e-07]\n",
      " [2.14859697e-07 1.50616647e-04 3.65476344e-04]\n",
      " [2.71127620e-07 1.70837513e-03 1.16611989e-03]\n",
      " [2.83446712e-07 2.83446712e-07 2.83446712e-07]\n",
      " [1.81097086e-07 1.81097086e-07 1.81097086e-07]\n",
      " [2.33765019e-07 1.63869279e-04 2.33765019e-07]\n",
      " [5.50007635e-04 3.05390136e-07 3.05390136e-07]\n",
      " [8.94632470e-03 4.40683941e-07 3.52987837e-04]\n",
      " [4.21834135e-07 4.21834135e-07 4.21834135e-07]\n",
      " [3.80546465e-07 3.80546465e-07 3.80546465e-07]\n",
      " [1.26307280e-07 1.26307280e-07 1.26307280e-07]]\n",
      "topic: 0 sum: 0.9999999999999415\n",
      "topic: 1 sum: 0.9999999999999214\n",
      "topic: 2 sum: 0.999999999999953\n",
      "topic: 3 sum: 0.9999999999999566\n",
      "topic: 4 sum: 0.9999999999999302\n",
      "topic: 5 sum: 0.9999999999999968\n",
      "topic: 6 sum: 0.9999999999999509\n",
      "topic: 7 sum: 0.9999999999999897\n",
      "topic: 8 sum: 1.0000000000000364\n",
      "topic: 9 sum: 0.9999999999999136\n",
      "topic: 10 sum: 0.9999999999999251\n",
      "topic: 11 sum: 1.0000000000000937\n",
      "*Topic 0\n",
      "- 体验 这次 可以 一起 还有 我们 系列 品牌 设计 感受 大家 链接 打卡 真的 免税 活动 全新 现场 非常 惊喜\n",
      "*Topic 1\n",
      "- 真的 这个 头发 可以 感觉 就是 而且 最近 你们 还是 但是 姐妹 起来 喜欢 时候 觉得 每次 很多 大家 现在\n",
      "*Topic 2\n",
      "- 皮肤 精华 保湿 质地 护肤 清爽 真的 不会 效果 可以 使用 适合 敏感 这个 非常 就是 温和 吸收 油皮 防晒\n",
      "*Topic 3\n",
      "- 娱乐 迷妹 真的 这个 可爱 喜欢 今天 哈哈哈 老师 超级 好看 现场 这么 什么 就是 可以 没有 你们 终于 自己\n",
      "*Topic 4\n",
      "- 香水 味道 浪漫 玫瑰 清新 礼物 香气 心动 喜欢 温柔 限定 香味 适合 花香 约会 氛围 自己 可以 520 木质\n",
      "*Topic 5\n",
      "- 链接 京东 还有 天猫 11 一起 福利 大牌 超级 618 活动 搜索 超多 赶紧 现在 20 优惠 直播间 直播 30\n",
      "*Topic 6\n",
      "- plog 生活 vlog 日常 快乐 日记 今天 ootd 碎片 记录 王国 潼话 博主 今日 最近 春天 周末 夏日 显示 地图\n",
      "*Topic 7\n",
      "- 妆容 今日 氛围 口红 美妆 真的 今天 眼影 搭配 温柔 颜色 日常 ootd 高级 分享 look 复古 适合 春天 好看\n",
      "*Topic 8\n",
      "- 底妆 这个 真的 粉底液 不会 自然 就是 可以 感觉 粉底 而且 持妆 定妆 出门 细腻 服帖 喜欢 适合 持久 最近\n",
      "*Topic 9\n",
      "- 分享 好物 开箱 品牌 礼盒 快乐 美妆 感谢 最近 购物 彩妆 礼物 大会 种草 收到 双十 你们 近期 大家 推荐\n",
      "*Topic 10\n",
      "- 自己 我们 没有 不是 就是 可以 知道 什么 希望 这个 还是 觉得 真的 一直 因为 如果 很多 现在 时候 一定\n",
      "*Topic 11\n",
      "- 护肤 分享 好物 今天 皮肤 换季 推荐 大家 美妆 精华 护理 姐妹 夏日 产品 种草 真的 如何 身体 宝藏 你们\n",
      "*Topic 12\n",
      "- 美妆 化妆 变美 今天 妆容 分享 姐妹 攻略 如何 教程 真的 春夏 你们 出游 起来 今日 大家 美出 手册 适合\n",
      "*Topic 13\n",
      "- 可以 10 抽奖 露露 90 20 琼琼 大家 chili 19 到手 00 12 活动 黄了 你们 需要 他家 14 不错\n",
      "*Topic 14\n",
      "- 大家 互动 你们 粉丝 评论 宝贝 抽奖 可以 私信 日常 一起 多多 记得 上榜 美妆 继续 感谢 安排 加入 宝子们\n",
      "*Topic 15\n",
      "- 新年 计划 完美 攻略 开春 大家 好颜 过年 一年 豆豆 抽奖 babe 2022 2023 虎年 快乐 一起 你们 时尚 今年\n",
      "*Topic 16\n",
      "- 肌肤 成分 皮肤 修护 抗老 状态 精华 问题 可以 熬夜 效果 护肤 紧致 我们 吸收 坚持 使用 改善 真的 里面\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(df_total[\"content_cutted\"])\n",
    "vocab=tf_vectorizer.get_feature_names_out()\n",
    "model = lda.LDA(n_topics=17, n_iter=100, random_state=1)  \n",
    "model.fit(tf)\n",
    "print('model done')\n",
    "\n",
    "#主题-单词（topic-word）分布\n",
    "topic_word = model.topic_word_ \n",
    "print(\"shape: {}\".format(topic_word.shape))\n",
    "print(vocab[:3])\n",
    "print(topic_word[:, :3])\n",
    "for n in range(12):\n",
    "    sum_pr = sum(topic_word[n,:])  \n",
    "    print(\"topic: {} sum: {}\".format(n, sum_pr))\n",
    "\n",
    "#计算各主题Top-N个单词\n",
    "import numpy as np\n",
    "n = 20\n",
    "for i, topic_dist in enumerate(topic_word):  \n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]  \n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45280688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "INFO:lda:n_documents: 28508\n",
      "INFO:lda:vocab_size: 5000\n",
      "INFO:lda:n_words: 706651\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -7757834\n",
      "INFO:lda:<10> log likelihood: -6223251\n",
      "INFO:lda:<20> log likelihood: -5919141\n",
      "INFO:lda:<30> log likelihood: -5809524\n",
      "INFO:lda:<40> log likelihood: -5759056\n",
      "INFO:lda:<50> log likelihood: -5731843\n",
      "INFO:lda:<60> log likelihood: -5714403\n",
      "INFO:lda:<70> log likelihood: -5702616\n",
      "INFO:lda:<80> log likelihood: -5694737\n",
      "INFO:lda:<90> log likelihood: -5687317\n",
      "INFO:lda:<99> log likelihood: -5680467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model done\n",
      "shape: (10, 5000)\n",
      "['00' '01' '02']\n",
      "[[1.60986525e-07 1.60986525e-07 1.60986525e-07]\n",
      " [1.22593445e-04 7.96183804e-04 7.47195415e-04]\n",
      " [9.05854538e-08 1.72202948e-04 9.05854538e-08]\n",
      " [6.34142774e-04 2.43807295e-07 2.43807295e-07]\n",
      " [1.35422450e-07 1.35422450e-07 1.35422450e-07]\n",
      " [1.41432713e-07 1.41432713e-07 1.41432713e-07]\n",
      " [1.14987466e-07 1.14987466e-07 1.14987466e-07]\n",
      " [2.24587881e-07 2.24587881e-07 1.12518529e-04]\n",
      " [1.56899663e-07 1.56899663e-07 1.56899663e-07]\n",
      " [4.58475304e-03 3.74110086e-04 2.78401064e-05]]\n",
      "topic: 0 sum: 1.0000000000000329\n",
      "topic: 1 sum: 0.9999999999999639\n",
      "topic: 2 sum: 1.0000000000000349\n",
      "topic: 3 sum: 0.9999999999999586\n",
      "topic: 4 sum: 1.0000000000000469\n",
      "topic: 5 sum: 1.0000000000000724\n",
      "topic: 6 sum: 0.9999999999999469\n",
      "topic: 7 sum: 1.000000000000122\n",
      "topic: 8 sum: 1.0000000000000762\n",
      "topic: 9 sum: 1.000000000000102\n",
      "*Topic 0\n",
      "- 香水 味道 头发 可以 就是 喜欢 玫瑰 高级 感觉 真的\n",
      "*Topic 1\n",
      "- 妆容 美妆 真的 今日 今天 口红 氛围 这个 眼影 好看\n",
      "*Topic 2\n",
      "- 真的 可以 皮肤 而且 这个 效果 感觉 质地 不会 就是\n",
      "*Topic 3\n",
      "- 新年 计划 完美 大家 攻略 开春 好颜 王国 潼话 你们\n",
      "*Topic 4\n",
      "- 娱乐 迷妹 真的 自己 这个 就是 没有 什么 还是 觉得\n",
      "*Topic 5\n",
      "- 分享 好物 护肤 今天 种草 美妆 推荐 最近 大家 你们\n",
      "*Topic 6\n",
      "- 肌肤 护肤 精华 抗老 修护 皮肤 成分 状态 我们 紧致\n",
      "*Topic 7\n",
      "- 礼物 礼盒 品牌 快乐 感谢 收到 大家 分享 开箱 一起\n",
      "*Topic 8\n",
      "- 生活 plog ootd vlog 日常 今天 快乐 日记 今日 碎片\n",
      "*Topic 9\n",
      "- 链接 还有 可以 一起 京东 活动 天猫 20 11 这次\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(df_total[\"content_cutted\"])\n",
    "vocab=tf_vectorizer.get_feature_names_out()\n",
    "model = lda.LDA(n_topics=10, n_iter=100, random_state=1)  \n",
    "model.fit(tf)\n",
    "print('model done')\n",
    "\n",
    "#主题-单词（topic-word）分布\n",
    "topic_word = model.topic_word_ \n",
    "print(\"shape: {}\".format(topic_word.shape))\n",
    "print(vocab[:3])\n",
    "print(topic_word[:, :3])\n",
    "for n in range(10):\n",
    "    sum_pr = sum(topic_word[n,:])  \n",
    "    print(\"topic: {} sum: {}\".format(n, sum_pr))\n",
    "\n",
    "#计算各主题Top-N个单词\n",
    "import numpy as np\n",
    "n = 10\n",
    "for i, topic_dist in enumerate(topic_word):  \n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]  \n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d6b87a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# 周小仙yoo的微博视频', '# 周小仙yoo的微博', '#周小仙yoo的微博视频', '#周小仙yoo的微博', '周小仙yoo', '# 张进ZJZJ的微博视频', '# 张进ZJZJ的微博', '#张进ZJZJ的微博视频', '#张进ZJZJ的微博', '张进ZJZJ', '# 陈大皮儿的微博视频', '# 陈大皮儿的微博', '#陈大皮儿的微博视频', '#陈大皮儿的微博', '陈大皮儿', '# 马锐的微博视频', '# 马锐的微博', '#马锐的微博视频', '#马锐的微博', '马锐', '# Kiko成晨的微博视频', '# Kiko成晨的微博', '#Kiko成晨的微博视频', '#Kiko成晨的微博', 'Kiko成晨', '# 彤彤_sakura的微博视频', '# 彤彤_sakura的微博', '#彤彤_sakura的微博视频', '#彤彤_sakura的微博', '彤彤_sakura', '# 手边巴黎urruolan的微博视频', '# 手边巴黎urruolan的微博', '#手边巴黎urruolan的微博视频', '#手边巴黎urruolan的微博', '手边巴黎urruolan', '# 叫我大表哥好吗好的的微博视频', '# 叫我大表哥好吗好的的微博', '#叫我大表哥好吗好的的微博视频', '#叫我大表哥好吗好的的微博', '叫我大表哥好吗好的', '# 牛明昱的微博视频', '# 牛明昱的微博', '#牛明昱的微博视频', '#牛明昱的微博', '牛明昱', '# LookLana的微博视频', '# LookLana的微博', '#LookLana的微博视频', '#LookLana的微博', 'LookLana', '# 孙一玮Well的微博视频', '# 孙一玮Well的微博', '#孙一玮Well的微博视频', '#孙一玮Well的微博', '孙一玮Well', '# Hedy北北的微博视频', '# Hedy北北的微博', '#Hedy北北的微博视频', '#Hedy北北的微博', 'Hedy北北', '# Ruby幼熙的微博视频', '# Ruby幼熙的微博', '#Ruby幼熙的微博视频', '#Ruby幼熙的微博', 'Ruby幼熙', '# 蒲一雯iwen的微博视频', '# 蒲一雯iwen的微博', '#蒲一雯iwen的微博视频', '#蒲一雯iwen的微博', '蒲一雯iwen', '# 美少女毛容易的微博视频', '# 美少女毛容易的微博', '#美少女毛容易的微博视频', '#美少女毛容易的微博', '美少女毛容易', '# Joey土播鼠的微博视频', '# Joey土播鼠的微博', '#Joey土播鼠的微博视频', '#Joey土播鼠的微博', 'Joey土播鼠', '# 章解放_的微博视频', '# 章解放_的微博', '#章解放_的微博视频', '#章解放_的微博', '章解放_', '# 一枝南南的微博视频', '# 一枝南南的微博', '#一枝南南的微博视频', '#一枝南南的微博', '一枝南南', '# 潘白雪s的微博视频', '# 潘白雪s的微博', '#潘白雪s的微博视频', '#潘白雪s的微博', '潘白雪s', '# 一番fane的微博视频', '# 一番fane的微博', '#一番fane的微博视频', '#一番fane的微博', '一番fane', '# 美少女Lisa酱的微博视频', '# 美少女Lisa酱的微博', '#美少女Lisa酱的微博视频', '#美少女Lisa酱的微博', '美少女Lisa酱', '# 潘南竹呀的微博视频', '# 潘南竹呀的微博', '#潘南竹呀的微博视频', '#潘南竹呀的微博', '潘南竹呀', '# 小考拉Lake的微博视频', '# 小考拉Lake的微博', '#小考拉Lake的微博视频', '#小考拉Lake的微博', '小考拉Lake', '# 周姊伊的微博视频', '# 周姊伊的微博', '#周姊伊的微博视频', '#周姊伊的微博', '周姊伊', '# 种草达人绵绵酱的微博视频', '# 种草达人绵绵酱的微博', '#种草达人绵绵酱的微博视频', '#种草达人绵绵酱的微博', '种草达人绵绵酱', '# Yuki_优酱的微博视频', '# Yuki_优酱的微博', '#Yuki_优酱的微博视频', '#Yuki_优酱的微博', 'Yuki_优酱', '# 兰阿雯的微博视频', '# 兰阿雯的微博', '#兰阿雯的微博视频', '#兰阿雯的微博', '兰阿雯', '# 优莉Uli的微博视频', '# 优莉Uli的微博', '#优莉Uli的微博视频', '#优莉Uli的微博', '优莉Uli', '# Echo桃小小的微博视频', '# Echo桃小小的微博', '#Echo桃小小的微博视频', '#Echo桃小小的微博', 'Echo桃小小', '# 呦呦仔的微博视频', '# 呦呦仔的微博', '#呦呦仔的微博视频', '#呦呦仔的微博', '呦呦仔', '# 宋素雯的微博视频', '# 宋素雯的微博', '#宋素雯的微博视频', '#宋素雯的微博', '宋素雯', '# 月野皮皮的微博视频', '# 月野皮皮的微博', '#月野皮皮的微博视频', '#月野皮皮的微博', '月野皮皮', '# 美硕的成分测评的微博视频', '# 美硕的成分测评的微博', '#美硕的成分测评的微博视频', '#美硕的成分测评的微博', '美硕的成分测评', '# 桃子百莉的微博视频', '# 桃子百莉的微博', '#桃子百莉的微博视频', '#桃子百莉的微博', '桃子百莉', '# 苍口小梨涡的微博视频', '# 苍口小梨涡的微博', '#苍口小梨涡的微博视频', '#苍口小梨涡的微博', '苍口小梨涡', '# Fairy小默的微博视频', '# Fairy小默的微博', '#Fairy小默的微博视频', '#Fairy小默的微博', 'Fairy小默', '# 莓灵酱的微博视频', '# 莓灵酱的微博', '#莓灵酱的微博视频', '#莓灵酱的微博', '莓灵酱', '# 你Rui哥的微博视频', '# 你Rui哥的微博', '#你Rui哥的微博视频', '#你Rui哥的微博', '你Rui哥', '# 刘魔王大人-的微博视频', '# 刘魔王大人-的微博', '#刘魔王大人-的微博视频', '#刘魔王大人-的微博', '刘魔王大人-', '# 蟹阿文AWEN的微博视频', '# 蟹阿文AWEN的微博', '#蟹阿文AWEN的微博视频', '#蟹阿文AWEN的微博', '蟹阿文AWEN', '# 陈端端儿的微博视频', '# 陈端端儿的微博', '#陈端端儿的微博视频', '#陈端端儿的微博', '陈端端儿', '# 迟池Chichi的微博视频', '# 迟池Chichi的微博', '#迟池Chichi的微博视频', '#迟池Chichi的微博', '迟池Chichi', '# 叫我桃maymay的微博视频', '# 叫我桃maymay的微博', '#叫我桃maymay的微博视频', '#叫我桃maymay的微博', '叫我桃maymay', '# 种草颜究生的微博视频', '# 种草颜究生的微博', '#种草颜究生的微博视频', '#种草颜究生的微博', '种草颜究生']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "blogger_names = [\n",
    "\"周小仙yoo\", \"张进ZJZJ\", \"陈大皮儿\", \"马锐\", \n",
    "\"Kiko成晨\", \"彤彤_sakura\",\"手边巴黎urruolan\", \"叫我大表哥好吗好的\", \n",
    "\"牛明昱\", \"LookLana\",\"孙一玮Well\", \"Hedy北北\", \n",
    "\"Ruby幼熙\",\"蒲一雯iwen\", \"美少女毛容易\", \"Joey土播鼠\", \n",
    "\"章解放_\", \"一枝南南\", \"潘白雪s\",\"一番fane\",\n",
    "\"美少女Lisa酱\",\"潘南竹呀\",\"小考拉Lake\", \"周姊伊\",\n",
    "\"种草达人绵绵酱\", \"Yuki_优酱\", \"兰阿雯\", \"优莉Uli\",\n",
    "\"Echo桃小小\", \"呦呦仔\", \"宋素雯\", \"月野皮皮\", \n",
    "\"美硕的成分测评\", \"桃子百莉\",\"苍口小梨涡\", \"Fairy小默\", \n",
    "\"莓灵酱\", \"你Rui哥\", \"刘魔王大人-\", \"蟹阿文AWEN\",\n",
    "\"陈端端儿\", \"迟池Chichi\", \"叫我桃maymay\", \"种草颜究生\"\n",
    "]\n",
    "\n",
    "self_references = []\n",
    "\n",
    "for name in blogger_names:\n",
    "    reference1 = f\"# {name}的微博视频\"\n",
    "    reference2 = f\"# {name}的微博\"\n",
    "    reference3 = f\"#{name}的微博视频\"\n",
    "    reference4 = f\"#{name}的微博\"\n",
    "    reference5 = name\n",
    "    self_references.append(reference1)\n",
    "    self_references.append(reference2)\n",
    "    self_references.append(reference3)\n",
    "    self_references.append(reference4)\n",
    "    self_references.append(reference5)\n",
    "\n",
    "blogger_names_variants = [\n",
    "    \"周小仙yoo\", \"周小仙\", \"yoo\",\n",
    "    \"张进ZJZJ\", \"张进\", \"ZJZJ\", \"zjzj\",\n",
    "    \"陈大皮儿\",\"陈大皮\"\n",
    "    \"马锐\",\n",
    "    \n",
    "    \"Kiko成晨\", \"Kiko\", \"成晨\",\n",
    "    \"彤彤_sakura\", \"彤彤\", \"sakura\",\n",
    "    \"手边巴黎urruolan\", \"手边巴黎\", \"urruolan\",\n",
    "    \"叫我大表哥好吗好的\",\n",
    "    \n",
    "    \"牛明昱\",\n",
    "    \"LookLana\",\n",
    "    \"孙一玮Well\", \"孙一玮\", \"Well\",\"well\",\n",
    "    \"Hedy北北\", \"Hedy\", \"北北\",\"hedy\",\n",
    "    \n",
    "    \"Ruby幼熙\", \"Ruby\", \"幼熙\",\"ruby\",\n",
    "    \"蒲一雯iwen\", \"蒲一雯\", \"iwen\",\n",
    "    \"美少女毛容易\", \"毛容易\",\n",
    "    \"Joey土播鼠\", \"Joey\",\"joey\",\n",
    "    \n",
    "    \"章解放_\", \"章解放\",\n",
    "    \"一枝南南\", \"南南\", \"一枝\",\n",
    "    \"潘白雪s\", \"潘白雪\",\n",
    "    \"一番fane\",\"fane\",\"一番\",\n",
    "    \n",
    "    \"美少女Lisa酱\",\"Lisa酱\",\n",
    "    \"潘南竹呀\",\"潘南竹\",\"南竹\",\n",
    "    \"小考拉Lake\", \"小考拉\", \"Lake\",\"lake\",\n",
    "    \"周姊伊\",\n",
    "    \n",
    "    \"种草达人绵绵酱\", \"绵绵酱\",\n",
    "    \"Yuki_优酱\", \"Yuki\", \"优酱\",\n",
    "    \"兰阿雯\",\n",
    "    \"优莉Uli\", \"优莉\", \"Uli\",\"uli\",\n",
    "    \n",
    "    \"Echo桃小小\", \"Echo\", \"桃小小\",\"echo\",\n",
    "    \"呦呦仔\",\n",
    "    \"宋素雯\",\n",
    "    \"月野皮皮\", \"皮皮\", \"月野\",\n",
    "    \n",
    "    \"美硕的成分测评\", \"美硕\",\n",
    "    \"桃子百莉\", \"百莉\",\n",
    "    \"苍口小梨涡\",\n",
    "    \"Fairy小默\", \"Fairy\", \"小默\",\"fairy\",\n",
    "    \n",
    "    \"莓灵酱\",\n",
    "    \"你Rui哥\",\n",
    "    \"刘魔王大人-\",\n",
    "    \"蟹阿文AWEN\", \"蟹阿文\", \"AWEN\",\"awen\",\n",
    "    \n",
    "    \"陈端端儿\",\n",
    "    \"迟池Chichi\", \"迟池\", \"Chichi\",\"chi\",\"Chi\",\n",
    "    \"叫我桃maymay\", \"maymay\", \"may\",\"May\",\n",
    "    \"种草颜究生\"\n",
    "]\n",
    "\n",
    "\n",
    "print(self_references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "45a0b5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#周姊伊的微博', '#Fairy小默的微博视频', '# 彤彤_sakura的微博', '# Ruby幼熙的微博', '# 牛明昱的微博', '# 刘魔王大人-的微博视频', '蒲一雯iwen', '# 苍口小梨涡的微博', '#迟池Chichi的微博视频', '潘南竹', '# 章解放_的微博', '# Yuki_优酱的微博视频', '# 一番fane的微博视频', '#苍口小梨涡的微博', '# 桃子百莉的微博', 'Kiko成晨', '手边巴黎', '你Rui哥', '月野皮皮', '#LookLana的微博视频', '美少女毛容易', '潘南竹呀', '优莉Uli', '#刘魔王大人-的微博视频', '#张进ZJZJ的微博', '#莓灵酱的微博视频', 'maymay', '#手边巴黎urruolan的微博视频', '小考拉', 'zjzj', '周小仙yoo', '# Hedy北北的微博', '# Fairy小默的微博视频', '# 你Rui哥的微博视频', '#宋素雯的微博', '桃子百莉', '迟池Chichi', '#一番fane的微博视频', '#宋素雯的微博视频', '#蒲一雯iwen的微博视频', '# 张进ZJZJ的微博', '# 一枝南南的微博', '#LookLana的微博', '#月野皮皮的微博', '张进', '孙一玮', 'Kiko', '# 呦呦仔的微博视频', '彤彤_sakura', '#美少女Lisa酱的微博视频', '一枝南南', '#Kiko成晨的微博视频', '# LookLana的微博视频', 'Yuki_优酱', '#马锐的微博', '#潘南竹呀的微博视频', '#种草颜究生的微博视频', '宋素雯', '马锐', 'Echo桃小小', '皮皮', '# 美少女毛容易的微博视频', '潘白雪s', 'ruby', '# 陈大皮儿的微博', '#Fairy小默的微博', '#蒲一雯iwen的微博', '#周小仙yoo的微博', '苍口小梨涡', '# 兰阿雯的微博视频', '#陈端端儿的微博', '# 陈端端儿的微博', '# 月野皮皮的微博', 'LookLana', '#刘魔王大人-的微博', '#苍口小梨涡的微博视频', '#章解放_的微博视频', '#彤彤_sakura的微博', '#月野皮皮的微博视频', '迟池', 'Lisa酱', '#陈端端儿的微博视频', 'Echo', '#兰阿雯的微博视频', '牛明昱', '# 美硕的成分测评的微博', '# 种草颜究生的微博', 'well', 'chi', '#桃子百莉的微博视频', 'Ruby', '绵绵酱', '# 马锐的微博', '# 迟池Chichi的微博', '# 优莉Uli的微博视频', '章解放', '# 叫我大表哥好吗好的的微博视频', '# 叫我大表哥好吗好的的微博', 'Hedy', '# 一枝南南的微博视频', '# 潘白雪s的微博', 'Fairy小默', '#优莉Uli的微博视频', 'may', '成晨', '# 宋素雯的微博', '# Joey土播鼠的微博视频', '#美硕的成分测评的微博视频', '# 月野皮皮的微博视频', '#种草达人绵绵酱的微博', 'echo', 'Fairy', '# 美少女Lisa酱的微博视频', '#叫我桃maymay的微博视频', '桃小小', '# 优莉Uli的微博', '小默', 'Uli', '#莓灵酱的微博', '优莉', '章解放_', '孙一玮Well', '# 潘南竹呀的微博', '# 牛明昱的微博视频', '#美少女毛容易的微博视频', 'yoo', '蟹阿文', '#周姊伊的微博视频', '# 叫我桃maymay的微博', '#种草达人绵绵酱的微博视频', '# 种草颜究生的微博视频', '叫我桃maymay', 'Well', '南南', 'uli', '#手边巴黎urruolan的微博', '陈端端儿', '#孙一玮Well的微博', '# 蟹阿文AWEN的微博', '一枝', '# 呦呦仔的微博', '#美少女毛容易的微博', '#陈大皮儿的微博', '#潘白雪s的微博视频', '#呦呦仔的微博视频', '#马锐的微博视频', '#彤彤_sakura的微博视频', '种草达人绵绵酱', '#Yuki_优酱的微博', '#兰阿雯的微博', '叫我大表哥好吗好的', '#Echo桃小小的微博视频', '# Kiko成晨的微博', '#你Rui哥的微博', '蟹阿文AWEN', '# 迟池Chichi的微博视频', '#迟池Chichi的微博', 'awen', '美硕', '#美少女Lisa酱的微博', '# 潘南竹呀的微博视频', '莓灵酱', '# Joey土播鼠的微博', '#潘南竹呀的微博', '# Yuki_优酱的微博', '#Joey土播鼠的微博视频', '# 陈大皮儿的微博视频', '种草颜究生', '彤彤', '毛容易', '#牛明昱的微博', '# 周小仙yoo的微博', '美硕的成分测评', '一番', 'Lake', '# 桃子百莉的微博视频', '#陈大皮儿的微博视频', '#叫我大表哥好吗好的的微博视频', '# LookLana的微博', '一番fane', '# 你Rui哥的微博', '# 陈端端儿的微博视频', 'ZJZJ', '#Kiko成晨的微博', '# 彤彤_sakura的微博视频', 'Yuki', '#Ruby幼熙的微博视频', '# 马锐的微博视频', '美少女Lisa酱', '# 潘白雪s的微博视频', '南竹', '#种草颜究生的微博', '# Ruby幼熙的微博视频', '# 小考拉Lake的微博视频', '# 莓灵酱的微博', '# 蟹阿文AWEN的微博视频', '# 周姊伊的微博', '陈大皮马锐', '#Joey土播鼠的微博', '刘魔王大人-', 'iwen', 'urruolan', '蒲一雯', '优酱', '月野', '# 宋素雯的微博视频', '手边巴黎urruolan', '#桃子百莉的微博', '# 叫我桃maymay的微博视频', '陈大皮儿', '潘白雪', '#Echo桃小小的微博', 'Joey土播鼠', 'sakura', '# 孙一玮Well的微博', '# Fairy小默的微博', 'hedy', '#孙一玮Well的微博视频', '#呦呦仔的微博', '#蟹阿文AWEN的微博', '# 周小仙yoo的微博视频', '# 兰阿雯的微博', '#美硕的成分测评的微博', 'Ruby幼熙', '呦呦仔', '#周小仙yoo的微博视频', '#小考拉Lake的微博视频', '# 莓灵酱的微博视频', '# 种草达人绵绵酱的微博视频', '# 苍口小梨涡的微博视频', '# 美少女Lisa酱的微博', '# 小考拉Lake的微博', '小考拉Lake', 'fane', '周姊伊', '# 刘魔王大人-的微博', '# 章解放_的微博视频', 'Chichi', '# Echo桃小小的微博视频', '#小考拉Lake的微博', '张进ZJZJ', 'lake', '# 周姊伊的微博视频', '北北', '# Hedy北北的微博视频', '#一枝南南的微博视频', '#蟹阿文AWEN的微博视频', 'joey', '#张进ZJZJ的微博视频', 'May', '# 蒲一雯iwen的微博', '#叫我大表哥好吗好的的微博', 'fairy', '#一番fane的微博', '# Kiko成晨的微博视频', '# 美少女毛容易的微博', '# 手边巴黎urruolan的微博视频', '# 蒲一雯iwen的微博视频', '#牛明昱的微博视频', '#Yuki_优酱的微博视频', '# 种草达人绵绵酱的微博', '#Hedy北北的微博视频', '#Hedy北北的微博', '#Ruby幼熙的微博', '#章解放_的微博', '# Echo桃小小的微博', '#一枝南南的微博', '# 手边巴黎urruolan的微博', '#你Rui哥的微博视频', '# 张进ZJZJ的微博视频', '幼熙', 'Joey', '百莉', '#潘白雪s的微博', '# 一番fane的微博', '兰阿雯', 'Chi', '# 美硕的成分测评的微博视频', '#优莉Uli的微博', '# 孙一玮Well的微博视频', '#叫我桃maymay的微博', '周小仙', 'Hedy北北', 'AWEN']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "combined_blogger_names = list(set(self_references + blogger_names_variants))\n",
    "print(combined_blogger_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3f0b6e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                 🥂 ✨ 𝗩𝗹𝗼𝗴  ‣  周记77 粥觉的生日特辑  ✨🥂                 ...\n",
      "1                 mini vlog 🧏🏻♀️用了新买的水泥盘  尊的很好看！#微博变美手册##妆容# 一番f...\n",
      "2                 【2022·年度爱用物🏆彩妆篇】超长25min完整版！20大品类！100+件爱用精选！一路常...\n",
      "3                 又双叒和𝐄𝐒𝐓𝐄𝐄 𝐋𝐀𝐔𝐃𝐄𝐑&𝐜𝐝𝐟老朋友们合作啦宇恒尊的太可爱啦 最后能够现场听歌真的...\n",
      "4                 MAKEUP|#泰式千金气质拿捏住了#宝贝们，泰版流星花园都看了吗，简直就是偶像剧的颜值天花...\n",
      "                                        ...                        \n",
      "28506                        爷爷也太宠福猪猪了吧！蝴蝶：真的没有人为我发声吗？ 迷妹说娱乐的微博视频  \n",
      "28507                 #关晓彤##于谦# 有网友扒出关晓彤按辈分是于谦姑姑，你相信吗？ 迷妹说娱乐的微博视频  \n",
      "28508              #那英##马嘉祺# 那英演我听马嘉祺唱歌的反应，马嘉祺唱的真的太好听了 迷妹说娱乐的微博视频  \n",
      "28509                #张云龙##娱乐# 我真的笑晕了！看你们把龙哥气的哈哈哈哈哈！！！ 迷妹说娱乐的微博视频  \n",
      "content_cutted    0        漫步     ： get 𝗹 见到 🩰 启动 words 今日 小粥 在 ...\n",
      "Length: 27041, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print(df_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c9f68207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['周记 粥 觉 生日 特辑 今日 keys words 逃离 城市 计划 启动 小粥 礼物 开箱 晨间 漫步 见到 偶像 甜蜜 夜 即兴 表演 头发 新 皮肤 get 杭 打工 记 妮妮宝 撒娇 日常 下周 见', 'mini vlog 新 买 水泥 盘 尊 好看 微博变 美 手册 妆容', '年度 爱用物 彩妆 篇 超长 min 完整版 品类 件 爱用 精选 一路 常绿 放心 食用 观看 指南 混干 微 瑕 适用 底妆 皮肤 炼成 不卡粉 淡颜 彩妆 心头 宝 一年 彩妆 爱 胞胎 口红 找 不同 一只 真的 纯 白开水 气质 色 变美 过程 神 辅助 化妆 刷 工具 发型 好物 护肤 篇 路上 咯 马上 一枝 南南 南得 花钱 年度 爱 彩妆', '双 叒 老朋友 合作 宇恒尊 太 可爱 最后 能够 现场 听歌 真的 幸福 小可 默契 依然 高 奚妍 老师 太 专业 惹 助理 姐姐 拍 花絮 昨晚 一直 直播间 陪伴 小可爱 谢谢你们 认真 支持 爱护 真的 感谢 全文', 'MAKEUP 泰式 千金 气质 拿捏 住 宝贝 泰版 流星花园 简直 偶像剧 颜值 天花板 气质 温柔 藤堂静 学姐 真的 太美 今天 毛仔 带来 一个 超级 适合 短 圆脸 姐妹 泰式 千金 妆 整体 妆容 特别 干净 高级 完美 呈现 富家 千金 即视感 感兴趣 宝子们 快点 开 微博美妆 春夏 清新 出游 妆 宅家 变美 日记', '置顶 JENNIE 回归 造型 太顶 blackpink 预告 真的 太绝 斗胆 浅仿 一下 仿妆 仿人 上海 分妮 前来 报道 举手 tips 底妆 哑光 高级 粉底液 眼妆 重点 打出 轮廓 感 重点 眼头 眉下 钻石 拉近 眼 间距 修容 重点 下颚 线 打造 小脸 精致 夏日 追凉 体验 夏日 不脱 妆 挑战', '最近 轻薄 羽绒服 风 真的 好大 穿 真的 巨 舒服 摸 蓬松 穿 身上 不会 压重 别看 比较 轻薄 非常 保暖 方便 携带 不会 过多 占 地方 感觉 穿 起来 非常 立体 普通 臃肿 羽绒服 轻薄 羽绒 非常 搭配 里面 搭 简单 白 t 非常 有型 下身 搭配 牛仔裤 半靴 分 分钟 成为 焦点 云顶山 真的 非常 原生态 可爱 小牛 非常 值得 拍照 非常 出片 享受 原生态 带来 快乐 山脚 山顶 非常 美丽 听说 今年 流行 轻薄 羽绒服 日常', '坦白 局 do 高个子 女生 变美 干货 微博变 美 手册 瘦 斤 年 逆 生长 变美逆袭 get 招让 脱胎换骨 幼熙 美妆', '辫子 客服 事件 会 影响 买 苹果 产品 现在 印第安人 辫子 伤害 loser 民族 感情 逼 印第安人 允许 买 华为 放心 苹果 销量 不会 受 影响 热 搜上 破口大骂 可能 买得起 苹果 买不起 华为 热爱 华为 现在 价格 价值 决定 代表 品牌', '吹 爆 ccd 夜晚 拍照 ccd 孙一玮', '置顶 超级 红人节', '置顶 海边 一定 夏天 想 做 事情 分 时间', '今天 生日 月 日 祝福', '置顶 社牛 女孩 养成 朋友 说 社牛 其实 社牛 属性 源于 高能量 待人 真诚 热爱 自由 洒脱 享受 生命 一个 瞬间 相信 生命 中 发生 一切都是 吸引 微博热 推 大赛 超 热爱 计划 抽奖 详情', 'Golden hour 美妆 今日 妆容', '年 拍 杂志 手机 翻出 杂志 照片 青春 谢谢 一路 走来 陪伴 走过 多年 ping 抽 三个 宝贝 喝 奶茶 Kiko 成晨 微博 新潮 计划', '身体 护理 好物 分享 姐妹 身体 头皮 护理 面部 重要 从头到脚 不能 放过 最近 一个月 户外 暴晒 身体 美白 头皮 修护 更是 一个 工程 特别 sesderma 身体 乳帮 大忙 今天 分享 陪 一路 披荆斩棘 宝贝 好物 分享 身体 护理', '', '张小斐 美 图 化妆 进发 型 StanLee 啸天 JIN STUDIO JIN STUDIO 张小斐 低 马尾', '置顶 心里 住 风 今日 妆容 学 化妆 妆容 变化 春日 出游 妆', '看着 不太直', 'Vlog 美丽 项目 注意事项 光子 初体验 变 美美 微博变 美 手册 护肤', '云之羽 原班 造型 团队 虞书 欣云 为衫妆造 还原 虞书欣 本剧 御用 化妆师 侍慧 Vicky 揭秘 衫 姐妆容 小心 机 剧组 封存 N年 戏服 原款 上身 大师级 底妆 秘籍 还原 美貌 欣欣 call 剧粉 圆梦 横店 快 一起 追云之羽 吼吼 仿妆 微博变 美 手册', '前方 高能 来袭 解放 全新 身份 牌来 直播 月 日晚 锁定 cdf 海口 日月 广场 免税店 应援 团团长 点唱 夜 特邀嘉宾 吴宇恒 不无 语 一起 黛 一场 大自然 共眠 沉浸 式 音乐 主题 直播 星空 点唱 夜 初秋 夜晚 分享 旅行 秋日 睡前 好物 全文', 'SHARING 毛仔 购物 分享 起来 开箱 看看 最近 入手 东西 微博变 美 手册 种草', '雨果 说 丑 美的 旁边 畸形 靠近 优美 放下 美的 成见 美 丑 定义 PRADA 思辨 美学 反对 美的 成见 拒绝 标签 定义 美 成见 PRADA 美妆', '超 喜欢 体育明星 谷爱凌 米兰 中国 品牌 走秀 这波 真是 赢麻 运动员 跨界 时尚界 浑身 散发 健康 活力 魅力四射 谷爱凌 爱 啊啊啊 谷爱凌 波司登 走秀 想法', '拜托 秋叶原 v 幼熙 美妆 幼熙 穿 搭 KNWLS CHANEL MIUMIU', '每日 一香 在野 南杂 银雾 弧光 肩膀 天鹅绒 爱尔兰 绿 花呢 白 帆布 绸缎 丹宁 布 毛 哔叽 雪纺纱 想 久 知道 到底 比喻 回忆起 几年 前 一起 跳舞 独自 舞台 露出 一段 腰 上面 星星点点 闪光 不知 汗水 洒 银粉 台下 有人 窃窃私语 说 骚货 竟 从未 摸过 肩 骚货 赵 碎碎的 耳环 手里 很久很久 仍然 手里 曾经 月光 丰盈 时看 宝光 灿烂 空泛 无稽 廉价 闪耀 东西 光芒 越盛 越 心虚 装进 盒子 里 锁到 柜子 里 仿佛 赵碎 碎 锁 起来 不再 离开 卑微 那次 跳舞 赵碎 碎 耳环 摘下来 交给 帮 看着 好不好 说完 路文 闯进 舞池 一会 孟昭 一起 跳 后来 认识 身边 飞来飞去 面颊 红粉 菲菲 镁光灯 扑朔迷离 一种 蝴蝶 颜色 夏季 死去 比荼 蘼 早 身上 一种 香气 说 爱 香水 银雾 弧光 开头 烈酒 快醉 焕发 出 致命 甜蜜 贴 耳朵 下面 一点一点 蒸发 开来 盛大 磅礴 生死 情仇 懂 蝴蝶 坠落 下去 了无痕迹 开 好多 酒 赵碎 碎 喜欢 喝 喝 最后 会 笑 好笑 笑 起来 好美 周围 跟着 一起 笑 顾左右而言他 魑魅魍魉 站 中间 不知所谓 知道 保管 耳环 小小 一对 银色 手里 握久 留下 朦胧 不明 暧昧 仿佛 某种 关系 松开 手 没 捏 耳环 一直 没有 放手 放开 会 丢 后来 赵碎 碎 问 耳环 是不是 手里 说 终于 想 起来 哈哈大笑 一阵 说 下次 带给 每次 见到 赵碎 碎 带 副 耳环 从未 没有 主动 捏 手里 滚烫 冰凉 从来不 会 沉寂 一个 惊天动地 故事 总 甘心 平淡 知道 银 耳环 迟早会 空气 中 氧化 掉 一点一点 泛起 黑色 涟漪 灼 目的 日光 不能 起死回生 赵碎 碎 说 忘记 问 耳环 没事儿 放 手里 放心 扭 扭腰 款 摆 踩 出 步步莲花 心头 刺 浮世 创痕 没有 跟路文 结婚 没有 认识 任何人 结婚 后来 离开 城市 不知 跳舞 一定 会 跳下去 白得 惊人 露出 那段 腰 妖 妖娆 娆 舞池 里 微微 泛起 浪花 一点一点 渗出 汁水 想像 没有 露出 肩 没有 得到 最好 耳环 没有 回去 躺 柜子 里 隐隐 寂寞 隐隐作痛 隔 很久很久 氧化 暗暗 黑 下去 永远 洁白 闪耀 赵 碎碎的 肩 不曾 见 肩 后来 我见 一次 赵 碎碎的 银雾 弧光 一个 女孩 包 里 浅蓝 液体 阳光 击中 闪耀 出 一点 不死心 光来 说 一款 香得 吓人 香水 美 要命 问 认识 赵碎 碎 茫然 看着 原来 赵碎 碎 已经 故事', '上海 出差 回来 临时 朋友 约 饭 高铁 紧急 撸 妆 文明 anyway 证明 Nars 大白 饼 尊嘟 飞粉 高铁 化妆 高铁 化妆 美妆博主 反击 高铁 化妆 争议 视频 制作方 回应 高铁 化妆 列为 文明 行为 网友 认可', '今日 主打 人设 文明 居然 高铁 化妆 高铁 化妆 美妆博主 反击 高铁 宣传片 化妆 列入 文明 行为', '', '', '蟑螂 居然 只用 两天 就治好 烂 脸 前段时间 突然 烂 脸 Doctor 开 蟑螂 汁 纱布 湿敷 早晚 一次 药膏 每天 三次 两天 表皮 愈合 泛红 没有 没想到 会 一天 命是 小强 美洲 大蠊 干燥 虫 形象 一下子 心里 拔高 爆痘 不好 姐妹 强推 试试 蟑螂 汁治好 烂 脸 美出 圈 换季 护肤 tips', '月 最近 生活 碎片 i 最近 太 享受 独处 一步 多多 社交 HO 加油 plog 博主 日记 电影 奥本海默 成都 名士 公馆', '路上 Kiko 成晨', '永远 自由 如风 今天 穿 ootd 微博变 美 手册 种草 博尔塔拉 赛里木湖', '看不到 动态 日子 努力 努力 证明 优秀 意外 可控 因素 临时 平常 努力 积淀 涵养 能力 成为 抗衡 风雨 底气 努力 拥有 自由选择 权利 上学 成绩 自由 选择 座位 高考 时多考 一分 自由 选择 大学 工作 时 努力 一点 选择 想 喝 奶茶 结婚 自由 选择 女友 孩子 以后 自由 孩子 选择 玩具 老 以后 自由 选择 养老 方式 努力 真正 意义 自由 选择 被迫 选择 无奈 选择 努力 尽可能 命运 拽 手里 被动 困在 嘴里 遇到 真正 喜欢 事情 一片 真心 出手 东西 一事无成 温柔 努力 意义 自由 选择 权利 日 直播 不见不散 会 马锐 直播', '迪丽 热巴 沛纳海 化妆 进发 型 李志辉 JIN STUDIO 迪丽 热巴 黑色 开叉 长裙', '氛围 感 小仙 日常 po', '周记 认真 生活 今日 keys words 童年 礼物 开箱 滋滋 滋 香 迷糊 一号 选手 申请 退赛 久违 情侣 写真 拳击 女孩 小狗 打包带 走 闪现 三亚 下周 见 格式 健康 重聚 美力 潘 白雪 s', '雪霜 珍珠 四个 用法 一块 更 四块 强 眼下 定妆 腮红 急救法 提亮 卧蚕 腮红 叠加 微博变 美 手册 种草', '懂 欣欣 云为 衫 剧组 衣服 穿 虞书 欣云 为衫妆造 还原 微博变 美 手册 仿妆', '人生 第一次 演唱会 居然 赶上 粉墨 首尔 安 最终 场 现场 气氛 真的 燃 炸炸 炸 四 闺女 舞台 感染力 真是 现实 感受 之后 觉得 太强 开场 真的 激动 眼含 热泪 平时 开车 化妆 干 会 听的歌 单 全文', '几天 天气 太好 分享 最近 收到 宝藏 好物们 微博变 美 手册 种草', '旅行 途中 能少 旅行 必备 免税 专售 雅诗兰黛 净润 修护 套装 再也 不用 担心 皮肤 环境 改变 带来 皮肤 敏感 日光 熬夜 带来 细纹 旅途 开心 加倍 找寻 生活 旅行 中 遇见 更 闪耀 还原 美貌 加倍 快乐 带上 雅诗兰黛 净润修 护套 一起 旅行 说走就走 旅行 遇见 更好', '说 Angelababy 无疑 美的 代言人 广告 造型 不得不 说真的 惊艳 每套 造型 散发 干练 又酷 有飒 勇于 做 条条框框 束缚 利落 发型 简单 修身 衣服 爱 Angelababy 新潮 无框 超出 片', '爆闪冰 咖妆容 获得 面试 第一眼 好感 开学 变美 攻略 幼熙 美妆', '昨天晚上 意识 秋天 真的 晚上 点 穿着 短裤 出去 试图 花 浇水 一阵 小风 吹 回 屋里 白天 叶绿 花红 夜晚 凉风 秋月 初秋 人间 美好 季节 推荐 秋天 第一瓶 香水 过去 几个 月 里 光顾 流汗 香水 挑 清爽 躲 空调 房里 闻 空气 清新剂 秋天 热 闷 香水 最好 季节 各路 神仙 打架 为求 欢心 Olfactive Studio 嗅觉 实验室 Chambre Noire 暗室 穿 风衣 风衣 袖口 下摆 喷 一点 带 这种 皮革 困于 焚香 香气 征服 世界 Saine Imsne 赛伊之梦 Tea Tao 茶道 这支 秋老虎 来袭 时用 千锤百炼 热 茶道 悠然 清甜 馨香 中 荡然无存 D Orsay 奥赛 甜蜜 等待 其实 不甜 微辛 肤 感香 闻久 像是 有人 身边 看不见 那种 隐隐 香 印在 心头 如影随形 奥赛 香水 没有 侵略性 意境 胜过 呐喊 高级 解放 橘郡 茉莉 雪茄 最帅 女孩 干净 男生 应该 秋天 喷 轻盈 茉莉 淡雅 雪茄 红尘 中 俗事 此刻 确实 重要 秋高气爽 撩人 香 Miller Harris 米勒 海莉 诗 Peau Santal 嫣 柔 檀香 气温 不到 度 嫣 柔 檀香 知道 香水 时 最佳 选择 追求 伪 体香 中 檀香 贴合 皮肤 烟火 气 柔情 万丈 bdk Parfums 巴杜克 Citrus Riviera 柠檬 气泡 水 秋天 会 怀念 夏天 或许 是因为 冰淇淋 或许 是因为 沙滩 或许 是因为 邂逅 男孩 这时候 巴杜克 柠檬 气泡 水该 上场 清甜 酸爽 一秒钟 想起 遗失 美好 观夏 空境 研茶 真的 推荐 上万次 推荐 秋天 用空境 研茶 入定 解困 消业 清障 微苦 禅意 秋天 正好 配它 DINshare 黑地 天幕 芭蕾 认识 黑地 有点 晚 刚刚 收到 这支 天幕 芭蕾 马上 推荐 确实 好看 好闻 这支 天幕 芭蕾 非常 讨巧 核心 味道 熟悉 爱 荔枝 玫瑰 晶莹剔透 复合 玫瑰 轻盈 圆润 穿 见 男孩 秋天 童话', '', '近期 快乐 碎片 OOTD 上海', 'HPV 攻略 守护 秒 就位 知道 女性 一生 中 感染 HPV 概率 高 持续 感染 高 危型 HPV 导致 宫颈癌 年 中国 子宫颈癌 综合 防控 指南 提出 三级预防 策略 第一步 接种 HPV 疫苗 目前 我国 个省 岁 女性 需 接种 价 HPV 疫苗 千万千万 不要 觉得 麻烦 毕竟 健康 守护 现在 HPV 全文', '', '朱 同学 THEO 朱正廷 反向 安利 居然 买 包 Delvaux SS 包包 比利时 艺术家 Kasper 设计 系列 壁虎 眼 可爱 住 朱正廷 出席 delvaux 新品 展览 巴黎 时装周 时装周', 'JP vlog 流水账 上个月 日本 vlog 终于 拖延 症 晚期 应该 早点 发 旅行 vlog 摄影 vlog', '梦回 巴黎 康朋街 号 kiko 美妆 微博变 美 手册 上海 外滩 源 壹号', 'PIPIs SHARING 早秋 护肤 分享 秋天 第一份 护肤品 请 查收 好物 分享 好物 推荐 微博变 美 手册 护肤', '变美 简单 期 视频 嘉宾 正 老师 男生 变帅 思路 到底 这期 明星 化妆师 笔记 揭秘 你好 造型师', '张婧仪 太平 鸟 化妆 张进 太平 鸟 大师 系列 上线 太平 鸟 女装品牌 代言人 张婧仪 JINSTUDIO', '赋能 女性 力量 非凡 引领 未来 感恩 邀请', '大头照 拍 完 觉得 韩国 女演员 演员 公式 久违 拍 写真', '库存 狮子 女 过生日 收到 pr 靴靴 品牌 霸霸 画 浓 妆 今日 妆容 日常 碎片 plog 好物 分享', '回家 饭后 散步 中 特别 舒服 初秋 上海 一枝 南南', '今日 开箱 好物 分享 周末 随手 拆 科颜氏 竟然 三体 联名 太酷 耶 今年 口红 酵色 承包 随便 一支 颜色 吼吼 爱住 开箱', '今日 分享 滴 超强 底妆 CP 鎏金 系列 真的 抗老 神器 微博变 美 手册 种草', '省份 姐妹 接 好运 最近 深切 感受 HPV 疫苗 增产 增量 身边 很多 姐妹 陆续 约 岁 适龄 女性 价 价 价 三种 价型 HPV 疫苗 选择 不同 HPV 疫苗 覆盖 HPV 型别 不同 需 保护 宫颈 健康 不容忽视 耐心 等待 终会 回响 点击 视频 查收 更 健康 知识 HPV 锦鲤 日', 'G 开学 季来 咯 最近 喜欢 蓝色 简单 干净 做 蓝 学弟 一起 city walk OOTD ASMEMMO qiyuyu MaisonMargiela Z Flip CASETiFY 开学 穿 搭 听 玩转 开学 季 今天 穿', '生活 不必 全盘 推翻 美丽 需要 一点点 改变 微博美妆 微博变 美 手册 IP 活动 重磅 上线 一起 惊艳 时光 美 同行 即日起 欢迎 带 话题 微博变 美 手册 发布 美丽 心得 参赛 期待 看到 变美 笔记 开学 变美 攻略 有变 美 问题 评论 区 留言 我会 在线 解答', '每日 一香 Saine Imsne Honeymoon 朋友 刘女士 离婚 说 恭喜 说 妈 A C 除以 二 说 离婚 好事 极大 解脱 说明 曾经 有过 痛苦 伤痕 时间 没法 抹平 难不倒 牛明昱 中国 最好 香水 博主 姐妹 刘女士 说 姐 送 一瓶 香水 帮 抹 平 月份 事 月 号 联系 告诉 我要 结婚 随礼 说 要脸 说 不要脸 就让 别过来 直接 打钱 说 结婚 离婚 速度 是不是 有点 快 说 第一次 结婚 不要 造谣 当事人 不肯 承认 统统 谣言 手机 屏幕 里 眉飞色舞 样子 说 祝你幸福 拉 黑 立省 一笔 钱 这瓶 迅速 忘掉 烦恼 走入 新春 香水 Saine Imsne 赛伊之梦 Honeymoon 蜜月 相信 没有 忧愁 值得 眉头 锁上 甜蜜 生活 脚下 左手 看不见 幸福 缔造 幸福 Honeymoon 甜蜜 令人 心碎 一款 香水 讨厌 甜蜜 香水 无比 喜欢 大概 甜蜜 轻糖 浓脂 饱满 胭脂 鎏金 膏腴 蜂蜜 味道 浓到 要流 触手可及 绮丽 盖 不住 烟草 香草 甜艳 风流 蜜渍 堡垒 陶然 入 瓮 木质 元素 大都 硬朗 Honeymoon 之中 柔软 娇媚 男生 女相 拜倒 蜂蜜 绕指 缠绵 里 一个 甜蜜 抽着 烟 狂放不羁 华美 末路 终途 哀艳 苦海无边 回头 靠岸', '最近 收到 礼物 购物 分享 CHILLBOYCREW 入秋 之后 帽衫 季节 最近 真的 灰色 衣服 上头 准备 整理 一下 出 一期 灰色 穿 搭 KIEHL S x 三体 护肤 礼盒 收到 科颜氏 高 保湿 礼盒 入秋 之后 天气 变 干燥 干皮 学生 党 尝试 一下 高 保湿 系列 小奥汀 液体 修容 性价比 高 液体 修容 价格 便宜 歌剧 魅影 遮 瑕 盘 火 久 遮 瑕 遮 胡青 黑眼圈 三文鱼 色 感觉 一盘 用到 三十岁 同仁堂 鱼胶 燕窝 即食 罐头 岁 真的 注重 养生 保持 运动 之外 内服 维生素 保养品 真的 懈怠 这种 即时 自热 罐头 真的 方便 Nike dunk 粉白 配色 真的 好看 一眼 爱 初秋 穿 搭 搭配 一点 亮色 真的 吸睛', '抗老 松弛 感抗 衰堪 大学生 最近 好多 姐妹 私信 说 感觉 脸蛋 有点 下垂 脸上 纹路 之前 明显 抗衰 功课 真滴 重要 今天 姐妹 分享 几个 简单 实用 技巧 姐妹 看起来 同龄人 状态 更好 护肤 好物 推荐', '宝贝 今天 点子 美丽 身上 成片 晒出 美好生活', '', '年 诞生 迪奥 真 香水 瓶身 瓶盖 原来 演变 设计 真的 感叹 一句 东西 岁月 打磨 已经 不单单是 一款 香水 更 一个 缪斯 启发 更 更 美好 艺术作品 诞生 时装周 巴黎 时装周', '美妆 探店 成都 Double RL Co 拉夫 劳伦之家 成都 远洋 太古 里店', 'kiko 美妆', 'bus 太 可爱 好物 推荐 好物 分享', '变美 简单 美国 纽约时代广场 马 小胖 马锐 你好 造型师', 'Vlog 下班 快乐 时光 篮球场 运动型 女孩 改造 成 美 拉德 风格 觉得 下班 唱歌 下班 干 回家 躺 刷 手机 出去玩 微博 vlog 大赛 寻找 闪光', '头像 壁纸 来噜 你好', '周记 幸福 具像化 今日 keys words 报告 小八新粉 出现 耳朵 出血 日常 约会 日 寺庙 一日 妮妮 小宝 陪 妈妈 做 美甲 下周 见 潘 白雪 s', '三款 平价 腮 紫 尊 狠 嫩 中 黄气 显白 减龄 快进来 get 平价 美妆 平价 好物', '月 第一天 水逆退 散 来接 好运 固定 栏目 pr 礼物 上线 评论 区 许愿池 开放 共享 快乐 ps 谢谢 品牌 霸霸们 爱 一枝 南南 抽奖 详情', '音乐 一响 时代 眼泪 登场 零 几年 古早 氛围 感 狠狠 拿捏 听 首歌 大小 烟熏 画好 只能 说 封神 烟熏 妆 次 无数次 高糊 质感 ccd 实现 烟熏 ccd 古早 韩系 女星 美妆 古早 味 韩系 穿 搭 韩素 希 烟熏 妆', 'SHARING 油皮 底妆 分享 油皮 挚爱 底妆 合集 终于 油痘 肌 姐妹 一定 过来 肤质 选 产品 找到 适合 微博变 美 手册 妆容', '前 几天 买 新年礼物 去逛 丝芙兰 SEPHORA 正好 看到 丝 芙 兰新 年 主题 新年 就耀红', '自信 从容 开启 发光 之旅 镜头 肖战 满满的 治愈 感 EVE LOM 不谋而合 一次 卸妆 治愈 过程 肌肤 清透 干净 透出 满满 光泽 肖战 梦境 大片', '情人节 快乐 姐妹 今天 幼熙 美妆 幼熙 美妆 惊喜 礼物 转发 评论 揪 铁粉 随机 送 单品 感谢 品牌 喜爱 GUCCI 植村秀 中国 雅诗兰黛 集团 旅游 零售 TOMFORDBEAUTY BobbiBrownChina 红门雅顿 卡姿兰 Carslan belif 碧 研菲 CeraVe 适乐肤 SABON 官方 微博 DrJart 蒂佳婷', '张拿铁 户口本 丢 真的 没想到 一天 我会 追着 踩 铃 视频 看着 没完 昨天 妈 分享 一堆 我妈 不亦乐乎 一直 一个 臭 不要脸 好色的人 喜欢 美的 踩 铃 肯定 不算什么 大美女 满嘴 东北 方言 绝对 审美 里 几年 大胖 媳妇 日常 挺好玩 累 不腻 制造 焦虑 不硬塞 道理 高潮 Wendy 滑冰 期 哭 真的 没想到 一天 我能 一个 加拿大 老太太 哭 演员', '慢悠悠 周末 晚安', '慢 节奏 拥有 自愈 能力 OOTD 南京 南京 珺 懋傲 途格 酒店', '懂 护肤 一定 不会 卸妆 省钱 滴 毕竟 妆 卸 干净 会 导致 毛孔 堵塞 闭口 痘痘 形成 恶性循环 真的 得不偿失 说 卸妆 势必 讲 一下 卸妆 清洁 养肤 三合一 EVE LOM 伊芙珑 品牌 卸妆 膏中 爱马仕 养卸 合一 过去 一句 噱头 EVE LOM 伊芙珑 板上钉钉 事实 毕竟 全文', '', '巴黎 第一场 秀 Dior 春夏 秀秀 场内 屏幕 大胆 说 女性 想 表达 女性 多种 身份 敢于 独立思考 表达 自我 展露 个性 一季 呈现 中世纪 风格 建筑 感廓 形 黑色 西装 外套 硬朗 气质 蕾丝 镂空 长裙 洋甘菊 色丹 套装 女性 多元 表达 王俊凯 后台 创意 总监 Maria Grazia 交流 巴黎 时装周 时装周', '夏天 夏天 周五 周六 更 快乐 plog 博主 日记', 'Log 连轴转 工作 日子 开开心心 吃吃喝喝 日子 kiko 美妆', '生命力 对话 OOTD 今天 穿 微博变 美 手册 种草', '檀健次 做好 一个 情绪稳定 保持 一直 岁 秘密 保持 精神状态 情绪 皮肤 状态 保持 人间 清醒 眼神 永远 干净 清澈 檀健次 想 变美先 听 劝 你好 造型师', '迪丽 热巴 力士 化妆 张进 JINSTUDIO', '悬溺 一响 纯爱 登场 小仙 日常 po 今日 妆容']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "\n",
    "# 加载停用词\n",
    "def load_stopwords():\n",
    "    url = \"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\"\n",
    "    return pd.read_csv(url, header=None, sep=\"\\t\", quoting=3, names=['stopword'], encoding='utf-8').stopword.tolist()\n",
    "\n",
    "# 获取停用词列表\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# 检查并删除df[0]中的重复数据\n",
    "df_total = df_total.drop_duplicates()\n",
    "\n",
    "\n",
    "# 假设的博主名字列表，如果你没有这个列表，可以将其定义为空列表\n",
    "combined_blogger_names = []\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 使用正则表达式去除“XX的微博视频”\n",
    "    text = re.sub(r'[^#]*的微博视频', '', text)\n",
    "    \n",
    "    # 去除特定无用词\n",
    "    useless_terms = [\"显示地图\", \"原图\", \"[超话]\", \"超话\"]\n",
    "    for term in useless_terms:\n",
    "        text = text.replace(term, '')\n",
    "    \n",
    "    # 去除自我指代\n",
    "    for name in combined_blogger_names:\n",
    "        text = text.replace(name, '')\n",
    "    \n",
    "    # 使用jieba进行分词\n",
    "    words = jieba.cut(text, cut_all=False)\n",
    "    \n",
    "    # 去除标点和特殊字符\n",
    "    words = [re.sub(r'[^a-zA-Z\\u4e00-\\u9fff]', '', word) for word in words if len(word) > 0]\n",
    "    \n",
    "    # 去除“组图”、“共”、“张”\n",
    "    words = [word for word in words if word not in [\"组图\", \"共\", \"张\"]]\n",
    "    \n",
    "    # 去除停用词\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    # 将连续的多个空格替换为一个空格\n",
    "    result = ' '.join(words)\n",
    "    result = re.sub(r'\\s+', ' ', result).strip()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 如果'content_cutted'存在于df_total中，则删除它\n",
    "if 'content_cutted' in df_total:\n",
    "    df_total = df_total.drop('content_cutted')\n",
    "\n",
    "# 然后应用预处理函数\n",
    "texts_cut = [preprocess_text(str(text)) for text in df_total]\n",
    "print(texts_cut[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b51f1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  warnings.warn(\n",
      "INFO:lda:n_documents: 27040\n",
      "INFO:lda:vocab_size: 1000\n",
      "INFO:lda:n_words: 448987\n",
      "INFO:lda:n_topics: 15\n",
      "INFO:lda:n_iter: 1000\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -4585892\n",
      "INFO:lda:<10> log likelihood: -3336410\n",
      "INFO:lda:<20> log likelihood: -3128911\n",
      "INFO:lda:<30> log likelihood: -3069378\n",
      "INFO:lda:<40> log likelihood: -3041090\n",
      "INFO:lda:<50> log likelihood: -3026912\n",
      "INFO:lda:<60> log likelihood: -3018262\n",
      "INFO:lda:<70> log likelihood: -3014367\n",
      "INFO:lda:<80> log likelihood: -3010530\n",
      "INFO:lda:<90> log likelihood: -3008359\n",
      "INFO:lda:<100> log likelihood: -3007044\n",
      "INFO:lda:<110> log likelihood: -3005228\n",
      "INFO:lda:<120> log likelihood: -3004639\n",
      "INFO:lda:<130> log likelihood: -3003713\n",
      "INFO:lda:<140> log likelihood: -3002827\n",
      "INFO:lda:<150> log likelihood: -3000438\n",
      "INFO:lda:<160> log likelihood: -3000794\n",
      "INFO:lda:<170> log likelihood: -2998279\n",
      "INFO:lda:<180> log likelihood: -2998523\n",
      "INFO:lda:<190> log likelihood: -2997655\n",
      "INFO:lda:<200> log likelihood: -2997268\n",
      "INFO:lda:<210> log likelihood: -2997807\n",
      "INFO:lda:<220> log likelihood: -2997660\n",
      "INFO:lda:<230> log likelihood: -2996910\n",
      "INFO:lda:<240> log likelihood: -2994958\n",
      "INFO:lda:<250> log likelihood: -2996143\n",
      "INFO:lda:<260> log likelihood: -2995327\n",
      "INFO:lda:<270> log likelihood: -2994575\n",
      "INFO:lda:<280> log likelihood: -2992733\n",
      "INFO:lda:<290> log likelihood: -2993480\n",
      "INFO:lda:<300> log likelihood: -2994070\n",
      "INFO:lda:<310> log likelihood: -2993217\n",
      "INFO:lda:<320> log likelihood: -2992352\n",
      "INFO:lda:<330> log likelihood: -2990968\n",
      "INFO:lda:<340> log likelihood: -2992370\n",
      "INFO:lda:<350> log likelihood: -2992044\n",
      "INFO:lda:<360> log likelihood: -2989646\n",
      "INFO:lda:<370> log likelihood: -2989613\n",
      "INFO:lda:<380> log likelihood: -2989958\n",
      "INFO:lda:<390> log likelihood: -2990283\n",
      "INFO:lda:<400> log likelihood: -2990248\n",
      "INFO:lda:<410> log likelihood: -2989594\n",
      "INFO:lda:<420> log likelihood: -2989316\n",
      "INFO:lda:<430> log likelihood: -2989758\n",
      "INFO:lda:<440> log likelihood: -2989794\n",
      "INFO:lda:<450> log likelihood: -2989345\n",
      "INFO:lda:<460> log likelihood: -2989192\n",
      "INFO:lda:<470> log likelihood: -2987902\n",
      "INFO:lda:<480> log likelihood: -2987493\n",
      "INFO:lda:<490> log likelihood: -2989122\n",
      "INFO:lda:<500> log likelihood: -2986790\n",
      "INFO:lda:<510> log likelihood: -2987761\n",
      "INFO:lda:<520> log likelihood: -2987709\n",
      "INFO:lda:<530> log likelihood: -2988874\n",
      "INFO:lda:<540> log likelihood: -2987778\n",
      "INFO:lda:<550> log likelihood: -2988024\n",
      "INFO:lda:<560> log likelihood: -2988826\n",
      "INFO:lda:<570> log likelihood: -2988032\n",
      "INFO:lda:<580> log likelihood: -2986926\n",
      "INFO:lda:<590> log likelihood: -2986284\n",
      "INFO:lda:<600> log likelihood: -2986597\n",
      "INFO:lda:<610> log likelihood: -2987450\n",
      "INFO:lda:<620> log likelihood: -2988068\n",
      "INFO:lda:<630> log likelihood: -2987673\n",
      "INFO:lda:<640> log likelihood: -2988392\n",
      "INFO:lda:<650> log likelihood: -2987925\n",
      "INFO:lda:<660> log likelihood: -2988099\n",
      "INFO:lda:<670> log likelihood: -2987215\n",
      "INFO:lda:<680> log likelihood: -2986985\n",
      "INFO:lda:<690> log likelihood: -2987177\n",
      "INFO:lda:<700> log likelihood: -2986749\n",
      "INFO:lda:<710> log likelihood: -2987584\n",
      "INFO:lda:<720> log likelihood: -2987857\n",
      "INFO:lda:<730> log likelihood: -2987288\n",
      "INFO:lda:<740> log likelihood: -2986921\n",
      "INFO:lda:<750> log likelihood: -2986018\n",
      "INFO:lda:<760> log likelihood: -2986940\n",
      "INFO:lda:<770> log likelihood: -2987204\n",
      "INFO:lda:<780> log likelihood: -2985694\n",
      "INFO:lda:<790> log likelihood: -2986159\n",
      "INFO:lda:<800> log likelihood: -2985903\n",
      "INFO:lda:<810> log likelihood: -2985527\n",
      "INFO:lda:<820> log likelihood: -2985339\n",
      "INFO:lda:<830> log likelihood: -2985042\n",
      "INFO:lda:<840> log likelihood: -2985460\n",
      "INFO:lda:<850> log likelihood: -2984944\n",
      "INFO:lda:<860> log likelihood: -2984475\n",
      "INFO:lda:<870> log likelihood: -2984877\n",
      "INFO:lda:<880> log likelihood: -2985871\n",
      "INFO:lda:<890> log likelihood: -2983455\n",
      "INFO:lda:<900> log likelihood: -2985785\n",
      "INFO:lda:<910> log likelihood: -2985206\n",
      "INFO:lda:<920> log likelihood: -2985365\n",
      "INFO:lda:<930> log likelihood: -2985420\n",
      "INFO:lda:<940> log likelihood: -2985562\n",
      "INFO:lda:<950> log likelihood: -2985727\n",
      "INFO:lda:<960> log likelihood: -2985133\n",
      "INFO:lda:<970> log likelihood: -2985795\n",
      "INFO:lda:<980> log likelihood: -2984946\n",
      "INFO:lda:<990> log likelihood: -2986280\n",
      "INFO:lda:<999> log likelihood: -2985596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model done\n",
      "Topic #0:\n",
      "互动 头发 头皮 护发 粉丝 宝贝 评论 精油 私信 上榜\n",
      "Topic #1:\n",
      "一个 生活 真的 没有 一起 工作 希望 很多 看到 时间\n",
      "Topic #2:\n",
      "香水 玫瑰 味道 系列 搭配 喜欢 设计 高级 香气 精致\n",
      "Topic #3:\n",
      "新年 plog 计划 完美 日常 攻略 开春 日记 碎片 好颜\n",
      "Topic #4:\n",
      "防晒 精华 美白 成分 护肤 效果 娇韵诗 使用 夏天 质地\n",
      "Topic #5:\n",
      "肌肤 皮肤 精华 修护 抗老 成分 真的 状态 护肤 面霜\n",
      "Topic #6:\n",
      "口红 底妆 粉底液 妆容 眼影 真的 腮红 颜色 定妆 高级\n",
      "Topic #7:\n",
      "今日 妆容 春天 ootd 今天 春夏 look 出游 春日 氛围\n",
      "Topic #8:\n",
      "分享 好物 种草 双十 推荐 购物 最近 开箱 彩妆 美妆\n",
      "Topic #9:\n",
      "抽奖 详情 vlog 生活 夏日 记录 快乐 旅行 体验 豆豆\n",
      "Topic #10:\n",
      "京东 链接 网页 天猫 一起 活动 ml 免税 超级 福利\n",
      "Topic #11:\n",
      "护肤 皮肤 面膜 分享 精华 好物 换季 面霜 身体 清洁\n",
      "Topic #12:\n",
      "妆容 美妆 化妆 变美 开学 视频 真的 眼妆 今天 教程\n",
      "Topic #13:\n",
      "礼盒 礼物 快乐 开箱 品牌 七夕 情人节 收到 感谢 王国\n",
      "Topic #14:\n",
      "真的 感觉 喜欢 觉得 没有 不会 这种 那种 一个 特别\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (27040) does not match length of index (7541)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-585d49412210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# 6. 为每个文档分配主题\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdoc_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_topic_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_topic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# 7. 计算困惑度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3948\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3949\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3950\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3952\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4141\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4142\u001b[0m         \"\"\"\n\u001b[0;32m-> 4143\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4145\u001b[0m         if (\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4870\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4871\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \"\"\"\n\u001b[1;32m    575\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (27040) does not match length of index (7541)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# 3. 文本向量化\n",
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, stop_words=stopwords, max_df=0.5, min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(texts_cut)\n",
    "\n",
    "# 4. LDA模型训练\n",
    "n_topics = 15\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=1000, random_state=2)\n",
    "model.fit(tf)\n",
    "print('Model done')\n",
    "\n",
    "# 5. 打印每个主题的关键词\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names_out()\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# 6. 为每个文档分配主题\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = [topics.argmax() for topics in doc_topic]\n",
    "\n",
    "# 7. 计算困惑度\n",
    "log_likelihood = model.loglikelihood()\n",
    "n_total_words = tf.sum()\n",
    "perplexity = np.exp(-log_likelihood / n_total_words)\n",
    "print(\"Model Perplexity: \", perplexity)\n",
    "\n",
    "# # 8. 保存结果\n",
    "# df.to_excel('/Users/laihuiqian/Documents/weibo_0925/total2-0925_with_topics.xlsx', index=False)\n",
    "# print('Results saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e517bf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  warnings.warn(\n",
      "INFO:lda:n_documents: 27040\n",
      "INFO:lda:vocab_size: 1000\n",
      "INFO:lda:n_words: 448987\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 1000\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -4319232\n",
      "INFO:lda:<10> log likelihood: -3268817\n",
      "INFO:lda:<20> log likelihood: -3102408\n",
      "INFO:lda:<30> log likelihood: -3053948\n",
      "INFO:lda:<40> log likelihood: -3032280\n",
      "INFO:lda:<50> log likelihood: -3021203\n",
      "INFO:lda:<60> log likelihood: -3012698\n",
      "INFO:lda:<70> log likelihood: -3004912\n",
      "INFO:lda:<80> log likelihood: -3000607\n",
      "INFO:lda:<90> log likelihood: -2998263\n",
      "INFO:lda:<100> log likelihood: -2996289\n",
      "INFO:lda:<110> log likelihood: -2995029\n",
      "INFO:lda:<120> log likelihood: -2993934\n",
      "INFO:lda:<130> log likelihood: -2993825\n",
      "INFO:lda:<140> log likelihood: -2990313\n",
      "INFO:lda:<150> log likelihood: -2989663\n",
      "INFO:lda:<160> log likelihood: -2990036\n",
      "INFO:lda:<170> log likelihood: -2988542\n",
      "INFO:lda:<180> log likelihood: -2987235\n",
      "INFO:lda:<190> log likelihood: -2986060\n",
      "INFO:lda:<200> log likelihood: -2982960\n",
      "INFO:lda:<210> log likelihood: -2982865\n",
      "INFO:lda:<220> log likelihood: -2982363\n",
      "INFO:lda:<230> log likelihood: -2980225\n",
      "INFO:lda:<240> log likelihood: -2979758\n",
      "INFO:lda:<250> log likelihood: -2980372\n",
      "INFO:lda:<260> log likelihood: -2978708\n",
      "INFO:lda:<270> log likelihood: -2978481\n",
      "INFO:lda:<280> log likelihood: -2977675\n",
      "INFO:lda:<290> log likelihood: -2978972\n",
      "INFO:lda:<300> log likelihood: -2978084\n",
      "INFO:lda:<310> log likelihood: -2977283\n",
      "INFO:lda:<320> log likelihood: -2978435\n",
      "INFO:lda:<330> log likelihood: -2977833\n",
      "INFO:lda:<340> log likelihood: -2976621\n",
      "INFO:lda:<350> log likelihood: -2976247\n",
      "INFO:lda:<360> log likelihood: -2975225\n",
      "INFO:lda:<370> log likelihood: -2974948\n",
      "INFO:lda:<380> log likelihood: -2975015\n",
      "INFO:lda:<390> log likelihood: -2974232\n",
      "INFO:lda:<400> log likelihood: -2972947\n",
      "INFO:lda:<410> log likelihood: -2973471\n",
      "INFO:lda:<420> log likelihood: -2973521\n",
      "INFO:lda:<430> log likelihood: -2973051\n",
      "INFO:lda:<440> log likelihood: -2973803\n",
      "INFO:lda:<450> log likelihood: -2972413\n",
      "INFO:lda:<460> log likelihood: -2971794\n",
      "INFO:lda:<470> log likelihood: -2971920\n",
      "INFO:lda:<480> log likelihood: -2972994\n",
      "INFO:lda:<490> log likelihood: -2970430\n",
      "INFO:lda:<500> log likelihood: -2971024\n",
      "INFO:lda:<510> log likelihood: -2970736\n",
      "INFO:lda:<520> log likelihood: -2970129\n",
      "INFO:lda:<530> log likelihood: -2971394\n",
      "INFO:lda:<540> log likelihood: -2971974\n",
      "INFO:lda:<550> log likelihood: -2970920\n",
      "INFO:lda:<560> log likelihood: -2971174\n",
      "INFO:lda:<570> log likelihood: -2970729\n",
      "INFO:lda:<580> log likelihood: -2970999\n",
      "INFO:lda:<590> log likelihood: -2970704\n",
      "INFO:lda:<600> log likelihood: -2969912\n",
      "INFO:lda:<610> log likelihood: -2970065\n",
      "INFO:lda:<620> log likelihood: -2969522\n",
      "INFO:lda:<630> log likelihood: -2969322\n",
      "INFO:lda:<640> log likelihood: -2970073\n",
      "INFO:lda:<650> log likelihood: -2969877\n",
      "INFO:lda:<660> log likelihood: -2969555\n",
      "INFO:lda:<670> log likelihood: -2969347\n",
      "INFO:lda:<680> log likelihood: -2968515\n",
      "INFO:lda:<690> log likelihood: -2968615\n",
      "INFO:lda:<700> log likelihood: -2969184\n",
      "INFO:lda:<710> log likelihood: -2969608\n",
      "INFO:lda:<720> log likelihood: -2970222\n",
      "INFO:lda:<730> log likelihood: -2969468\n",
      "INFO:lda:<740> log likelihood: -2970250\n",
      "INFO:lda:<750> log likelihood: -2968392\n",
      "INFO:lda:<760> log likelihood: -2970050\n",
      "INFO:lda:<770> log likelihood: -2968903\n",
      "INFO:lda:<780> log likelihood: -2969030\n",
      "INFO:lda:<790> log likelihood: -2969492\n",
      "INFO:lda:<800> log likelihood: -2969212\n",
      "INFO:lda:<810> log likelihood: -2968438\n",
      "INFO:lda:<820> log likelihood: -2968591\n",
      "INFO:lda:<830> log likelihood: -2969019\n",
      "INFO:lda:<840> log likelihood: -2968242\n",
      "INFO:lda:<850> log likelihood: -2969292\n",
      "INFO:lda:<860> log likelihood: -2968861\n",
      "INFO:lda:<870> log likelihood: -2969059\n",
      "INFO:lda:<880> log likelihood: -2969058\n",
      "INFO:lda:<890> log likelihood: -2967905\n",
      "INFO:lda:<900> log likelihood: -2970348\n",
      "INFO:lda:<910> log likelihood: -2969673\n",
      "INFO:lda:<920> log likelihood: -2969421\n",
      "INFO:lda:<930> log likelihood: -2968703\n",
      "INFO:lda:<940> log likelihood: -2967732\n",
      "INFO:lda:<950> log likelihood: -2968729\n",
      "INFO:lda:<960> log likelihood: -2969513\n",
      "INFO:lda:<970> log likelihood: -2968871\n",
      "INFO:lda:<980> log likelihood: -2969019\n",
      "INFO:lda:<990> log likelihood: -2970300\n",
      "INFO:lda:<999> log likelihood: -2969997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model done\n",
      "Topic #0:\n",
      "香水 头发 真的 味道 玫瑰 头皮 身体 护发 感觉 喜欢\n",
      "Topic #1:\n",
      "妆容 美妆 今日 化妆 氛围 春夏 变美 出游 真的 今天\n",
      "Topic #2:\n",
      "分享 好物 护肤 美妆 种草 最近 推荐 双十 彩妆 今天\n",
      "Topic #3:\n",
      "plog 生活 互动 日常 vlog 快乐 日记 碎片 一起 旅行\n",
      "Topic #4:\n",
      "防晒 真的 粉底液 口红 底妆 质地 不会 定妆 颜色 适合\n",
      "Topic #5:\n",
      "新年 抽奖 详情 计划 完美 开箱 礼盒 快乐 开春 微博\n",
      "Topic #6:\n",
      "真的 一个 没有 觉得 喜欢 感觉 很多 现在 知道 一定\n",
      "Topic #7:\n",
      "夏日 ootd 今天 时尚 春天 look 搭配 春季 夏天 浪漫\n",
      "Topic #8:\n",
      "皮肤 精华 肌肤 护肤 修护 抗老 成分 面霜 真的 效果\n",
      "Topic #9:\n",
      "京东 链接 网页 天猫 一起 礼盒 礼物 活动 超级 姐妹\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (27040) does not match length of index (7541)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-2cd264c65155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# 6. 为每个文档分配主题\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdoc_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_topic_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_topic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# 7. 计算困惑度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3948\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3949\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3950\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3952\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4141\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4142\u001b[0m         \"\"\"\n\u001b[0;32m-> 4143\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4145\u001b[0m         if (\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4870\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4871\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \"\"\"\n\u001b[1;32m    575\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (27040) does not match length of index (7541)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# 3. 文本向量化\n",
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, stop_words=stopwords, max_df=0.5, min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(texts_cut)\n",
    "\n",
    "# 4. LDA模型训练\n",
    "n_topics = 10\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=1000, random_state=2)\n",
    "model.fit(tf)\n",
    "print('Model done')\n",
    "\n",
    "# 5. 打印每个主题的关键词\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names_out()\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# 6. 为每个文档分配主题\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = [topics.argmax() for topics in doc_topic]\n",
    "\n",
    "# 7. 计算困惑度\n",
    "log_likelihood = model.loglikelihood()\n",
    "n_total_words = tf.sum()\n",
    "perplexity = np.exp(-log_likelihood / n_total_words)\n",
    "print(\"Model Perplexity: \", perplexity)\n",
    "\n",
    "# # 8. 保存结果\n",
    "# df.to_excel('/Users/laihuiqian/Documents/weibo_0925/total2-0925_with_topics.xlsx', index=False)\n",
    "# print('Results saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b8479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
