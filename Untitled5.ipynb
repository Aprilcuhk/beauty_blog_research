{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea35b447",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'正文'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '正文'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8661003837aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mdirectory_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/laihuiqian/weibo/'\u001b[0m  \u001b[0;31m# 替换为你的文件夹路径\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0moutput_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'total-all-100.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mdf_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_csvs_in_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8661003837aa>\u001b[0m in \u001b[0;36mmerge_csvs_in_directory\u001b[0;34m(directory_path, output_filename)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# 去除NaN值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"正文\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mdf_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3761\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '正文'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_csvs_in_directory(directory_path, output_filename):\n",
    "    # 初始化一个空的DataFrame，用于存储所有的数据\n",
    "    df_total = pd.DataFrame()\n",
    "    \n",
    "    # 定义要遍历的文件夹名\n",
    "    folder_ranges = [f\"{i}-{i+3}\" for i in range(1, 98, 4)]\n",
    "    folder_ranges[-1] = \"97-100\"  # 最后一个文件夹名为\"97-100\"\n",
    "    \n",
    "    # 遍历每个子文件夹\n",
    "    for folder in folder_ranges:\n",
    "        subdirectory_path = os.path.join(directory_path, folder)\n",
    "        \n",
    "        # 判断文件夹是否存在\n",
    "        if not os.path.exists(subdirectory_path):\n",
    "            print(f\"Directory {folder} does not exist.\")\n",
    "            continue\n",
    "        \n",
    "        # 遍历子文件夹\n",
    "        for subfolder in os.listdir(subdirectory_path):\n",
    "            subfolder_path = os.path.join(subdirectory_path, subfolder)\n",
    "            \n",
    "            # 确保是文件夹\n",
    "            if not os.path.isdir(subfolder_path):\n",
    "                continue\n",
    "            \n",
    "            # 列出子文件夹中的所有CSV文件\n",
    "            files = [f for f in os.listdir(subfolder_path) if f.endswith('.csv')]\n",
    "            \n",
    "            # 确保只有一个CSV文件\n",
    "            if len(files) != 1:\n",
    "                print(f\"Expected one CSV file in the subfolder: {subfolder}, but found {len(files)}.\")\n",
    "                continue\n",
    "            \n",
    "            # 读取CSV文件\n",
    "            df = pd.read_csv(os.path.join(subfolder_path, files[0]), encoding='utf-8')\n",
    "            \n",
    "            # 提取\"微博正文\"或\"正文\"这一列\n",
    "            if \"微博正文\" in df.columns:\n",
    "                df_total = pd.concat([df_total, df[\"微博正文\"]], ignore_index=True)\n",
    "            elif \"正文\" in df.columns:\n",
    "                df_total = pd.concat([df_total, df[\"正文\"]], ignore_index=True)\n",
    "            else:\n",
    "                print(f\"Neither '微博正文' nor '正文' column found in the CSV file in subfolder: {subfolder}\")\n",
    "        \n",
    "    print(df_total[\"正文\"])\n",
    "    # 去除NaN值\n",
    "    df_total.dropna(inplace=True)\n",
    "    \n",
    "    # 保存合并后的数据到txt文件\n",
    "    with open(os.path.join(directory_path, output_filename), 'w', encoding='utf-8') as file:\n",
    "        for item in df_total.values:\n",
    "            file.write(\"%s\\n\" % item[0])\n",
    "    \n",
    "    print(f\"All data merged and saved as {output_filename}.\")\n",
    "    return df_total\n",
    "\n",
    "# 使用方法\n",
    "directory_path = '/Users/laihuiqian/weibo/'  # 替换为你的文件夹路径\n",
    "output_filename = 'total-all-100.txt'\n",
    "df_result = merge_csvs_in_directory(directory_path, output_filename)\n",
    "print(df_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e4291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/laihuiqian/beauty_research\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f64720d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/laihuiqian/weibo/ exists.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(directory_path):\n",
    "    print(f\"{directory_path} exists.\")\n",
    "else:\n",
    "    print(f\"{directory_path} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d4da791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/tx/327gmf1d3vd2fzt8k0972vz00000gn/T/jieba.cache\n",
      "Loading model cost 0.583 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       0  \\\n",
      "0      身体护理好物分享 姐妹们，身体头皮的护理和我们面部一样重要哦！从头到脚都不能放过！ 最近一个...   \n",
      "1      永远自由如风~#今天穿什么##ootd# #微博变美手册# #种草# 博尔塔拉·赛里木湖  ...   \n",
      "2      PIPIs SHARING | 早秋护肤分享秋天的第一份护肤品请查收～#好物分享##好物推荐...   \n",
      "3                   小bus也太可爱了吧！#好物推荐##好物分享#  [组图共9张] 原图    \n",
      "4         与生命力对话#OOTD##今天穿什么# #微博变美手册# #种草#  [组图共9张] 原图    \n",
      "...                                                  ...   \n",
      "26467                                                NaN   \n",
      "26468                                                NaN   \n",
      "26469                                                NaN   \n",
      "26470                                                NaN   \n",
      "26471                                                NaN   \n",
      "\n",
      "                                          content_cutted  \n",
      "0      我们 给 好物 乳帮 都 6 一样 # 身体 特别 护理 从头到脚 户外 ## 你们 工程 ...  \n",
      "1      ootd # 博尔塔拉 手册 ## 张 地图 穿 美 [ 微博变 赛里木湖 什么 永远   ...  \n",
      "2      秋天 好物 | 早秋 # PIPIs 手册 请 ## 美 微博变 的 SHARING 皮儿 ...  \n",
      "3         小 好物 也 # ## 张 吧 [ 了 ！   ] 共 太 9   分享 可爱 bus 推荐  \n",
      "4      OOTD # 手册 ## 张 穿 美 [ 微博变 与 什么   ] 共 9   今天 对话 ...  \n",
      "...                                                  ...  \n",
      "26467                                                nan  \n",
      "26468                                                nan  \n",
      "26469                                                nan  \n",
      "26470                                                nan  \n",
      "26471                                                nan  \n",
      "\n",
      "[26472 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "def open_dict(Dict):\n",
    "    path = '%s.txt' % Dict\n",
    "    dictionary = open(path, 'r', encoding='utf-8')\n",
    "    dict = []\n",
    "    for word in dictionary:\n",
    "        word = word.strip('\\n')\n",
    "        dict.append(word)\n",
    "    return dict\n",
    "\n",
    "clearwords=open_dict('clearwords')\n",
    "\n",
    "#分词\n",
    "import numpy as np\n",
    "\n",
    "def chinese_word_cut(mytext):\n",
    "    tempcut = jieba.cut(str(mytext))\n",
    "    result = \" \".join(set(tempcut) - set(clearwords))\n",
    "    return result if result.strip() else np.nan\n",
    "\n",
    "\n",
    "\n",
    "#打印前n_top_words关键词\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "df = pd.read_csv(\"/Users/laihuiqian/weibo/total-all-100.csv\",index_col=None)\n",
    "# print(df)\n",
    "df[\"content_cutted\"] = df[\"0\"].apply(chinese_word_cut)\n",
    "# print(df)\n",
    "df = df.dropna(subset=['content_cutted'])\n",
    "\n",
    "print(df)\n",
    "\n",
    "# n_features = 5000\n",
    "\n",
    "# tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "#                                 max_features=n_features,\n",
    "#                                 stop_words='english',\n",
    "#                                 max_df = 2,\n",
    "#                                 min_df = 0.5)\n",
    "# tf = tf_vectorizer.fit_transform(df_result.content_cutted)\n",
    "# vocab=tf_vectorizer.get_feature_names_out()\n",
    "# model = lda.LDA(n_topics=17, n_iter=100, random_state=1)  \n",
    "# model.fit(tf)\n",
    "# print('model done')\n",
    "\n",
    "# #主题-单词（topic-word）分布\n",
    "# topic_word = model.topic_word_ \n",
    "# print(\"shape: {}\".format(topic_word.shape))\n",
    "# print(vocab[:3])\n",
    "# print(topic_word[:, :3])\n",
    "# for n in range(12):\n",
    "#     sum_pr = sum(topic_word[n,:])  \n",
    "#     print(\"topic: {} sum: {}\".format(n, sum_pr))\n",
    "\n",
    "# #计算各主题Top-N个单词\n",
    "# import numpy as np\n",
    "# n = 20\n",
    "# for i, topic_dist in enumerate(topic_word):  \n",
    "#     topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]  \n",
    "#     print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dae6685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       0\n",
      "0      身体护理好物分享 姐妹们，身体头皮的护理和我们面部一样重要哦！从头到脚都不能放过！ 最近一个...\n",
      "1      永远自由如风~#今天穿什么##ootd# #微博变美手册# #种草# 博尔塔拉·赛里木湖  ...\n",
      "2      PIPIs SHARING | 早秋护肤分享秋天的第一份护肤品请查收～#好物分享##好物推荐...\n",
      "3                   小bus也太可爱了吧！#好物推荐##好物分享#  [组图共9张] 原图 \n",
      "4         与生命力对话#OOTD##今天穿什么# #微博变美手册# #种草#  [组图共9张] 原图 \n",
      "...                                                  ...\n",
      "26467                                                NaN\n",
      "26468                                                NaN\n",
      "26469                                                NaN\n",
      "26470                                                NaN\n",
      "26471                                                NaN\n",
      "\n",
      "[26472 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "def open_dict(Dict):\n",
    "    path = '%s.txt' % Dict\n",
    "    dictionary = open(path, 'r', encoding='utf-8')\n",
    "    dict = []\n",
    "    for word in dictionary:\n",
    "        word = word.strip('\\n')\n",
    "        dict.append(word)\n",
    "    return dict\n",
    "\n",
    "clearwords=open_dict('clearwords')\n",
    "\n",
    "#分词\n",
    "import numpy as np\n",
    "\n",
    "def chinese_word_cut(mytext):\n",
    "    tempcut = jieba.cut(str(mytext))\n",
    "    result = \" \".join(set(tempcut) - set(clearwords))\n",
    "    return result if result.strip() else np.nan\n",
    "\n",
    "\n",
    "\n",
    "#打印前n_top_words关键词\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "df = pd.read_csv(\"/Users/laihuiqian/weibo/total-all-100.csv\",index_col=None)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87e3dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       0\n",
      "0      身体护理好物分享 姐妹们，身体头皮的护理和我们面部一样重要哦！从头到脚都不能放过！ 最近一个...\n",
      "1      永远自由如风~#今天穿什么##ootd# #微博变美手册# #种草# 博尔塔拉·赛里木湖  ...\n",
      "2      PIPIs SHARING | 早秋护肤分享秋天的第一份护肤品请查收～#好物分享##好物推荐...\n",
      "3                   小bus也太可爱了吧！#好物推荐##好物分享#  [组图共9张] 原图 \n",
      "4         与生命力对话#OOTD##今天穿什么# #微博变美手册# #种草#  [组图共9张] 原图 \n",
      "...                                                  ...\n",
      "18722  前几天更新的《稳，就赢了》视频，应该有不少小鹏友看过了吼~时长太短看不够？更多虎年稳中求胜的...\n",
      "18723  刚刚拍摄完@GUCCI 中国新年系列的平面照，一整个被大种草！！！so，我直接空降Gucci...\n",
      "18724  时间过得真快，还有半个月就要到虎年春节啦，迎来全年间最有团圆氛围感的节日之一！和家人老友相聚...\n",
      "18725  这期视频有八卦❗❗❗好朋友➕开盲盒➕真心话➕开箱＝快乐！哈哈哈🤣  #一起开盲盒有多欢乐#付...\n",
      "18726  送礼博主上线！开年的礼物请收下参与规则在评论区~btw，视频很快就更新啦~  [组图共18张...\n",
      "\n",
      "[18028 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=[\"0\"], inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3978cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "df[\"content_cutted\"] = df[\"0\"].apply(chinese_word_cut)\n",
    "n_features = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "783273df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "INFO:lda:n_documents: 18028\n",
      "INFO:lda:vocab_size: 5000\n",
      "INFO:lda:n_words: 401136\n",
      "INFO:lda:n_topics: 17\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -4853181\n",
      "INFO:lda:<10> log likelihood: -3573612\n",
      "INFO:lda:<20> log likelihood: -3388560\n",
      "INFO:lda:<30> log likelihood: -3321124\n",
      "INFO:lda:<40> log likelihood: -3288816\n",
      "INFO:lda:<50> log likelihood: -3271235\n",
      "INFO:lda:<60> log likelihood: -3258678\n",
      "INFO:lda:<70> log likelihood: -3250365\n",
      "INFO:lda:<80> log likelihood: -3241887\n",
      "INFO:lda:<90> log likelihood: -3236115\n",
      "INFO:lda:<99> log likelihood: -3232944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model done\n",
      "shape: (17, 5000)\n",
      "['00' '01' '02']\n",
      "[[2.51711639e-07 2.51711639e-07 2.51711639e-07]\n",
      " [4.72567459e-07 4.72567459e-07 4.72567459e-07]\n",
      " [6.96281855e-07 6.96281855e-07 6.96281855e-07]\n",
      " [5.23669879e-07 5.23669879e-07 5.23669879e-07]\n",
      " [2.76583988e-04 6.75650786e-04 8.90532908e-04]\n",
      " [4.16857726e-07 4.17274584e-04 4.16857726e-07]\n",
      " [5.32594802e-07 5.32594802e-07 5.32594802e-07]\n",
      " [5.47555166e-07 1.10058588e-04 5.47555166e-07]\n",
      " [3.25563224e-07 3.25563224e-07 3.25563224e-07]\n",
      " [5.71624557e-07 5.71624557e-07 5.71624557e-07]\n",
      " [5.80147357e-07 5.80147357e-07 5.80147357e-07]\n",
      " [4.08379957e-07 4.08379957e-07 4.08379957e-07]\n",
      " [3.64365094e-07 3.64365094e-07 3.64365094e-07]\n",
      " [3.96982930e-07 3.96982930e-07 3.96982930e-07]\n",
      " [8.49185472e-03 4.61488763e-07 4.61488763e-07]\n",
      " [5.16715755e-07 5.16715755e-07 5.16715755e-07]\n",
      " [3.28471948e-07 3.28471948e-07 3.28471948e-07]]\n",
      "topic: 0 sum: 1.0000000000000904\n",
      "topic: 1 sum: 1.0000000000000693\n",
      "topic: 2 sum: 0.9999999999998679\n",
      "topic: 3 sum: 0.9999999999998984\n",
      "topic: 4 sum: 1.0000000000000346\n",
      "topic: 5 sum: 0.9999999999999659\n",
      "topic: 6 sum: 1.0000000000001112\n",
      "topic: 7 sum: 1.0000000000000813\n",
      "topic: 8 sum: 0.9999999999999947\n",
      "topic: 9 sum: 1.0000000000000608\n",
      "topic: 10 sum: 0.9999999999998642\n",
      "topic: 11 sum: 1.0000000000000395\n",
      "*Topic 0\n",
      "- 质地 而且 真的 可以 效果 皮肤 不会 夏天 这个 滋润 非常 完全 头发 清爽 吸收 好物 就是 一样 保湿 里面\n",
      "*Topic 1\n",
      "- 香水 浪漫 味道 玫瑰 温柔 香气 清新 花香 喜欢 高级 感受 木质 生活 元素 限定 适合 气息 美好 心动 氛围\n",
      "*Topic 2\n",
      "- 娱乐 迷妹 互动 粉丝 大家 真的 评论 多多 私信 你们 可以 宝贝 日常 加入 美妆 恭喜 演唱会 上榜 继续 哈哈哈\n",
      "*Topic 3\n",
      "- plog 王国 潼话 日常 日记 生活 快乐 博主 碎片 最近 真的 vlog 告别 你们 12 超级 记录 10 朋友 旅行\n",
      "*Topic 4\n",
      "- 妆容 口红 眼影 真的 look 出街 氛围 颜色 高级 搭配 美妆 腮红 春季 适合 今日 就是 这个 温柔 可以 今天\n",
      "*Topic 5\n",
      "- 链接 还有 京东 一起 11 天猫 心动 姐妹 真的 直接 搜索 大牌 就是 可以 礼物 白雪 限定 好物 送礼 超级\n",
      "*Topic 6\n",
      "- 攻略 计划 完美 美妆 新年 开春 好颜 妆容 化妆 今日 氛围 今天 出游 春夏 教程 夏日 挑战 变美 姐妹 造型\n",
      "*Topic 7\n",
      "- 生活 vlog 大家 今天 你们 分享 什么 变美 如何 记录 自己 真的 一起 开学 琼琼 终于 chili 手册 一些 薄荷\n",
      "*Topic 8\n",
      "- 自己 没有 就是 不是 可以 我们 什么 知道 如果 这个 因为 还是 一直 一样 这样 那些 看到 觉得 所有 每日\n",
      "*Topic 9\n",
      "- 新年 快乐 礼盒 品牌 开箱 感谢 大家 礼物 收到 分享 2022 计划 各位 一年 完美 一起 美妆 2023 你们 抽奖\n",
      "*Topic 10\n",
      "- ootd 今日 显示 地图 妆容 今天 美妆 生活 夏日 夏天 春天 分享 什么 上海 一种 时尚 日常 成都 快乐 大会\n",
      "*Topic 11\n",
      "- 真的 这个 可以 喜欢 还是 觉得 就是 感觉 这么 没有 有点 但是 非常 还有 已经 时候 这种 特别 自己 不错\n",
      "*Topic 12\n",
      "- 自己 我们 大家 可以 很多 真的 就是 工作 所以 姐妹 因为 一定 还是 现在 不要 需要 重要 起来 时候 今天\n",
      "*Topic 13\n",
      "- 分享 好物 推荐 护肤 种草 美妆 抽奖 最近 你们 babe 豆豆 大家 爱用物 彩妆 一些 看看 宝藏 爱用 购物 近期\n",
      "*Topic 14\n",
      "- 一起 20 活动 可以 体验 00 还有 10 这次 现场 感受 19 打卡 系列 12 90 15 官方 品牌 旗舰店\n",
      "*Topic 15\n",
      "- 精华 护肤 面霜 修护 分享 皮肤 换季 肌肤 保湿 雅诗兰黛 抗老 真的 眼霜 美妆 敏感 维稳 可以 护肤品 状态 补水\n",
      "*Topic 16\n",
      "- 肌肤 皮肤 护肤 成分 状态 熬夜 姐妹 抗老 紧致 效果 就是 坚持 修护 改善 问题 我们 美白 现在 吸收 真的\n"
     ]
    }
   ],
   "source": [
    "n_features = 5000\n",
    "\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "tf = tf_vectorizer.fit_transform(df[\"content_cutted\"])\n",
    "vocab=tf_vectorizer.get_feature_names_out()\n",
    "model = lda.LDA(n_topics=17, n_iter=100, random_state=1)  \n",
    "model.fit(tf)\n",
    "print('model done')\n",
    "\n",
    "#主题-单词（topic-word）分布\n",
    "topic_word = model.topic_word_ \n",
    "print(\"shape: {}\".format(topic_word.shape))\n",
    "print(vocab[:3])\n",
    "print(topic_word[:, :3])\n",
    "for n in range(12):\n",
    "    sum_pr = sum(topic_word[n,:])  \n",
    "    print(\"topic: {} sum: {}\".format(n, sum_pr))\n",
    "\n",
    "#计算各主题Top-N个单词\n",
    "import numpy as np\n",
    "n = 20\n",
    "for i, topic_dist in enumerate(topic_word):  \n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]  \n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "560c7024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "INFO:lda:n_documents: 18028\n",
      "INFO:lda:vocab_size: 5000\n",
      "INFO:lda:n_words: 401136\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -4480016\n",
      "INFO:lda:<10> log likelihood: -3506618\n",
      "INFO:lda:<20> log likelihood: -3344361\n",
      "INFO:lda:<30> log likelihood: -3284851\n",
      "INFO:lda:<40> log likelihood: -3253054\n",
      "INFO:lda:<50> log likelihood: -3236282\n",
      "INFO:lda:<60> log likelihood: -3224952\n",
      "INFO:lda:<70> log likelihood: -3218583\n",
      "INFO:lda:<80> log likelihood: -3212808\n",
      "INFO:lda:<90> log likelihood: -3207587\n",
      "INFO:lda:<99> log likelihood: -3203813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model done\n",
      "shape: (10, 5000)\n",
      "['00' '01' '02']\n",
      "[[1.46720073e-07 1.46720073e-07 1.46720073e-07]\n",
      " [2.92387350e-03 8.94949450e-05 1.56448769e-04]\n",
      " [2.66173126e-03 1.45642812e-04 4.83863163e-07]\n",
      " [3.18582943e-07 3.18582943e-07 3.18582943e-07]\n",
      " [1.74721468e-04 6.73213529e-04 5.48590514e-04]\n",
      " [3.24970753e-07 3.24970753e-07 3.24970753e-07]\n",
      " [2.06598765e-07 2.06598765e-07 2.06598765e-07]\n",
      " [3.31334283e-07 3.31334283e-07 3.31334283e-07]\n",
      " [1.84097645e-07 1.84097645e-07 1.84097645e-07]\n",
      " [3.04692261e-07 3.04692261e-07 3.04692261e-07]]\n",
      "topic: 0 sum: 0.9999999999999714\n",
      "topic: 1 sum: 1.0000000000000386\n",
      "topic: 2 sum: 1.0000000000000848\n",
      "topic: 3 sum: 0.9999999999999264\n",
      "topic: 4 sum: 0.9999999999999167\n",
      "topic: 5 sum: 1.000000000000033\n",
      "topic: 6 sum: 1.000000000000005\n",
      "topic: 7 sum: 0.9999999999999262\n",
      "topic: 8 sum: 0.9999999999999364\n",
      "topic: 9 sum: 1.0000000000000995\n",
      "*Topic 0\n",
      "- 护肤 皮肤 精华 肌肤 效果 成分 修护 可以 抗老 保湿\n",
      "*Topic 1\n",
      "- 可以 一起 感受 还有 礼物 系列 限定 浪漫 自己 礼盒\n",
      "*Topic 2\n",
      "- 互动 大家 一起 你们 粉丝 可以 评论 美妆 日常 福利\n",
      "*Topic 3\n",
      "- ootd 今日 妆容 今天 显示 地图 春天 look 真的 美妆\n",
      "*Topic 4\n",
      "- 妆容 美妆 真的 口红 眼影 底妆 这个 今日 腮红 氛围\n",
      "*Topic 5\n",
      "- plog 快乐 生活 日常 王国 潼话 日记 博主 碎片 感谢\n",
      "*Topic 6\n",
      "- 娱乐 迷妹 真的 没有 自己 就是 这个 什么 不是 知道\n",
      "*Topic 7\n",
      "- 新年 计划 完美 大家 攻略 分享 开春 今天 好颜 你们\n",
      "*Topic 8\n",
      "- 链接 现在 我们 还有 姐妹 就是 真的 可以 一起 自己\n",
      "*Topic 9\n",
      "- 分享 好物 推荐 最近 抽奖 种草 你们 真的 夏天 头发\n"
     ]
    }
   ],
   "source": [
    "n_features = 5000\n",
    "\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "tf = tf_vectorizer.fit_transform(df[\"content_cutted\"])\n",
    "vocab=tf_vectorizer.get_feature_names_out()\n",
    "model = lda.LDA(n_topics=10, n_iter=100, random_state=1)  \n",
    "model.fit(tf)\n",
    "print('model done')\n",
    "\n",
    "#主题-单词（topic-word）分布\n",
    "topic_word = model.topic_word_ \n",
    "print(\"shape: {}\".format(topic_word.shape))\n",
    "print(vocab[:3])\n",
    "print(topic_word[:, :3])\n",
    "for n in range(10):\n",
    "    sum_pr = sum(topic_word[n,:])  \n",
    "    print(\"topic: {} sum: {}\".format(n, sum_pr))\n",
    "\n",
    "#计算各主题Top-N个单词\n",
    "import numpy as np\n",
    "n = 10\n",
    "for i, topic_dist in enumerate(topic_word):  \n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]  \n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8ac75a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# 周小仙yoo的微博视频', '# 周小仙yoo的微博', '#周小仙yoo的微博视频', '#周小仙yoo的微博', '周小仙yoo', '# 张进ZJZJ的微博视频', '# 张进ZJZJ的微博', '#张进ZJZJ的微博视频', '#张进ZJZJ的微博', '张进ZJZJ', '# 陈大皮儿的微博视频', '# 陈大皮儿的微博', '#陈大皮儿的微博视频', '#陈大皮儿的微博', '陈大皮儿', '# 马锐的微博视频', '# 马锐的微博', '#马锐的微博视频', '#马锐的微博', '马锐', '# Kiko成晨的微博视频', '# Kiko成晨的微博', '#Kiko成晨的微博视频', '#Kiko成晨的微博', 'Kiko成晨', '# 彤彤_sakura的微博视频', '# 彤彤_sakura的微博', '#彤彤_sakura的微博视频', '#彤彤_sakura的微博', '彤彤_sakura', '# 手边巴黎urruolan的微博视频', '# 手边巴黎urruolan的微博', '#手边巴黎urruolan的微博视频', '#手边巴黎urruolan的微博', '手边巴黎urruolan', '# 叫我大表哥好吗好的的微博视频', '# 叫我大表哥好吗好的的微博', '#叫我大表哥好吗好的的微博视频', '#叫我大表哥好吗好的的微博', '叫我大表哥好吗好的', '# 牛明昱的微博视频', '# 牛明昱的微博', '#牛明昱的微博视频', '#牛明昱的微博', '牛明昱', '# LookLana的微博视频', '# LookLana的微博', '#LookLana的微博视频', '#LookLana的微博', 'LookLana', '# 孙一玮Well的微博视频', '# 孙一玮Well的微博', '#孙一玮Well的微博视频', '#孙一玮Well的微博', '孙一玮Well', '# Hedy北北的微博视频', '# Hedy北北的微博', '#Hedy北北的微博视频', '#Hedy北北的微博', 'Hedy北北', '# Ruby幼熙的微博视频', '# Ruby幼熙的微博', '#Ruby幼熙的微博视频', '#Ruby幼熙的微博', 'Ruby幼熙', '# 蒲一雯iwen的微博视频', '# 蒲一雯iwen的微博', '#蒲一雯iwen的微博视频', '#蒲一雯iwen的微博', '蒲一雯iwen', '# 美少女毛容易的微博视频', '# 美少女毛容易的微博', '#美少女毛容易的微博视频', '#美少女毛容易的微博', '美少女毛容易', '# Joey土播鼠的微博视频', '# Joey土播鼠的微博', '#Joey土播鼠的微博视频', '#Joey土播鼠的微博', 'Joey土播鼠', '# 章解放_的微博视频', '# 章解放_的微博', '#章解放_的微博视频', '#章解放_的微博', '章解放_', '# 一枝南南的微博视频', '# 一枝南南的微博', '#一枝南南的微博视频', '#一枝南南的微博', '一枝南南', '# 潘白雪s的微博视频', '# 潘白雪s的微博', '#潘白雪s的微博视频', '#潘白雪s的微博', '潘白雪s', '# 一番fane的微博视频', '# 一番fane的微博', '#一番fane的微博视频', '#一番fane的微博', '一番fane', '# 美少女Lisa酱的微博视频', '# 美少女Lisa酱的微博', '#美少女Lisa酱的微博视频', '#美少女Lisa酱的微博', '美少女Lisa酱', '# 潘南竹呀的微博视频', '# 潘南竹呀的微博', '#潘南竹呀的微博视频', '#潘南竹呀的微博', '潘南竹呀', '# 小考拉Lake的微博视频', '# 小考拉Lake的微博', '#小考拉Lake的微博视频', '#小考拉Lake的微博', '小考拉Lake', '# 周姊伊的微博视频', '# 周姊伊的微博', '#周姊伊的微博视频', '#周姊伊的微博', '周姊伊', '# 种草达人绵绵酱的微博视频', '# 种草达人绵绵酱的微博', '#种草达人绵绵酱的微博视频', '#种草达人绵绵酱的微博', '种草达人绵绵酱', '# Yuki_优酱的微博视频', '# Yuki_优酱的微博', '#Yuki_优酱的微博视频', '#Yuki_优酱的微博', 'Yuki_优酱', '# 兰阿雯的微博视频', '# 兰阿雯的微博', '#兰阿雯的微博视频', '#兰阿雯的微博', '兰阿雯', '# 优莉Uli的微博视频', '# 优莉Uli的微博', '#优莉Uli的微博视频', '#优莉Uli的微博', '优莉Uli', '# Echo桃小小的微博视频', '# Echo桃小小的微博', '#Echo桃小小的微博视频', '#Echo桃小小的微博', 'Echo桃小小', '# 呦呦仔的微博视频', '# 呦呦仔的微博', '#呦呦仔的微博视频', '#呦呦仔的微博', '呦呦仔', '# 宋素雯的微博视频', '# 宋素雯的微博', '#宋素雯的微博视频', '#宋素雯的微博', '宋素雯', '# 月野皮皮的微博视频', '# 月野皮皮的微博', '#月野皮皮的微博视频', '#月野皮皮的微博', '月野皮皮', '# 美硕的成分测评的微博视频', '# 美硕的成分测评的微博', '#美硕的成分测评的微博视频', '#美硕的成分测评的微博', '美硕的成分测评', '# 桃子百莉的微博视频', '# 桃子百莉的微博', '#桃子百莉的微博视频', '#桃子百莉的微博', '桃子百莉', '# 苍口小梨涡的微博视频', '# 苍口小梨涡的微博', '#苍口小梨涡的微博视频', '#苍口小梨涡的微博', '苍口小梨涡', '# Fairy小默的微博视频', '# Fairy小默的微博', '#Fairy小默的微博视频', '#Fairy小默的微博', 'Fairy小默', '# 莓灵酱的微博视频', '# 莓灵酱的微博', '#莓灵酱的微博视频', '#莓灵酱的微博', '莓灵酱', '# 你Rui哥的微博视频', '# 你Rui哥的微博', '#你Rui哥的微博视频', '#你Rui哥的微博', '你Rui哥', '# 刘魔王大人-的微博视频', '# 刘魔王大人-的微博', '#刘魔王大人-的微博视频', '#刘魔王大人-的微博', '刘魔王大人-', '# 蟹阿文AWEN的微博视频', '# 蟹阿文AWEN的微博', '#蟹阿文AWEN的微博视频', '#蟹阿文AWEN的微博', '蟹阿文AWEN', '# 陈端端儿的微博视频', '# 陈端端儿的微博', '#陈端端儿的微博视频', '#陈端端儿的微博', '陈端端儿', '# 迟池Chichi的微博视频', '# 迟池Chichi的微博', '#迟池Chichi的微博视频', '#迟池Chichi的微博', '迟池Chichi', '# 叫我桃maymay的微博视频', '# 叫我桃maymay的微博', '#叫我桃maymay的微博视频', '#叫我桃maymay的微博', '叫我桃maymay', '# 种草颜究生的微博视频', '# 种草颜究生的微博', '#种草颜究生的微博视频', '#种草颜究生的微博', '种草颜究生']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "blogger_names = [\n",
    "\"周小仙yoo\", \"张进ZJZJ\", \"陈大皮儿\", \"马锐\", \n",
    "\"Kiko成晨\", \"彤彤_sakura\",\"手边巴黎urruolan\", \"叫我大表哥好吗好的\", \n",
    "\"牛明昱\", \"LookLana\",\"孙一玮Well\", \"Hedy北北\", \n",
    "\"Ruby幼熙\",\"蒲一雯iwen\", \"美少女毛容易\", \"Joey土播鼠\", \n",
    "\"章解放_\", \"一枝南南\", \"潘白雪s\",\"一番fane\",\n",
    "\"美少女Lisa酱\",\"潘南竹呀\",\"小考拉Lake\", \"周姊伊\",\n",
    "\"种草达人绵绵酱\", \"Yuki_优酱\", \"兰阿雯\", \"优莉Uli\",\n",
    "\"Echo桃小小\", \"呦呦仔\", \"宋素雯\", \"月野皮皮\", \n",
    "\"美硕的成分测评\", \"桃子百莉\",\"苍口小梨涡\", \"Fairy小默\", \n",
    "\"莓灵酱\", \"你Rui哥\", \"刘魔王大人-\", \"蟹阿文AWEN\",\n",
    "\"陈端端儿\", \"迟池Chichi\", \"叫我桃maymay\", \"种草颜究生\"\n",
    "]\n",
    "\n",
    "self_references = []\n",
    "\n",
    "for name in blogger_names:\n",
    "    reference1 = f\"# {name}的微博视频\"\n",
    "    reference2 = f\"# {name}的微博\"\n",
    "    reference3 = f\"#{name}的微博视频\"\n",
    "    reference4 = f\"#{name}的微博\"\n",
    "    reference5 = name\n",
    "    self_references.append(reference1)\n",
    "    self_references.append(reference2)\n",
    "    self_references.append(reference3)\n",
    "    self_references.append(reference4)\n",
    "    self_references.append(reference5)\n",
    "\n",
    "blogger_names_variants = [\n",
    "    \"周小仙yoo\", \"周小仙\", \"yoo\",\n",
    "    \"张进ZJZJ\", \"张进\", \"ZJZJ\", \"zjzj\",\n",
    "    \"陈大皮儿\",\"陈大皮\"\n",
    "    \"马锐\",\n",
    "    \n",
    "    \"Kiko成晨\", \"Kiko\", \"成晨\",\n",
    "    \"彤彤_sakura\", \"彤彤\", \"sakura\",\n",
    "    \"手边巴黎urruolan\", \"手边巴黎\", \"urruolan\",\n",
    "    \"叫我大表哥好吗好的\",\n",
    "    \n",
    "    \"牛明昱\",\n",
    "    \"LookLana\",\n",
    "    \"孙一玮Well\", \"孙一玮\", \"Well\",\"well\",\n",
    "    \"Hedy北北\", \"Hedy\", \"北北\",\"hedy\",\n",
    "    \n",
    "    \"Ruby幼熙\", \"Ruby\", \"幼熙\",\"ruby\",\n",
    "    \"蒲一雯iwen\", \"蒲一雯\", \"iwen\",\n",
    "    \"美少女毛容易\", \"毛容易\",\n",
    "    \"Joey土播鼠\", \"Joey\",\"joey\",\n",
    "    \n",
    "    \"章解放_\", \"章解放\",\n",
    "    \"一枝南南\", \"南南\", \"一枝\",\n",
    "    \"潘白雪s\", \"潘白雪\",\n",
    "    \"一番fane\",\"fane\",\"一番\",\n",
    "    \n",
    "    \"美少女Lisa酱\",\"Lisa酱\",\n",
    "    \"潘南竹呀\",\"潘南竹\",\"南竹\",\n",
    "    \"小考拉Lake\", \"小考拉\", \"Lake\",\"lake\",\n",
    "    \"周姊伊\",\n",
    "    \n",
    "    \"种草达人绵绵酱\", \"绵绵酱\",\n",
    "    \"Yuki_优酱\", \"Yuki\", \"优酱\",\n",
    "    \"兰阿雯\",\n",
    "    \"优莉Uli\", \"优莉\", \"Uli\",\"uli\",\n",
    "    \n",
    "    \"Echo桃小小\", \"Echo\", \"桃小小\",\"echo\",\n",
    "    \"呦呦仔\",\n",
    "    \"宋素雯\",\n",
    "    \"月野皮皮\", \"皮皮\", \"月野\",\n",
    "    \n",
    "    \"美硕的成分测评\", \"美硕\",\n",
    "    \"桃子百莉\", \"百莉\",\n",
    "    \"苍口小梨涡\",\n",
    "    \"Fairy小默\", \"Fairy\", \"小默\",\"fairy\",\n",
    "    \n",
    "    \"莓灵酱\",\n",
    "    \"你Rui哥\",\n",
    "    \"刘魔王大人-\",\n",
    "    \"蟹阿文AWEN\", \"蟹阿文\", \"AWEN\",\"awen\",\n",
    "    \n",
    "    \"陈端端儿\",\n",
    "    \"迟池Chichi\", \"迟池\", \"Chichi\",\"chi\",\"Chi\",\n",
    "    \"叫我桃maymay\", \"maymay\", \"may\",\"May\",\n",
    "    \"种草颜究生\"\n",
    "]\n",
    "\n",
    "\n",
    "print(self_references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c779baa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#彤彤_sakura的微博视频', '#月野皮皮的微博', '# 种草颜究生的微博视频', '# 美少女Lisa酱的微博视频', '# 月野皮皮的微博', '彤彤', '牛明昱', '#周小仙yoo的微博视频', '#叫我大表哥好吗好的的微博', '#一枝南南的微博视频', '一枝南南', '# 潘白雪s的微博', '迟池', '#Fairy小默的微博', '# 蒲一雯iwen的微博视频', '张进ZJZJ', '#Hedy北北的微博视频', '#Ruby幼熙的微博', '叫我大表哥好吗好的', '小考拉', '# Joey土播鼠的微博视频', '#蒲一雯iwen的微博', '绵绵酱', '# 桃子百莉的微博视频', 'Chichi', 'uli', '#苍口小梨涡的微博', '优酱', 'Ruby幼熙', '美少女Lisa酱', '#呦呦仔的微博', '#你Rui哥的微博', '#Hedy北北的微博', '# 种草达人绵绵酱的微博', '#宋素雯的微博视频', '陈大皮儿', '#刘魔王大人-的微博', '#Kiko成晨的微博视频', '# 周小仙yoo的微博', '# Joey土播鼠的微博', '#牛明昱的微博视频', '# 小考拉Lake的微博视频', '#一番fane的微博视频', '潘白雪s', '#牛明昱的微博', '#小考拉Lake的微博视频', '# 莓灵酱的微博', '#Kiko成晨的微博', '# 潘白雪s的微博视频', 'joey', '# 小考拉Lake的微博', '#Yuki_优酱的微博视频', '# 马锐的微博', '# 马锐的微博视频', '# 宋素雯的微博', '周小仙', '# 兰阿雯的微博', '美硕的成分测评', '#Echo桃小小的微博视频', '#优莉Uli的微博', 'echo', '#陈大皮儿的微博视频', '蟹阿文AWEN', '# LookLana的微博视频', '#你Rui哥的微博视频', '#美少女Lisa酱的微博视频', '# LookLana的微博', '# 你Rui哥的微博视频', '# 美硕的成分测评的微博视频', '#苍口小梨涡的微博视频', '孙一玮', '#Ruby幼熙的微博视频', 'Joey', '# 一番fane的微博', '#叫我大表哥好吗好的的微博视频', '#章解放_的微博', '# 陈大皮儿的微博视频', '百莉', '桃子百莉', 'Hedy', '南竹', '#张进ZJZJ的微博', 'may', '# 叫我桃maymay的微博', '# 张进ZJZJ的微博', '# Echo桃小小的微博视频', '# 你Rui哥的微博', '# 周姊伊的微博视频', '#兰阿雯的微博', '# 蟹阿文AWEN的微博', '陈端端儿', 'Fairy', '# 孙一玮Well的微博视频', '# 美硕的成分测评的微博', '一番', '蟹阿文', '# 优莉Uli的微博', '#刘魔王大人-的微博视频', '#月野皮皮的微博视频', '#Echo桃小小的微博', '潘白雪', '#蟹阿文AWEN的微博视频', '# Ruby幼熙的微博视频', 'Joey土播鼠', '# Yuki_优酱的微博视频', 'Echo桃小小', '#迟池Chichi的微博视频', '小考拉Lake', '# 一番fane的微博视频', 'chi', 'ruby', '# Yuki_优酱的微博', '宋素雯', '#潘白雪s的微博视频', '苍口小梨涡', '#一枝南南的微博', 'Uli', '# 手边巴黎urruolan的微博视频', '# 叫我大表哥好吗好的的微博视频', '#陈端端儿的微博视频', '# 手边巴黎urruolan的微博', '#种草颜究生的微博', '迟池Chichi', '# Fairy小默的微博', '皮皮', '#蒲一雯iwen的微博视频', '孙一玮Well', '成晨', 'urruolan', '#呦呦仔的微博视频', '月野', '张进', 'Kiko成晨', 'Hedy北北', '# Echo桃小小的微博', '# 牛明昱的微博视频', '#章解放_的微博视频', '章解放_', '# 潘南竹呀的微博视频', '#LookLana的微博视频', '# Ruby幼熙的微博', '#潘南竹呀的微博视频', '#迟池Chichi的微博', '章解放', 'Ruby', '#Joey土播鼠的微博', '# 兰阿雯的微博视频', '# Kiko成晨的微博', '#优莉Uli的微博视频', '#宋素雯的微博', 'Chi', '#Yuki_优酱的微博', '月野皮皮', '美硕', '# 牛明昱的微博', 'iwen', 'LookLana', 'ZJZJ', '马锐', '#美硕的成分测评的微博视频', '# 陈端端儿的微博', '#叫我桃maymay的微博', 'AWEN', '# 一枝南南的微博', '莓灵酱', '# 章解放_的微博', '# 迟池Chichi的微博视频', '# Fairy小默的微博视频', '毛容易', 'fairy', 'awen', '#蟹阿文AWEN的微博', '#LookLana的微博', '#种草达人绵绵酱的微博', 'sakura', '#孙一玮Well的微博', '#种草达人绵绵酱的微博视频', '#潘白雪s的微博', '#一番fane的微博', '# 刘魔王大人-的微博视频', '# 蟹阿文AWEN的微博视频', '#兰阿雯的微博视频', '# Hedy北北的微博', '# 月野皮皮的微博视频', '#彤彤_sakura的微博', '#小考拉Lake的微博', '# 莓灵酱的微博视频', 'May', '#桃子百莉的微博视频', '# 周姊伊的微博', 'maymay', '# 彤彤_sakura的微博', '#手边巴黎urruolan的微博视频', '# 叫我大表哥好吗好的的微博', '#Joey土播鼠的微博视频', '陈大皮马锐', 'well', '#美少女Lisa酱的微博', '一枝', '#孙一玮Well的微博视频', '# 陈端端儿的微博视频', '兰阿雯', '蒲一雯', '彤彤_sakura', '#陈大皮儿的微博', '一番fane', '蒲一雯iwen', '#潘南竹呀的微博', '手边巴黎urruolan', 'Well', '#莓灵酱的微博视频', '#Fairy小默的微博视频', '# 彤彤_sakura的微博视频', '优莉Uli', '#张进ZJZJ的微博视频', 'Lisa酱', '# 美少女Lisa酱的微博', '# 呦呦仔的微博视频', '#美少女毛容易的微博', 'Yuki_优酱', '美少女毛容易', '# 呦呦仔的微博', '周姊伊', '# 优莉Uli的微博视频', '手边巴黎', '#陈端端儿的微博', '# 美少女毛容易的微博', '幼熙', 'Yuki', 'Kiko', '#周姊伊的微博视频', '#周姊伊的微博', '#种草颜究生的微博视频', '#马锐的微博', '种草颜究生', '#叫我桃maymay的微博视频', 'fane', '北北', '刘魔王大人-', '# 陈大皮儿的微博', '# 蒲一雯iwen的微博', '# 一枝南南的微博视频', '# 桃子百莉的微博', '周小仙yoo', '#莓灵酱的微博', 'zjzj', '# 周小仙yoo的微博视频', '呦呦仔', '#美硕的成分测评的微博', 'lake', '# 种草颜究生的微博', '# 苍口小梨涡的微博', '# 迟池Chichi的微博', '# 章解放_的微博视频', '# 刘魔王大人-的微博', 'yoo', '# 潘南竹呀的微博', '# 宋素雯的微博视频', '叫我桃maymay', '小默', '优莉', 'Lake', '# 张进ZJZJ的微博视频', 'hedy', '桃小小', '南南', '#桃子百莉的微博', '种草达人绵绵酱', '#马锐的微博视频', '# 孙一玮Well的微博', '# 苍口小梨涡的微博视频', '# Kiko成晨的微博视频', '# 种草达人绵绵酱的微博视频', '潘南竹呀', '#周小仙yoo的微博', '#美少女毛容易的微博视频', '你Rui哥', '# 叫我桃maymay的微博视频', 'Echo', '# Hedy北北的微博视频', '潘南竹', 'Fairy小默', '# 美少女毛容易的微博视频', '#手边巴黎urruolan的微博']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "combined_blogger_names = list(set(self_references + blogger_names_variants))\n",
    "print(combined_blogger_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec2c0efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['身体 护理 好物 分享 姐妹 身体 头皮 护理 面部 重要 从头到脚 不能 放过 最近 一个月 户外 暴晒 身体 美白 头皮 修护 更是 一个 工程 特别 sesderma 身体 乳帮 大忙 今天 分享 陪 一路 披荆斩棘 宝贝 好物 分享 身体 护理', '永远 自由 如风 今天 穿 ootd 微博变 美 手册 种草 博尔塔拉 赛里木湖', 'PIPIs SHARING 早秋 护肤 分享 秋天 第一份 护肤品 请 查收 好物 分享 好物 推荐 微博变 美 手册 护肤', 'bus 太 可爱 好物 推荐 好物 分享', '生命力 对话 OOTD 今天 穿 微博变 美 手册 种草', '疯狂 心动 华为 Mate Pro 简直 梦 中 情机 奶 fufu 南 糯 紫 真的 美到 心坎 里 最近 出门 会 带上 随手 记录 每个 瞬间 发现 华为 标志性 星环 设计 升级 不再 限缩 摄像头 扩展 整个 机身 弥漫着 一道道 优雅 弧线 建构 出 低调 隐秘 同心圆 设计 全球 首款 深 四曲 屏 终于 实现 屏幕 四角 曲度 一致 握感 舒适 加上 素皮 材质 后盖 设计 整天 想 捧 玩 手机 Mate 遥遥领先 特别 爱 紫色 神秘 高贵 感觉 手机 颜色 偏爱 紫色 手上 真的 秒 变 时髦 精 华为 南 糯 紫 有大 来头 取自 中国 六大 茶山 之首 南 糯 山上 覆盖 紫色 土壤 颜色 土壤 丰富 有机质 孕育出 无数 古 茶园 大地 而来 色彩 山河 美 融入 手机 概念 真的 太 惊艳 最近 想 入手 性能 颜值 兼具 手机 华为 Mate Pro 真的 绝绝子 拿在手上 存在 感 强 拍照 起来 色调 氛围 感 炒鸡 好看 Mate 绝美', '月 收到 超爱 宝贝 好物 分享 好物 推荐 微博变 美 手册 种草', '最近 幸福 法国 彭真 太 适合 送 对象 美出 圈 好物 分享 上海 Jean Georges', '最近 爱死 抗老 cp 美出 圈 好物 推荐', '吐鲁番 火热 美出 圈 今天 穿', 'PIPIs SHARING 化妆包 分享 长 达 半个 多月 旅程 看看 化妆包 里 塞 宝贝 美出 圈 无平替 大牌 化妆品', '总要 新疆 走 一趟 美出 圈 七夕 快乐 博尔塔拉 赛里木湖', 'PIPIs SHARING 礼盒 开箱 下饭 视频 看看 月 收到 宝贝 美出 圈 七夕 快乐', '最近 喜爱 香 CHLOE 经典 女士 香氛 玫瑰 芬芳 加上 牡丹 荔枝 小苍兰 瞬间 感受 百花齐放 魅力 玫瑰 香 永远都是 女生 入门 香 首选 CHLOE 仙境 花园 系列 微醺 芙蓉 一支 带有 果香 植物 花香 微甜 清新 太 适合 夏天 木兰 诗语 清新 淡雅 木兰 略带 淡淡的 柠檬 香气 夏日 里 一股 清流 阿蒂仙 寻找 蝴蝶 冥府 路 缪斯 黑莓 这三只 出游 带 便携 装 尤其 寻找 蝴蝶 冥府 路 真的 太 戳 美出 圈 七夕 约会 前', '收到 七夕 礼物 太美 惹 美出 圈 七夕 约会 前 运动 界 热烈 告白 BYYOURSIDE 身边', '感受 灵感 视界 美出 圈 找到 妆点', 'PIPIs SHARING 夏日 妆前 打底 分享 最爱 妆 前 打底 夏天 必须 拥有 美出 圈 无平替 大牌 化妆品', '买 两只 超爱 新色 本命 色 那种 狠狠 锁住 嘴 最爱 MAC 冰 可可 初秋 一个 冰 可可 我会 伤心 质地 真的 颠覆 子弹头 刻板 形象 只能 说 买回去 惊喜 闭眼 NARS SHOW OFF 家 嘟嘟 唇 质地 太 喜欢 嘴 没有 存在 全文', 'PIPIs SHARING 三伏天 养出 皮肤 年纪 增长 真的 知道 保养 身体 重要 身体 皮肤 真的 肉眼 变好 冲刺 最后 一伏 美出 圈 护肤 课堂', '浮生 若梦 彩妆 艺术展 太出 片 怒 拍 美出 圈 熠熠 人生 无畏 向前', '哇哇 最近 超爱 底妆 cp 干皮 夏天 想要 润 持妆 今日 妆容 好物 推荐', '一场 精美绝伦 彩妆 大赛 太 养眼 每日 穿 搭 今日 妆容', '跟随 张钧 甯 来到 普吉岛 香奈儿 国际 沟通 总监 Armelle 成分 创新 研发 总监 Nicola 一起 探讨 香奈儿 一号 红 山茶花 系列 中 蕴含 自然 能量 更深 感受 红 山茶花 护肤 奥秘 非凡 功效 全新 香奈儿 一号 红 山茶花 精华液 升级 ml 更好 满足 长期 抗氧化 护肤 需求 坚持 使用 六个月 肌肤 活力 upupup 光泽 弹性 得到 提升 细腻 毛孔 淡化 干纹 细纹 不在话下 肤感 超级 棒', '地中海 水蔓 邂逅 记忆 中 美好 美出 圈 暑期 变美逆袭', '终于 找到 旅行 本命 护发 伴侣 旅游 离不开 小样 从来 带 登 机箱 护肤品 护发 产品 中 小样 真的 每次 必带 轻松 方便 暑假 出游 宝贝 赶紧 囤 起来 美出 圈 暑期 变美逆袭', 'PIPIs SHARING 夏日 香香 女孩 今天 一期 味道 视频 看看 夏天 私藏 香 美出 圈 暑期 变美逆袭', 'DRAMA BY YOU 美出 圈 暑期 变美逆袭', 'PIPIs SHARING 完美 底妆 秘诀 夏天 底妆 一整天 在线 妆前 护肤 重要 摆脱 卡粉 拒绝 脱妆 美出 圈 暑期 变美逆袭', '今天 香香 女孩 美出 圈 暑期 变美逆袭', '最近 挖 组合 茶灵 保湿 水 老朋友 小棕瓶 保湿 抗老 喜欢 茶 灵先 湿 敷 直接 精华 吸收 真的 超级 皮肤 嫩嫩的 美出 圈 暑期 变美逆袭', '邂逅 沙美 大楼 浪漫 紫色花 海 美出 圈 暑期 变美逆袭', 'PIPIs SHARING 夏日 防晒 作战 化学 防晒 物理 防晒 抗老 第一道 防线 夏天 抗住 美出 圈 暑期 变美逆袭', '持妆不假 热爱 不变 美出 圈 暑期 变美逆袭', '最近 超爱 三只 美出 圈 暑期 变美逆袭', 'PIPIs SHARING 夏日 定妆 大法 底妆 一整天 在线 秘密 定妆 喷雾 散粉 绝对 夏季 定妆 王炸 美出 圈 暑期 变美逆袭', '姐妹 挑 一套 王炸 组合 入门 抗老 首选 美出 圈 暑期 变美逆袭', '六月 PIPI 粉丝 互动 榜 粉群 眼熟 榜来 宝子 萌快 康康 有没有 名字 上榜 滴 宝速来 私信 大皮 领取 兑奖 截止 时间 即日起 月 日 下午 点 橡皮 看好 时间 速来 领奖 赢 牛奶 七月 继续 大皮 多多 互动 也许 一个 幸运儿 彩虹 屁 粉群 互动 方式 加入 大皮 粉丝 群 大皮 专属 页 大皮 分享 日常生活 内容 ps 页 发布 生活 日常 近期 满意 自拍 近期 拍到 最美 日落 近期 美好 生活 片段 里 大皮 分享 大皮 会 不定期 互动 进群 链接 橡皮 团大皮 陈大 皮儿', '阳光 日子 今天 穿 OOTD', '最近 超爱 底妆 cp 锁住 好物 分享 今日 妆容', '浅过 一个 冬天 ERDOS 秋冬 新品 太棒了 鄂尔多斯 集团 秋冬 新品 预览 时尚 穿 搭', '找到 本命 洗发水 元气 姜 说 拯救 脱发 无限 回购 真的 亿 点点 爱 新入 pmpm 爱 角鲨烷 修护 精华 油 日常 必 囤 好物 推荐 好物 分享', '夜光 斑马', 'PIPIs SHARING 坚持 以油 养肤 以油 养肤 绝对 干皮星 必备 最近 发现 养肤 宝藏 新油 姐妹 赶紧 住 好物 推荐 好物 分享', 'PIPIs SHARING 购物 分享 购物 分享 第二 波来 买 东西 个个 精华 好物 推荐 好物 分享', '寰宇 古驰 典藏 展 太棒了', '姐妹 快 分享 几个 超爱 单品 好物 推荐 好物 分享', 'PIPIs SHARING 购物 分享 没有 买 很多 个个 精华 好物 分享 好物 推荐', '逛街 偶遇 超赞 摄影 作品 今天 穿 今日 妆容', '家 懂 雅顿 竟然 泡泡 玛特 联名 竟然 爱 pupu 欧莱雅 新品 唇膏 美翻 好物 推荐 今日 妆容', '囤 囤 囤 买买 买 节奏 姐妹 买 宝贝 快 分享 好物 分享 好物 推荐', '咯 咯 彩虹 屁 五月 PIPI 粉丝 互动 榜 粉群 眼熟 榜来 咯 宝子 萌快 康康 有没有 名字 恭喜 以下 六位 幸运 粉丝 欧气 少女 浅浅 浅 木土 兰牛寺 林嬉 bb 迟野 yoo 奶昔 Yun LL 金色 城堡 上榜 滴 宝速来 私信 大皮 领取 兑奖 截止 时间 即日起 月 日 下午 点 橡皮 看好 时间 速来 领奖 赢 牛奶 没有 上榜 滴宝 不要 气馁 六月 继续 大皮 互动 说不定 下次 幸运 鹅粉 群 互动 方式 加入 大皮 粉丝 群 大皮 专属 页 大皮 分享 日常生活 内容 ps 页 发布 生活 日常 近期 满意 自拍 近期 拍到 最美 日落 近期 美好 生活 片段 里 大皮 分享 大皮 会 不定期 互动 进群 链接 http t cn AiEnnuok 陈大 皮儿', '喝茶 养生 今日 妆容 每日 穿 搭', '宝贝 夏天 底妆 抗造 出门 硬 防晒 一定 做好 好物 分享 今天 穿', 'PIPIs VLOG 第三届 上海 咖啡 文化周 难得 一期 vlog 姐妹 一起 感受 上海 咖啡 文化 vlog vlog 生活', '开心 能够 受邀 参加 群星 计划 现场 布置 成 超级 浪漫 紫色 拍照 超级 出片 精美 下午茶 diy 手工 氛围 感 直接 拉满 现场 邀请 知名 医生 徐士亮 王颖 院长 徐院 分享 美学 知识 干货 重新 认识 一次 玻玻 相对 颗粒 型 玻 玻 更 建议 选择 凝胶 型 支撑力 强 组织 融合度 高 内部结构 稳定 容易 位移 维持 时间 久 自然 高效 达到 提升 塑形 效果 而王 院长 分享 情绪 美学 知识 现在 更 追求 个性 美 突出 美 需要 足够 了解 面部 特点 全面 部 一起 整体 情绪 问题 局部 最后 一定 牢记 三 正规 正规 医院 正规 资质 审美 在线 医生 国际 知名品牌 产品 问题 评论 区 留言 一起 变美', '熬夜 尽头 有夜 修 黄日 修 垮 王道 最近 疯狂 出差 工作 半夜 改 稿子 拍 视频 工作 完想 追剧 一不小心 天亮 脸蛋 子 垮 素颜 蜡黄 冠军 熬夜 脸 真的 无解 必须 采取 进阶 手段 击退 熬夜 黄 熬夜 垮 YSL 熬夜 双 精华 熬夜 上分 好伴侣 夜 皇后 精华 搭上 玻色 精华 真的 绝配 做到 H 日夜 分 时 修护 夜修 黄 日修 垮 轻松 恢复 肌肤 活力 真的 爱 熬夜 很配 夜间 YSL 夜 皇后 精华 夜间 抗氧化 王者 黄绝 高级 酸 仙人掌 花 精萃 双管齐下 高级 酸 快速 嫩肤 熬夜 生出 闭口 很快 消 下去 夜晚 开花 仙人掌 花里 萃取 出 仙人掌 花 精萃 抗氧去 黄 超绝 摇匀 后油 包水 设计 更亲 皮脂 膜 很快 渗透 进 肌肤 里 肌底 焕肤 黄气 退散 晚上 完 隔天 脸蛋 变得 透亮 很多 一个 社畜 工人 熬夜 作息 规律 获得 蜡黄 暗沉 肌 容易 水肿 垮 脸 长 细纹 白天 工作 拿出 最好 状态 应对 拍摄 工作 白天 修护 维稳 工作 非常 重要 日间 YSL 玻色 精华 日间 抗老 修护 维稳 必备 这破 脸蛋 子 回复 嘎嘎 棒 状态 早上 少不了 抗老 精华 拿捏 嘭 弹 胶原 肌 里面 欧莱雅 家 两大 抗老 顶流 成分 高浓度 玻色 鼠李糖 配合 协同 抗老 一起 松垮 肌肤 往上 拽 熬夜 休想 脸上 留下 松垮 痕迹 肤感 讨喜 有种 实打实 肌肤 补进 胶原蛋白 脸蛋 饱满 很多 自带 高光感 常常 熬夜 修仙 姐妹 放心 入 这组 稳稳 养肤 祛 黄 紧致 效果 真的 会 惊艳', 'PIPIs SHARING 姐姐 提升 颜值 秘诀 越活 越美 女人 终极目标 好物 分享 触动 心潮', 'PIPIs SHARING 敏肌 白白 朋友 精简 护肤 情况 美白 抗老 今天 分享 敏 肌 姐妹 不要 错过 好物 推荐 好物 分享', '最近 收到 好多 夏天 味道 香香 女孩 没错 好物 推荐 好物 分享', '送礼 资生堂 迪士尼 限定 礼盒 us 拒绝 今年 资生堂 迪士尼 限定 礼盒 不得不 说 唐老鸭 黛西 真的 太甜 天生 绝配 礼物 送给 爱人 恰到好处 礼盒 里 爱 悦薇 水乳 已经 几年 囤货 不断 护肤品 悦薇 水乳是 专为 亚洲 肌抗 初 老 研发 水乳 王者 越来越 懂 护肤 总得 选 适合 了解 皮肤 机制 有效 护肤 亚洲 肌 角质层 薄弱 容易 环境 护肤 不当 产生 干燥 粗糙 暗沉 问题 套悦薇 水乳 角质 调理 成分 AQIP 三重 调理 角质 清理 代谢 滋润 重塑 排列 规整 角质 天就养 角质 加上 玻 尿酸 Pro 深层 保湿 补水 共同 协作 平衡 修护 皮脂 膜 水乳 质地 清爽 素颜 做到 透嫩 水光 肌 状态 更 底气 特别 喜欢 用悦薇 水来 湿 敷 白天 化妆 前 敷 分钟 整个 脸 喝 饱水 悦薇 乳 轻盈 乳液 质地 一抹 脸 延展性 吸收 很快 双重 作用 滋养 肌肤 角质层 后续 上底 妆 会 特别 服帖 我肤质 姐妹 水光 四 象限 搭配 选择 不同 质地 悦薇 水乳 更 炸裂 礼盒 里 好多 惊喜 正装 水乳外 中样 水乳 ml 资生堂 针管 一大 盒 面膜 纸 一个 礼盒 拥有 可爱 实用 产品 想 没有 女生 逃 真诚 可爱 礼盒 妥妥 送 会 夸 很会选 礼物 程度 礼盒 送礼 悦薇 水乳', 'CRD 克徕帝 星耀 花园 长宁 来福士 心 心念 终于 见到 张若昀 帅麻 现场 定制 专属 明信片 兑换 礼物 姐妹 赶紧 带上 另一半', 'PIPIs VLOG 快乐 万宁 行做 岛民 真的 太 快乐 已经 期待 一次 海岛 行 vlog 带 微博去 旅行', '恭喜 Zz 罗一舟 成为 祖玛珑 JoMaloneLondon 祖玛珑 新 品牌 大使 最近 追舟 舟 新剧 完全 迷住 纯粹 少年 感似 春风 拂面 山风 过川 眼前 少年 祖玛珑 蓝 风铃 香水 氛围 完美 契合 温柔 花香 融合 甜美 柿子 每次 感觉 一股 柔和 清新 风拂过 闭上 双眼 闻到 夏天 味道 祖玛珑 老 粉丝 家里 蓝 风铃 味道 蓝 风铃 比较 温柔 花香调 香型 混 搭 不会 冲突 会 解锁 一种 独特 香型 知道 入手 一款 盲选 香气 舟舟 冷冽 清新 不会 一种 距离感 一定 夸夸 外观设计 有种 独 属于 祖玛珑 质感 简直 爱 爱 罗一舟', '保持 年轻 秘密 味道 状态 最近 淘到 不少 噔 西 好物 推荐 好物 分享', '咯 咯 彩虹 屁 四月 PIPI 粉丝 互动 榜 粉群 眼熟 榜来 咯 宝子 萌快 康康 有没有 名字 恭喜 以下 六位 幸运 粉丝 名气 不大 权 宝贝 宝贝 真的 喜欢 吃火锅 诺维茨基 小迷妹 一口 十个 煎饼果子 甜 崽 小熊 上榜 滴 宝速来 私信 大皮 领取 兑奖 截止 时间 即日起 月 日 下午 点 橡皮 看好 时间 速来 领奖 赢 牛奶 没有 上榜 滴宝 不要 气馁 五月 继续 大皮 互动 说不定 下次 幸运 鹅粉 群 互动 方式 加入 大皮 粉丝 群 大皮 专属 页 大皮 分享 日常生活 内容 ps 页 发布 生活 日常 近期 满意 自拍 近期 拍到 最美 日落 近期 美好 生活 片段 里 大皮 分享 大皮 会 不定期 互动 进群 链接 http t cn AiEnnuok 陈大 皮儿', '五一 假期 掉落 初夏 碎片 清清 库存 五一 留守 上海 美汁源 飞猪 山 湖海 自在营 玩 一回 朋友 逛街 看到 美汁源 快 闪 一眼 吸引住 完美 实现 心中 清爽 夏天 抢先 尝到了 美汁源 香柠 口味 上海站 首发 入口 微酸 巧妙 拿捏 住 天然 果味 平衡 酸酸甜甜 ju 清爽 夏天 真的 离不开 喝 限定 葡萄 气泡 特调 清甜 果汁 绵密 气泡 糖 脂 实现 肥宅 快乐 五一 众 众众 没有 景点 凑热闹 喜欢 逛街 闲逛 慢悠悠 走 累 找 一个 阳光 地方 喝咖啡 聊天 身在 地方 心在 桃源 一种 放假 放松 身心 放空 大脑 特调 饮品 外 扭蛋 抽奖 机 玩快 闪 活动 清新 马卡龙 色系 布置 hin 夏天 都市 野营 氛围 超 适合 拍照 打卡 领 限定 露营 周边 福利 简直 不要 太 丰富 溜达 一天 真的 不会 腻 月 月 美汁源 城 巡游 快 闪 开启 持续 关注 美汁源 饮料 了解 更 城市 快 闪 消息 美汁源 果汁 气泡 饮 随时随地 自然 自在 美汁 源山 湖海 自在营', '立夏 走出 格子 间 穿梭 自然 中 触碰 春天 获得 片刻 带上 野餐 布 席地而坐 享受 舒服 太阳 柔软 微风 草地 花朵 春天 少不了 拍照 特地 打扮 想 一边 拥抱 阳光 丝滑 出片 必须 肌肤 维持 清透 细腻 最佳 状态 兰蔻 小黑 瓶 精华 帮 keep 住 水灵 脸蛋 修护 好物 一瓶 满足 强韧 修护 抗氧 保湿 需求 专利 高浓度 二裂 酵母 及益 生元 精粹 微 生态 屏障 再到 肌底 注入 活力 层层 深入 修护 益生元 精粹 平衡 肌肤 微 生态 二裂 酵母 跟上来 修护 受损 屏障 专研 高 渗透 配方 直达 肌底 强韧 维稳 天 get 稳定 强韧 肌 VC 腺苷 双重 抗氧而 深润 玻 尿酸 长效 保湿 hold 住 户外 拍摄 脸蛋 总能 保持 水润 通透 光泽感 有兰蔻 小黑 瓶 颜值 绝不会 掉队 蛋清 质地 脸 软软 舒服 会 加一点 精华 粉底液 里 轻薄 妆 一整天 下来 几乎 不怎么 补妆 修护 贼 全能 精华 舒服 晒 一整天 太阳 高清 怼 脸 拍照 生活 快乐 兰蔻 第二代 小黑 瓶 GET 换季 必备 精华', '欧莱雅 新生 前来 报道 历史课 化学课 实验课 下车 特别 实验课 超级 有意思 亲眼 见证 VC 还原 过程 添加 原型 VC 真的 新 突破 期待 真 C 瓶 欧莱雅 真 c 瓶 上海 TX 淮海', '夏日 一起 漫游 罗意 威伊 维萨 岛 香水 世界', 'PIPIs SHARING 月 购物 分享 久违 购物 分享 月 买 几件 超 中意 赶紧 分享 下饭 香香的 好物 分享 好物 推荐', 'PIPIs SHARING 敏感 肌 自救 指南 敏感 肌 基础 今天 手把手 教 拯救 敏感 一个 健康 皮肤 好物 推荐 好物 分享', '兰蔻持 妆 不愧 心头 爱 岛上 水上 项目 一整天 下来 在线 妥妥 玩儿 水 必备 防晒 带 修丽 凡士林 凡士林 防晒 真的 惊艳 味道 超级 好闻 清爽 蜜瓜 味 好物 推荐 带 微博去 海边', '年 中国 子宫颈癌 综合 防控 指南 提出 三级预防 策略 接种 HPV 疫苗 第一步 关注 健康 重要 现在 省份 岁 姐妹 价 价 价 三种 价型 HPV 疫苗 选择 等待时间 更 短 预约 更 便捷 需 更好 保护 快 转给 关心 HPV 锦鲤 日', 'PIPIs SHARING 旅行 妆容 分享 三亚 搞 一个 超爱 妆容 妆容 其实 越 简单 更能 凸显 质感 赶紧 我学 起来 有手 会 那种 好物 推荐 今日 妆容', '编辑部 实习生 前来 报道 OOTD 今天 穿', '有幸 受邀 参加 TOMFORDBEAUTY 亚太区 旅游 零售 三亚 倾 呈 奢光 粉底 大师 课 大师 课 深刻 了解 了解 面部轮廓 展现出 专属 独特 风格 以前 一直 觉得 脸要 白要 平整 好看 其实 立体 度 凸显 原生 轮廓 脸 真实 自然 魅力 大师 课 活动 现场 走过 奢光 空间 奢光 隧道 动态 墙 感受 品牌 独特 美妆 哲学 动人 处 大师 课时 看着 模特 老师 化妆 刷 没有 底妆 掩盖 掉 特色 奢华 光泽感 轮廓 特色 更加 立体 明显 一下子 顿悟 原来 化妆 使人 更加 自信 这句 话 指 化妆 无限 接近 一种 标准 美人 展现 独一无二 美 成为 闪耀 存在 概念 挚爱 TOMFORD 奢光 粉底液 里 展现 淋漓尽致 蕴含 D 折光 微粒 感知 调节 光线 变化 只上 底妆 脸部 不同 光线 更 自然 立体 光泽 照镜子 有种 天生 贵妇 皮肤 感觉 添加 三重 养肤 油 透明质 酸 质地 细腻 滋润 长效 持妆 自带 SPF PA 高倍 防晒 聚光 不惧 光 真的 太 贴心 旅行 带 够 试 TOM FORD 奢光 气垫 立体 双色 塑颜膏 惊喜 满满 TOM FORD 奢光 气垫 带 出门 补妆超 方便 奢光 粉底液 同款 养肤 成分 高倍 防晒 系数 脸 服帖 走 拥有 立体 奢光妆感 TOM FORD 立体 双色 塑颜膏 很润 膏状 质地 一抹 上色 推开 完美 底妆 融合 快速 完成 立体 高级 妆容 立体 世界 立体 面部轮廓 探索 适合 奢光妆容 更 自信 特别 喜欢 探索 世界 不同 样貌 内在 越来越 丰富 TOM FORD 妆备 随时随地 尽显 立体 奢光 从容 出发 探索 立体 世界 最近 打算 三亚 姐妹 各大 免税店 TOM FORD BEAUTY 品牌 专柜 一起 体验 奢光 美学 旅程 中尽释 自信 奢光', '浅浅 做 一个 公主 AERIN 新品 玫瑰 真的 太 戳 女孩子 不能 拒绝 味道 做 一个 手工 感觉 点点 美感 裙子 ZZFY STUDIO 好物 推荐 OOTD', '大声 告诉 头发 柔顺 健康 光泽 秘密 无限 回购 欧莱雅 pro 今天 口红 nars 细管 哑光 no angel 最近 真的 爱 淡妆 浓妆 日常 妆素 颜都会 问 色 号 好物 推荐 美妆', '浅浅 个展 春天 新 花 怒放 OOTD', '最近 快乐 大自然 雅顿 白茶 扩香 礼盒 太 戳 洗面奶 新 肌霜 敏感 肌 放心 质地 轻薄 非常 吸收 用过 白茶 知道 味道 治愈 茶香 真的 适合 春夏 清清爽爽 完 感觉 做 一个 芳香 spa 干发 喷雾 海岛 必备 出汗 出油 喷 喷 刚 洗过 时刻 保持 美丽 发质 选择 三亚 艾迪 逊 草坪 太 拍 拍 好看 SUPERR CHRIS BY CHRISTOPHER BU 好物 推荐 春天 新 花 怒放', '姐妹 早上好 科罗娜 日落 真的 太出 片 人太多 非常 考验 拍照 机位 COTTIA 春天 新 花 怒放 OOTD 万宁 神州 半岛', 'PIPIs SHARING 颈部 护理 攻略 脸 以外 女孩子 在意 无外乎 秃头 颈 纹 发际 线 之前 分享 今天 起来 看看 打理 颈部 颈霜 精油 手法 拥有 一个 完美 仙女 脖子 好物 推荐 春天 新 花 怒放', '美好 四月 nanajacqueline pedder red 春天 新 花 怒放 OOTD', '岛民 n 天 分享 旅行 好物 之前 宝贝 问 海边 妆容 穿 搭 觉得 妆容 这部分 重底 妆 轻 彩妆 底妆 选择 一定 清透 持妆 TF 奢光 粉底液 真的 爱 打造 妆 感 D 立体感 非常 强 关键 超长 待机 真的 太 适合 海岛 眼影 选择 号 唇 釉 号 整个 妆面 色调 落日 绝配 三亚 重要 任务 参加 奢光 粉底 大师 课 整个 期待 住 入住 艾迪 逊 酒店 美住 关键 随便 拍拍 出片 姐妹 美照 春天 新 花 怒放 好物 推荐 三亚 三亚 艾迪 逊 酒店', 'Laura Mercier 玫瑰色 散粉 简直 少女 本女 ABH 腮红 棒 高光 棒 颜色 绝 绝子 一头 搭配 刷子 超级 方便 最爱 迷你 综合 盘 配色 高级 日常 百 搭 出门 旅行 必备 太爱 好物 分享 今日 妆容', 'PIPIs SHARING 短途旅行 化妆包 分享 压箱底 宝贝 短途旅行 一定 从简 化妆包 虽小 浓缩 精华 好物 推荐 春天 新 花 怒放', '知道 是不是 年龄 原因 越来越 喜欢 松弛 感觉 松弛 味道 好物 推荐 春天 新 花 怒放', 'PIPIs SHARING 摆脱 毛孔 焦虑 毛孔 烦恼 期 视频 刷新 毛孔 认知 打开 毛孔 新世界 重新认识 毛孔 全程 干货 我码 住 抗老 更 温和', '最近 一个 快乐 岛民 顺便 分享 海边 包包 里 带 温和 早 c 晚 a 绝对 不能 落下 素颜 超能 前段时间 新入 ghd 铂金 直板 夹 搭配 卡诗 护发 精油 真的 绝绝子 春天 新 花 怒放 好物 推荐 万宁 神州 半岛', '奥 伦纳 素 新品 分享 会 真的 太赞 修护 霜 质地 科技 真的 重新认识 抗老 类 面霜 愈见 年轻 肌 狠狠 期待 住 连衣裙 COTTIA 耳环 ONCHI C 春天 新 花 怒放 OOTD', 'PIPIs VLOG 一周 vlog 最近 天天 穿梭 品牌 新品 发布会 时隔 三年 终于 好好 享受 春天 美好 今天 看看 一周 品牌 分享 会 真的 很多 产品 不错 浅浅 期待 一下 春天 新 花 怒放 vlog', 'Vans 一直 推动 中国 滑板 文化 发展 扶持 本土 滑板 社群 一直 希望 邀请 更 滑板 爱好者 加入 Vans 大家庭 今天 感受 一下 Vans 滑板 学校 教练 带 练 非常 专业 超级 奶思 这种 基础 摔倒 全新 职业 滑板鞋 Zahba 发售 期待 一下 春天 新 花 怒放 OOTD 设限 无限', '人生 中 拍 第一个 短 视频 分享 欧缇丽 今天 有幸 欧缇丽 创始人 合照 马蒂德 女士 太 nice 幸福 套装 COTTIA 鞋子 LABER THREE 春天 新 花 怒放 真的', '兰花 系列 真的 太 治愈 太 美好', 'PIPIs SHARING 抗老 产品 分享 坦白 局 岁 首选 抗老 产品 水 精华 眼霜 面霜 经典 四件套 生活 做 减法 好物 推荐', '吃 防晒 季节 心目 中 防晒 白月光 种种 草 生活 做 减法 好物 推荐', 'MUF 新品 仿真 肌 丝绒 粉饼 太绝 冲冲', '最近 展 美好 美 宝格丽 香氛 遇见 一个', '春日 寻开心 跟着 欧阳 娜娜 学习 早春 穿 搭 说不定 听到 现场 live 小甜歌 月 新欢 月 日 日 打开 手 淘 搜 天猫 官方 直播间 看起来 焕新周', '紫里 紫气 月 新欢 费力 时髦 穿 搭']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "\n",
    "# 加载停用词\n",
    "def load_stopwords():\n",
    "    url = \"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\"\n",
    "    return pd.read_csv(url, header=None, sep=\"\\t\", quoting=3, names=['stopword'], encoding='utf-8').stopword.tolist()\n",
    "\n",
    "# 获取停用词列表\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# 检查并删除df[0]中的重复数据\n",
    "df = df.drop_duplicates(subset=[\"0\"])\n",
    "\n",
    "# 假设的博主名字列表，如果你没有这个列表，可以将其定义为空列表\n",
    "combined_blogger_names = []\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 使用正则表达式去除“XX的微博视频”\n",
    "    text = re.sub(r'[^#]*的微博视频', '', text)\n",
    "    \n",
    "    # 去除特定无用词\n",
    "    useless_terms = [\"显示地图\", \"原图\", \"[超话]\", \"超话\"]\n",
    "    for term in useless_terms:\n",
    "        text = text.replace(term, '')\n",
    "    \n",
    "    # 去除自我指代\n",
    "    for name in combined_blogger_names:\n",
    "        text = text.replace(name, '')\n",
    "    \n",
    "    # 使用jieba进行分词\n",
    "    words = jieba.cut(text, cut_all=False)\n",
    "    \n",
    "    # 去除标点和特殊字符\n",
    "    words = [re.sub(r'[^a-zA-Z\\u4e00-\\u9fff]', '', word) for word in words if len(word) > 0]\n",
    "    \n",
    "    # 去除“组图”、“共”、“张”\n",
    "    words = [word for word in words if word not in [\"组图\", \"共\", \"张\"]]\n",
    "    \n",
    "    # 去除停用词\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    # 将连续的多个空格替换为一个空格\n",
    "    result = ' '.join(words)\n",
    "    result = re.sub(r'\\s+', ' ', result).strip()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 应用预处理函数\n",
    "texts_cut = [preprocess_text(text) for text in df[\"0\"]]\n",
    "print(texts_cut[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41922da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  warnings.warn(\n",
      "INFO:lda:n_documents: 16811\n",
      "INFO:lda:vocab_size: 1000\n",
      "INFO:lda:n_words: 247195\n",
      "INFO:lda:n_topics: 15\n",
      "INFO:lda:n_iter: 1000\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -2566683\n",
      "INFO:lda:<10> log likelihood: -1819291\n",
      "INFO:lda:<20> log likelihood: -1726206\n",
      "INFO:lda:<30> log likelihood: -1695721\n",
      "INFO:lda:<40> log likelihood: -1679839\n",
      "INFO:lda:<50> log likelihood: -1669183\n",
      "INFO:lda:<60> log likelihood: -1660953\n",
      "INFO:lda:<70> log likelihood: -1657143\n",
      "INFO:lda:<80> log likelihood: -1654393\n",
      "INFO:lda:<90> log likelihood: -1652091\n",
      "INFO:lda:<100> log likelihood: -1650350\n",
      "INFO:lda:<110> log likelihood: -1650194\n",
      "INFO:lda:<120> log likelihood: -1648225\n",
      "INFO:lda:<130> log likelihood: -1647244\n",
      "INFO:lda:<140> log likelihood: -1646570\n",
      "INFO:lda:<150> log likelihood: -1646055\n",
      "INFO:lda:<160> log likelihood: -1644692\n",
      "INFO:lda:<170> log likelihood: -1644088\n",
      "INFO:lda:<180> log likelihood: -1643844\n",
      "INFO:lda:<190> log likelihood: -1643549\n",
      "INFO:lda:<200> log likelihood: -1642614\n",
      "INFO:lda:<210> log likelihood: -1643464\n",
      "INFO:lda:<220> log likelihood: -1642830\n",
      "INFO:lda:<230> log likelihood: -1642376\n",
      "INFO:lda:<240> log likelihood: -1641515\n",
      "INFO:lda:<250> log likelihood: -1642439\n",
      "INFO:lda:<260> log likelihood: -1642584\n",
      "INFO:lda:<270> log likelihood: -1642629\n",
      "INFO:lda:<280> log likelihood: -1641558\n",
      "INFO:lda:<290> log likelihood: -1642354\n",
      "INFO:lda:<300> log likelihood: -1641925\n",
      "INFO:lda:<310> log likelihood: -1642432\n",
      "INFO:lda:<320> log likelihood: -1642251\n",
      "INFO:lda:<330> log likelihood: -1641306\n",
      "INFO:lda:<340> log likelihood: -1641231\n",
      "INFO:lda:<350> log likelihood: -1641287\n",
      "INFO:lda:<360> log likelihood: -1641486\n",
      "INFO:lda:<370> log likelihood: -1641273\n",
      "INFO:lda:<380> log likelihood: -1641535\n",
      "INFO:lda:<390> log likelihood: -1642333\n",
      "INFO:lda:<400> log likelihood: -1642476\n",
      "INFO:lda:<410> log likelihood: -1641224\n",
      "INFO:lda:<420> log likelihood: -1641549\n",
      "INFO:lda:<430> log likelihood: -1641036\n",
      "INFO:lda:<440> log likelihood: -1641085\n",
      "INFO:lda:<450> log likelihood: -1641075\n",
      "INFO:lda:<460> log likelihood: -1641257\n",
      "INFO:lda:<470> log likelihood: -1641749\n",
      "INFO:lda:<480> log likelihood: -1640965\n",
      "INFO:lda:<490> log likelihood: -1641023\n",
      "INFO:lda:<500> log likelihood: -1640512\n",
      "INFO:lda:<510> log likelihood: -1641024\n",
      "INFO:lda:<520> log likelihood: -1640738\n",
      "INFO:lda:<530> log likelihood: -1640250\n",
      "INFO:lda:<540> log likelihood: -1640938\n",
      "INFO:lda:<550> log likelihood: -1641411\n",
      "INFO:lda:<560> log likelihood: -1640861\n",
      "INFO:lda:<570> log likelihood: -1640598\n",
      "INFO:lda:<580> log likelihood: -1639565\n",
      "INFO:lda:<590> log likelihood: -1639623\n",
      "INFO:lda:<600> log likelihood: -1639301\n",
      "INFO:lda:<610> log likelihood: -1639327\n",
      "INFO:lda:<620> log likelihood: -1639272\n",
      "INFO:lda:<630> log likelihood: -1639776\n",
      "INFO:lda:<640> log likelihood: -1640284\n",
      "INFO:lda:<650> log likelihood: -1640550\n",
      "INFO:lda:<660> log likelihood: -1639885\n",
      "INFO:lda:<670> log likelihood: -1641202\n",
      "INFO:lda:<680> log likelihood: -1640432\n",
      "INFO:lda:<690> log likelihood: -1640378\n",
      "INFO:lda:<700> log likelihood: -1640161\n",
      "INFO:lda:<710> log likelihood: -1639250\n",
      "INFO:lda:<720> log likelihood: -1639979\n",
      "INFO:lda:<730> log likelihood: -1640271\n",
      "INFO:lda:<740> log likelihood: -1639852\n",
      "INFO:lda:<750> log likelihood: -1640516\n",
      "INFO:lda:<760> log likelihood: -1639465\n",
      "INFO:lda:<770> log likelihood: -1639925\n",
      "INFO:lda:<780> log likelihood: -1639124\n",
      "INFO:lda:<790> log likelihood: -1639906\n",
      "INFO:lda:<800> log likelihood: -1640081\n",
      "INFO:lda:<810> log likelihood: -1640192\n",
      "INFO:lda:<820> log likelihood: -1639998\n",
      "INFO:lda:<830> log likelihood: -1640364\n",
      "INFO:lda:<840> log likelihood: -1640576\n",
      "INFO:lda:<850> log likelihood: -1639411\n",
      "INFO:lda:<860> log likelihood: -1639872\n",
      "INFO:lda:<870> log likelihood: -1640278\n",
      "INFO:lda:<880> log likelihood: -1639972\n",
      "INFO:lda:<890> log likelihood: -1640226\n",
      "INFO:lda:<900> log likelihood: -1640707\n",
      "INFO:lda:<910> log likelihood: -1639915\n",
      "INFO:lda:<920> log likelihood: -1640345\n",
      "INFO:lda:<930> log likelihood: -1639567\n",
      "INFO:lda:<940> log likelihood: -1639568\n",
      "INFO:lda:<950> log likelihood: -1640009\n",
      "INFO:lda:<960> log likelihood: -1639523\n",
      "INFO:lda:<970> log likelihood: -1639421\n",
      "INFO:lda:<980> log likelihood: -1641337\n",
      "INFO:lda:<990> log likelihood: -1640583\n",
      "INFO:lda:<999> log likelihood: -1639898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model done\n",
      "Topic #0:\n",
      "香水 味道 真的 玫瑰 没有 喜欢 一个 感觉 起来 一点\n",
      "Topic #1:\n",
      "头发 粉底液 底妆 真的 护发 头皮 定妆 粉底 持妆 喷雾\n",
      "Topic #2:\n",
      "互动 微博 粉丝 评论 宝贝 美妆 日常 多多 一起 上榜\n",
      "Topic #3:\n",
      "抽奖 详情 潼话 王国 开箱 双十 豆豆 babe 分享 大会\n",
      "Topic #4:\n",
      "皮肤 护肤 真的 肌肤 精华 敏感 修护 效果 面膜 换季\n",
      "Topic #5:\n",
      "一个 真的 生活 没有 很多 希望 觉得 一直 看到 工作\n",
      "Topic #6:\n",
      "精华 抗老 肌肤 护肤 眼霜 面霜 修护 皮肤 雅诗兰黛 熬夜\n",
      "Topic #7:\n",
      "京东 链接 网页 天猫 一起 姐妹 手机 福利 活动 ml\n",
      "Topic #8:\n",
      "妆容 口红 今日 美妆 真的 眼影 腮红 氛围 眼妆 搭配\n",
      "Topic #9:\n",
      "真的 旅行 vlog 一起 体验 超级 一个 拍照 现场 长沙\n",
      "Topic #10:\n",
      "礼盒 礼物 快乐 品牌 七夕 收到 情人节 心动 限定 感谢\n",
      "Topic #11:\n",
      "分享 好物 护肤 美妆 推荐 彩妆 种草 最近 爱用物 身体\n",
      "Topic #12:\n",
      "plog 生活 日常 日记 ootd 快乐 碎片 夏日 今天 vlog\n",
      "Topic #13:\n",
      "新年 计划 完美 攻略 春天 开春 好颜 look 春夏 出游\n",
      "Topic #14:\n",
      "防晒 视频 变美 化妆 美妆 护肤 开学 今天 姐妹 全文\n",
      "\n",
      "Model Perplexity:  760.5370119962331\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# 3. 文本向量化\n",
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, stop_words=stopwords, max_df=0.5, min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(texts_cut)\n",
    "\n",
    "# 4. LDA模型训练\n",
    "n_topics = 15\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=1000, random_state=2)\n",
    "model.fit(tf)\n",
    "print('Model done')\n",
    "\n",
    "# 5. 打印每个主题的关键词\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names_out()\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# 6. 为每个文档分配主题\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = [topics.argmax() for topics in doc_topic]\n",
    "\n",
    "# 7. 计算困惑度\n",
    "log_likelihood = model.loglikelihood()\n",
    "n_total_words = tf.sum()\n",
    "perplexity = np.exp(-log_likelihood / n_total_words)\n",
    "print(\"Model Perplexity: \", perplexity)\n",
    "\n",
    "# # 8. 保存结果\n",
    "# df.to_excel('/Users/laihuiqian/Documents/weibo_0925/total2-0925_with_topics.xlsx', index=False)\n",
    "# print('Results saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1383785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  warnings.warn(\n",
      "INFO:lda:n_documents: 16811\n",
      "INFO:lda:vocab_size: 1000\n",
      "INFO:lda:n_words: 247195\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 1000\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -2411590\n",
      "INFO:lda:<10> log likelihood: -1792468\n",
      "INFO:lda:<20> log likelihood: -1706446\n",
      "INFO:lda:<30> log likelihood: -1680831\n",
      "INFO:lda:<40> log likelihood: -1668318\n",
      "INFO:lda:<50> log likelihood: -1662249\n",
      "INFO:lda:<60> log likelihood: -1657309\n",
      "INFO:lda:<70> log likelihood: -1653937\n",
      "INFO:lda:<80> log likelihood: -1652583\n",
      "INFO:lda:<90> log likelihood: -1650840\n",
      "INFO:lda:<100> log likelihood: -1649289\n",
      "INFO:lda:<110> log likelihood: -1648556\n",
      "INFO:lda:<120> log likelihood: -1646738\n",
      "INFO:lda:<130> log likelihood: -1647399\n",
      "INFO:lda:<140> log likelihood: -1646834\n",
      "INFO:lda:<150> log likelihood: -1646142\n",
      "INFO:lda:<160> log likelihood: -1645454\n",
      "INFO:lda:<170> log likelihood: -1644615\n",
      "INFO:lda:<180> log likelihood: -1643745\n",
      "INFO:lda:<190> log likelihood: -1644535\n",
      "INFO:lda:<200> log likelihood: -1642533\n",
      "INFO:lda:<210> log likelihood: -1642827\n",
      "INFO:lda:<220> log likelihood: -1641649\n",
      "INFO:lda:<230> log likelihood: -1641573\n",
      "INFO:lda:<240> log likelihood: -1639751\n",
      "INFO:lda:<250> log likelihood: -1640500\n",
      "INFO:lda:<260> log likelihood: -1639875\n",
      "INFO:lda:<270> log likelihood: -1638737\n",
      "INFO:lda:<280> log likelihood: -1637879\n",
      "INFO:lda:<290> log likelihood: -1638364\n",
      "INFO:lda:<300> log likelihood: -1638211\n",
      "INFO:lda:<310> log likelihood: -1636920\n",
      "INFO:lda:<320> log likelihood: -1637874\n",
      "INFO:lda:<330> log likelihood: -1636869\n",
      "INFO:lda:<340> log likelihood: -1637868\n",
      "INFO:lda:<350> log likelihood: -1637609\n",
      "INFO:lda:<360> log likelihood: -1637056\n",
      "INFO:lda:<370> log likelihood: -1637377\n",
      "INFO:lda:<380> log likelihood: -1637241\n",
      "INFO:lda:<390> log likelihood: -1636208\n",
      "INFO:lda:<400> log likelihood: -1636924\n",
      "INFO:lda:<410> log likelihood: -1636621\n",
      "INFO:lda:<420> log likelihood: -1635680\n",
      "INFO:lda:<430> log likelihood: -1636779\n",
      "INFO:lda:<440> log likelihood: -1636585\n",
      "INFO:lda:<450> log likelihood: -1636503\n",
      "INFO:lda:<460> log likelihood: -1636042\n",
      "INFO:lda:<470> log likelihood: -1636251\n",
      "INFO:lda:<480> log likelihood: -1635480\n",
      "INFO:lda:<490> log likelihood: -1634662\n",
      "INFO:lda:<500> log likelihood: -1634911\n",
      "INFO:lda:<510> log likelihood: -1634927\n",
      "INFO:lda:<520> log likelihood: -1633928\n",
      "INFO:lda:<530> log likelihood: -1634603\n",
      "INFO:lda:<540> log likelihood: -1634673\n",
      "INFO:lda:<550> log likelihood: -1634490\n",
      "INFO:lda:<560> log likelihood: -1634532\n",
      "INFO:lda:<570> log likelihood: -1634562\n",
      "INFO:lda:<580> log likelihood: -1634304\n",
      "INFO:lda:<590> log likelihood: -1634028\n",
      "INFO:lda:<600> log likelihood: -1635392\n",
      "INFO:lda:<610> log likelihood: -1634524\n",
      "INFO:lda:<620> log likelihood: -1635035\n",
      "INFO:lda:<630> log likelihood: -1634560\n",
      "INFO:lda:<640> log likelihood: -1635516\n",
      "INFO:lda:<650> log likelihood: -1634421\n",
      "INFO:lda:<660> log likelihood: -1634296\n",
      "INFO:lda:<670> log likelihood: -1634314\n",
      "INFO:lda:<680> log likelihood: -1634673\n",
      "INFO:lda:<690> log likelihood: -1633743\n",
      "INFO:lda:<700> log likelihood: -1634764\n",
      "INFO:lda:<710> log likelihood: -1633900\n",
      "INFO:lda:<720> log likelihood: -1634125\n",
      "INFO:lda:<730> log likelihood: -1634179\n",
      "INFO:lda:<740> log likelihood: -1634476\n",
      "INFO:lda:<750> log likelihood: -1634551\n",
      "INFO:lda:<760> log likelihood: -1634562\n",
      "INFO:lda:<770> log likelihood: -1633170\n",
      "INFO:lda:<780> log likelihood: -1633697\n",
      "INFO:lda:<790> log likelihood: -1634138\n",
      "INFO:lda:<800> log likelihood: -1633402\n",
      "INFO:lda:<810> log likelihood: -1633854\n",
      "INFO:lda:<820> log likelihood: -1633965\n",
      "INFO:lda:<830> log likelihood: -1633457\n",
      "INFO:lda:<840> log likelihood: -1633488\n",
      "INFO:lda:<850> log likelihood: -1633778\n",
      "INFO:lda:<860> log likelihood: -1632903\n",
      "INFO:lda:<870> log likelihood: -1634499\n",
      "INFO:lda:<880> log likelihood: -1633454\n",
      "INFO:lda:<890> log likelihood: -1633070\n",
      "INFO:lda:<900> log likelihood: -1633598\n",
      "INFO:lda:<910> log likelihood: -1633449\n",
      "INFO:lda:<920> log likelihood: -1633440\n",
      "INFO:lda:<930> log likelihood: -1633765\n",
      "INFO:lda:<940> log likelihood: -1633715\n",
      "INFO:lda:<950> log likelihood: -1633612\n",
      "INFO:lda:<960> log likelihood: -1634167\n",
      "INFO:lda:<970> log likelihood: -1634346\n",
      "INFO:lda:<980> log likelihood: -1634016\n",
      "INFO:lda:<990> log likelihood: -1633476\n",
      "INFO:lda:<999> log likelihood: -1633614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model done\n",
      "Topic #0:\n",
      "真的 粉底液 防晒 底妆 感觉 不会 夏天 非常 适合 喜欢\n",
      "Topic #1:\n",
      "皮肤 精华 肌肤 护肤 修护 抗老 真的 成分 面霜 状态\n",
      "Topic #2:\n",
      "分享 好物 护肤 美妆 种草 最近 推荐 彩妆 双十 购物\n",
      "Topic #3:\n",
      "妆容 口红 抽奖 look 详情 春天 今日 氛围 今天 ootd\n",
      "Topic #4:\n",
      "生活 plog vlog 日常 快乐 日记 今天 碎片 ootd 真的\n",
      "Topic #5:\n",
      "京东 链接 网页 一起 天猫 活动 姐妹 新年 手机 体验\n",
      "Topic #6:\n",
      "香水 一个 没有 真的 喜欢 玫瑰 味道 觉得 知道 全文\n",
      "Topic #7:\n",
      "新年 妆容 美妆 计划 完美 攻略 今日 开春 化妆 好颜\n",
      "Topic #8:\n",
      "礼盒 礼物 头发 开箱 快乐 七夕 情人节 品牌 收到 分享\n",
      "Topic #9:\n",
      "互动 王国 潼话 抽奖 粉丝 详情 微博 宝贝 评论 日常\n",
      "\n",
      "Model Perplexity:  741.4488491783408\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# 3. 文本向量化\n",
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, stop_words=stopwords, max_df=0.5, min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(texts_cut)\n",
    "\n",
    "# 4. LDA模型训练\n",
    "n_topics = 10\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=1000, random_state=2)\n",
    "model.fit(tf)\n",
    "print('Model done')\n",
    "\n",
    "# 5. 打印每个主题的关键词\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names_out()\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# 6. 为每个文档分配主题\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = [topics.argmax() for topics in doc_topic]\n",
    "\n",
    "# 7. 计算困惑度\n",
    "log_likelihood = model.loglikelihood()\n",
    "n_total_words = tf.sum()\n",
    "perplexity = np.exp(-log_likelihood / n_total_words)\n",
    "print(\"Model Perplexity: \", perplexity)\n",
    "\n",
    "# # 8. 保存结果\n",
    "# df.to_excel('/Users/laihuiqian/Documents/weibo_0925/total2-0925_with_topics.xlsx', index=False)\n",
    "# print('Results saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a19fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
