{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73778afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values saved to /Users/laihuiqian/weibo/all_data/all_data_2022/1-12-2022-new.xlsx.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-4a37cd5113b4>:17: UserWarning: Pandas requires version '1.4.3' or newer of 'xlsxwriter' (version '1.3.8' currently installed).\n",
      "  all_values_df.to_excel(output_excel_path, index=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "excel_file_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/1-12-2022.xlsx\"\n",
    "output_excel_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/1-12-2022-new.xlsx\"\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# 将所有的列堆叠起来形成一个单一的列\n",
    "all_values = df.stack().reset_index(drop=True)\n",
    "\n",
    "# 转换为DataFrame\n",
    "all_values_df = pd.DataFrame(all_values, columns=[\"Values\"])\n",
    "\n",
    "# 保存到新的Excel文件\n",
    "all_values_df.to_excel(output_excel_path, index=False)\n",
    "print(f\"All values saved to {output_excel_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ef76b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-816105838357>:17: UserWarning: Pandas requires version '1.4.3' or newer of 'xlsxwriter' (version '1.3.8' currently installed).\n",
      "  all_values_df.to_excel(output_excel_path, index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values saved to /Users/laihuiqian/weibo/all_data/all_data_2022/13-28-2022-new.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "excel_file_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/13-28-2022.xlsx\"\n",
    "output_excel_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/13-28-2022-new.xlsx\"\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# 将所有的列堆叠起来形成一个单一的列\n",
    "all_values = df.stack().reset_index(drop=True)\n",
    "\n",
    "# 转换为DataFrame\n",
    "all_values_df = pd.DataFrame(all_values, columns=[\"Values\"])\n",
    "\n",
    "# 保存到新的Excel文件\n",
    "all_values_df.to_excel(output_excel_path, index=False)\n",
    "print(f\"All values saved to {output_excel_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74d4ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values saved to /Users/laihuiqian/weibo/all_data/all_data_2022/29-36-2022-new.xlsx.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-7f7114a93ccb>:17: UserWarning: Pandas requires version '1.4.3' or newer of 'xlsxwriter' (version '1.3.8' currently installed).\n",
      "  all_values_df.to_excel(output_excel_path, index=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "excel_file_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/29-36-2022.xlsx\"\n",
    "output_excel_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/29-36-2022-new.xlsx\"\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# 将所有的列堆叠起来形成一个单一的列\n",
    "all_values = df.stack().reset_index(drop=True)\n",
    "\n",
    "# 转换为DataFrame\n",
    "all_values_df = pd.DataFrame(all_values, columns=[\"Values\"])\n",
    "\n",
    "# 保存到新的Excel文件\n",
    "all_values_df.to_excel(output_excel_path, index=False)\n",
    "print(f\"All values saved to {output_excel_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b338f745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values saved to /Users/laihuiqian/weibo/all_data/all_data_2022/37-48-2022-new.xlsx.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-3c3dce27206e>:17: UserWarning: Pandas requires version '1.4.3' or newer of 'xlsxwriter' (version '1.3.8' currently installed).\n",
      "  all_values_df.to_excel(output_excel_path, index=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "excel_file_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/37-48-2022.xlsx\"\n",
    "output_excel_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/37-48-2022-new.xlsx\"\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# 将所有的列堆叠起来形成一个单一的列\n",
    "all_values = df.stack().reset_index(drop=True)\n",
    "\n",
    "# 转换为DataFrame\n",
    "all_values_df = pd.DataFrame(all_values, columns=[\"Values\"])\n",
    "\n",
    "# 保存到新的Excel文件\n",
    "all_values_df.to_excel(output_excel_path, index=False)\n",
    "print(f\"All values saved to {output_excel_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15195a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values saved to /Users/laihuiqian/weibo/all_data/all_data_2022/49-56-2022-new.xlsx.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-2cbb75d1857f>:17: UserWarning: Pandas requires version '1.4.3' or newer of 'xlsxwriter' (version '1.3.8' currently installed).\n",
      "  all_values_df.to_excel(output_excel_path, index=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "excel_file_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/49-56-2022.xlsx\"\n",
    "output_excel_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/49-56-2022-new.xlsx\"\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# 将所有的列堆叠起来形成一个单一的列\n",
    "all_values = df.stack().reset_index(drop=True)\n",
    "\n",
    "# 转换为DataFrame\n",
    "all_values_df = pd.DataFrame(all_values, columns=[\"Values\"])\n",
    "\n",
    "# 保存到新的Excel文件\n",
    "all_values_df.to_excel(output_excel_path, index=False)\n",
    "print(f\"All values saved to {output_excel_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5afef577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values saved to /Users/laihuiqian/weibo/all_data/all_data_2022/57-68-2022-new.xlsx.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-2d342416af1f>:17: UserWarning: Pandas requires version '1.4.3' or newer of 'xlsxwriter' (version '1.3.8' currently installed).\n",
      "  all_values_df.to_excel(output_excel_path, index=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "excel_file_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/57-68-2022.xlsx\"\n",
    "output_excel_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/57-68-2022-new.xlsx\"\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# 将所有的列堆叠起来形成一个单一的列\n",
    "all_values = df.stack().reset_index(drop=True)\n",
    "\n",
    "# 转换为DataFrame\n",
    "all_values_df = pd.DataFrame(all_values, columns=[\"Values\"])\n",
    "\n",
    "# 保存到新的Excel文件\n",
    "all_values_df.to_excel(output_excel_path, index=False)\n",
    "print(f\"All values saved to {output_excel_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cb1b593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dc6f699fc565>:17: UserWarning: Pandas requires version '1.4.3' or newer of 'xlsxwriter' (version '1.3.8' currently installed).\n",
      "  all_values_df.to_excel(output_excel_path, index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values saved to /Users/laihuiqian/weibo/all_data/all_data_2022/69-84-2022-new.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "excel_file_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/69-84-2022.xlsx\"\n",
    "output_excel_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/69-84-2022-new.xlsx\"\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# 将所有的列堆叠起来形成一个单一的列\n",
    "all_values = df.stack().reset_index(drop=True)\n",
    "\n",
    "# 转换为DataFrame\n",
    "all_values_df = pd.DataFrame(all_values, columns=[\"Values\"])\n",
    "\n",
    "# 保存到新的Excel文件\n",
    "all_values_df.to_excel(output_excel_path, index=False)\n",
    "print(f\"All values saved to {output_excel_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d878873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-92217504b6b8>:17: UserWarning: Pandas requires version '1.4.3' or newer of 'xlsxwriter' (version '1.3.8' currently installed).\n",
      "  all_values_df.to_excel(output_excel_path, index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values saved to /Users/laihuiqian/weibo/all_data/all_data_2022/85-100-2022-new.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "excel_file_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/85-100-2022.xlsx\"\n",
    "output_excel_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/85-100-2022-new.xlsx\"\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# 将所有的列堆叠起来形成一个单一的列\n",
    "all_values = df.stack().reset_index(drop=True)\n",
    "\n",
    "# 转换为DataFrame\n",
    "all_values_df = pd.DataFrame(all_values, columns=[\"Values\"])\n",
    "\n",
    "# 保存到新的Excel文件\n",
    "all_values_df.to_excel(output_excel_path, index=False)\n",
    "print(f\"All values saved to {output_excel_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "799f802a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    【直播预告｜年度爱用盘点（下）】趁热打铁，下期来啦！🏃♀️清洁｜氨基酸洁面慕斯•口腔冲牙器滋...\n",
      "1    最好的礼物从来不是礼物本身而是意料之中的温暖和出其不意的惊喜💫#新年礼物已就位##上新了新年...\n",
      "2    2022年最后一天必须抽“红包”呀㊗️大家新年红红火火～平安健康～🧨留言里揪3⃣️0⃣️位哦...\n",
      "3    妆容变化！3年的变美逆袭秘诀[哇][哇]几个美妆技巧 就可以改变妆容！底妆——眼妆——中庭—...\n",
      "4    贵气美女的养成茂密的头发和好皮肤才是关键哦喜欢熬夜的妹妹们看看这里～#爱用##好物分享#  ...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 定义文件夹路径\n",
    "folder_path = \"/Users/laihuiqian/weibo/all_data/all_data_2022/2022_data\"\n",
    "\n",
    "# 初始化一个空的DataFrame，用于存储所有的数据\n",
    "all_data = []\n",
    "\n",
    "# 遍历文件夹中的所有Excel文件\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith('.xlsx'):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        # 读取Excel文件\n",
    "        df = pd.read_excel(file_path, engine='openpyxl')\n",
    "        \n",
    "        # 将所有的列堆叠起来形成一个单一的列\n",
    "        stacked_values = df.stack().reset_index(drop=True)\n",
    "        \n",
    "        # 追加到总的数据列表中\n",
    "        all_data.append(stacked_values)\n",
    "\n",
    "# 合并所有数据到一个DataFrame\n",
    "df_total = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# 重命名列名\n",
    "df_total.columns = ['Values']\n",
    "\n",
    "# 展示头部内容\n",
    "print(df_total.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b2fd973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17630,)\n"
     ]
    }
   ],
   "source": [
    "print(df_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0530b148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/tx/327gmf1d3vd2fzt8k0972vz00000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        【直播预告｜年度爱用盘点（下）】趁热打铁，下期来啦！🏃♀️清洁｜氨基酸洁面慕斯•口腔冲牙器滋...\n",
      "1        最好的礼物从来不是礼物本身而是意料之中的温暖和出其不意的惊喜💫#新年礼物已就位##上新了新年...\n",
      "2        2022年最后一天必须抽“红包”呀㊗️大家新年红红火火～平安健康～🧨留言里揪3⃣️0⃣️位哦...\n",
      "3        妆容变化！3年的变美逆袭秘诀[哇][哇]几个美妆技巧 就可以改变妆容！底妆——眼妆——中庭—...\n",
      "4        贵气美女的养成茂密的头发和好皮肤才是关键哦喜欢熬夜的妹妹们看看这里～#爱用##好物分享#  ...\n",
      "                               ...                        \n",
      "17625    昨天出去玩嘛，凌晨回到家我朋友跟我说的第一句话是：天啦这个底妆怎么会到现在还这么完整一点都没...\n",
      "17626                                     2022的第二天也美美出门啦！ \n",
      "17627                                          2022 远离烦恼🥳 \n",
      "17628    救命！！🆘\\n这个下睫毛也太好看了吧！！\\n也是胖给我推荐的，之前买分段式小恶魔睫毛那家16...\n",
      "17629                   2021年的遗憾不开心都过去啦！！\\n2022很崭新，要快乐噢！！ \n",
      "Length: 17630, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.565 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "def open_dict(Dict):\n",
    "    path = '%s.txt' % Dict\n",
    "    dictionary = open(path, 'r', encoding='utf-8')\n",
    "    dict = []\n",
    "    for word in dictionary:\n",
    "        word = word.strip('\\n')\n",
    "        dict.append(word)\n",
    "    return dict\n",
    "\n",
    "clearwords=open_dict('clearwords')\n",
    "\n",
    "def chinese_word_cut(mytext):\n",
    "    tempcut = jieba.cut(str(mytext))\n",
    "    result = \" \".join(set(tempcut) - set(clearwords))\n",
    "    return result if result.strip() else np.nan\n",
    "\n",
    "print(df_total)\n",
    "df_total[\"content_cutted\"] = df_total.apply(chinese_word_cut)\n",
    "n_features = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cb19ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                 【直播预告｜年度爱用盘点（下）】趁热打铁，下期来啦！🏃♀️清洁｜氨基酸洁面慕斯•口腔冲牙器滋...\n",
      "1                 最好的礼物从来不是礼物本身而是意料之中的温暖和出其不意的惊喜💫#新年礼物已就位##上新了新年...\n",
      "2                 2022年最后一天必须抽“红包”呀㊗️大家新年红红火火～平安健康～🧨留言里揪3⃣️0⃣️位哦...\n",
      "3                 妆容变化！3年的变美逆袭秘诀[哇][哇]几个美妆技巧 就可以改变妆容！底妆——眼妆——中庭—...\n",
      "4                 贵气美女的养成茂密的头发和好皮肤才是关键哦喜欢熬夜的妹妹们看看这里～#爱用##好物分享#  ...\n",
      "                                        ...                        \n",
      "17626                                              2022的第二天也美美出门啦！ \n",
      "17627                                                   2022 远离烦恼🥳 \n",
      "17628             救命！！🆘\\n这个下睫毛也太好看了吧！！\\n也是胖给我推荐的，之前买分段式小恶魔睫毛那家16...\n",
      "17629                            2021年的遗憾不开心都过去啦！！\\n2022很崭新，要快乐噢！！ \n",
      "content_cutted    0        下   直播 防护 滋补 （ • 精华 爱 抽奖 的 次 ！ 洁面 年度 ...\n",
      "Length: 17631, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "def open_dict(Dict):\n",
    "    path = '%s.txt' % Dict\n",
    "    dictionary = open(path, 'r', encoding='utf-8')\n",
    "    dict = []\n",
    "    for word in dictionary:\n",
    "        word = word.strip('\\n')\n",
    "        dict.append(word)\n",
    "    return dict\n",
    "\n",
    "clearwords=open_dict('clearwords')\n",
    "\n",
    "def chinese_word_cut(mytext):\n",
    "    tempcut = jieba.cut(str(mytext))\n",
    "    result = \" \".join(set(tempcut) - set(clearwords))\n",
    "    return result if result.strip() else np.nan\n",
    "\n",
    "print(df_total)\n",
    "df_total[\"content_cutted\"] = df_total.apply(chinese_word_cut)\n",
    "n_features = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b11336f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                 下   直播 防护 滋补 （ • 精华 爱 抽奖 的 次 ！ 洁面 年度 ， 修护 【 B5...\n",
      "1                   本身 ] 意料之中 的 上 了 新 出其不意 💫 是 最好 张 从来不 温暖 新年礼物 ...\n",
      "2                 红红火火 0 位 潼话   平安 2022 呀 ] 王国 ⃣ 必须 留言 抽奖 哦 抽 ！ ...\n",
      "3                 唇妆   — 快学 ] 可以 的 ！ 底妆 秘诀 就 遮 美妆 哇 起来 瑕 眼妆 技巧 改...\n",
      "4                   分享 ] 哦 的 看看 熬夜 是 贵气 们 美女 才 张 好 妹妹 这里 9 茂密 [ ...\n",
      "                                        ...                        \n",
      "17626                                      的   ！ 出门 第二天 2022 也 啦 美美\n",
      "17627                                                  远离 2022 烦恼 🥳\n",
      "17628             下 说 可可 有 这个   分享 感 \\n 式 1688 很 🆘 卷 拍照 给 papado...\n",
      "17629               快乐 2022 很 \\n 遗憾 的 ！ ， 啦 崭新 过去 不 都 要 噢 年 开心 2021\n",
      "content_cutted    下 红红火火 嘛   出去玩 😲 粉底 , 服帖 去 完 式 • 就是 17626 拍照 粉...\n",
      "Length: 17631, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_total[\"content_cutted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e1a8f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                 【直播预告｜年度爱用盘点（下）】趁热打铁，下期来啦！🏃♀️清洁｜氨基酸洁面慕斯•口腔冲牙器滋...\n",
      "1                 最好的礼物从来不是礼物本身而是意料之中的温暖和出其不意的惊喜💫#新年礼物已就位##上新了新年...\n",
      "2                 2022年最后一天必须抽“红包”呀㊗️大家新年红红火火～平安健康～🧨留言里揪3⃣️0⃣️位哦...\n",
      "3                 妆容变化！3年的变美逆袭秘诀[哇][哇]几个美妆技巧 就可以改变妆容！底妆——眼妆——中庭—...\n",
      "4                 贵气美女的养成茂密的头发和好皮肤才是关键哦喜欢熬夜的妹妹们看看这里～#爱用##好物分享#  ...\n",
      "                                        ...                        \n",
      "17626                                              2022的第二天也美美出门啦！ \n",
      "17627                                                   2022 远离烦恼🥳 \n",
      "17628             救命！！🆘\\n这个下睫毛也太好看了吧！！\\n也是胖给我推荐的，之前买分段式小恶魔睫毛那家16...\n",
      "17629                            2021年的遗憾不开心都过去啦！！\\n2022很崭新，要快乐噢！！ \n",
      "content_cutted    0                 下   直播 防护 滋补 （ • 精华 爱 抽奖 的 次...\n",
      "Length: 17631, dtype: object\n",
      "0                 下   直播 防护 滋补 （ • 精华 爱 抽奖 的 次 ！ 洁面 年度 ， 修护 【 B5...\n",
      "1                   本身 ] 意料之中 的 上 了 新 出其不意 💫 是 最好 张 从来不 温暖 新年礼物 ...\n",
      "2                 红红火火 0 位 潼话   平安 2022 呀 ] 王国 ⃣ 必须 留言 抽奖 哦 抽 ！ ...\n",
      "3                 唇妆   — 快学 ] 可以 的 ！ 底妆 秘诀 就 遮 美妆 哇 起来 瑕 眼妆 技巧 改...\n",
      "4                   分享 ] 哦 的 看看 熬夜 是 贵气 们 美女 才 张 好 妹妹 这里 9 茂密 [ ...\n",
      "                                        ...                        \n",
      "17626                                      的   ！ 出门 第二天 2022 也 啦 美美\n",
      "17627                                                  远离 2022 烦恼 🥳\n",
      "17628             下 说 可可 有 这个   分享 感 \\n 式 1688 很 🆘 卷 拍照 给 papado...\n",
      "17629               快乐 2022 很 \\n 遗憾 的 ！ ， 啦 崭新 过去 不 都 要 噢 年 开心 2021\n",
      "content_cutted    下 红红火火 嘛   出去玩 😲 粉底 , 服帖 去 完 式 • 就是 17626 拍照 粉...\n",
      "Length: 17631, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_total[\"content_cutted\"] = df_total[\"content_cutted\"].dropna()\n",
    "print(df_total)\n",
    "print(df_total[\"content_cutted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c73a9ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 17631\n",
      "INFO:lda:vocab_size: 5000\n",
      "INFO:lda:n_words: 475044\n",
      "INFO:lda:n_topics: 17\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -5699584\n",
      "INFO:lda:<10> log likelihood: -4248661\n",
      "INFO:lda:<20> log likelihood: -4018465\n",
      "INFO:lda:<30> log likelihood: -3939298\n",
      "INFO:lda:<40> log likelihood: -3900269\n",
      "INFO:lda:<50> log likelihood: -3877477\n",
      "INFO:lda:<60> log likelihood: -3861385\n",
      "INFO:lda:<70> log likelihood: -3844122\n",
      "INFO:lda:<80> log likelihood: -3836012\n",
      "INFO:lda:<90> log likelihood: -3828636\n",
      "INFO:lda:<99> log likelihood: -3823603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model done\n",
      "shape: (17, 5000)\n",
      "['00' '01' '02']\n",
      "[[3.32912446e-03 6.60315092e-04 2.86968749e-07]\n",
      " [2.82334340e-07 2.82334340e-07 2.82334340e-07]\n",
      " [3.41868654e-07 1.64131141e-03 1.26525589e-03]\n",
      " [1.90617792e-07 1.90617792e-07 1.90617792e-07]\n",
      " [4.56183568e-07 4.56183568e-07 4.56183568e-07]\n",
      " [9.18458220e-03 6.95748974e-07 6.95748974e-07]\n",
      " [6.51508242e-07 6.51508242e-07 6.51508242e-07]\n",
      " [3.63848057e-07 3.63848057e-07 3.63848057e-07]\n",
      " [2.46791708e-07 2.46791708e-07 2.46791708e-07]\n",
      " [4.15765841e-07 4.15765841e-07 4.15765841e-07]\n",
      " [3.88108360e-07 3.91989443e-05 3.88496468e-04]\n",
      " [4.26093996e-07 4.26093996e-07 4.26093996e-07]\n",
      " [2.53794224e-07 2.53794224e-07 2.53794224e-07]\n",
      " [8.56930806e-04 5.70906600e-07 5.70906600e-07]\n",
      " [3.80908849e-07 3.80908849e-07 3.80908849e-07]\n",
      " [3.32922729e-07 3.32922729e-07 3.32922729e-07]\n",
      " [5.62619557e-07 5.62619557e-07 5.62619557e-07]]\n",
      "topic: 0 sum: 0.9999999999999782\n",
      "topic: 1 sum: 1.0000000000000677\n",
      "topic: 2 sum: 0.9999999999999664\n",
      "topic: 3 sum: 0.9999999999999293\n",
      "topic: 4 sum: 0.9999999999999689\n",
      "topic: 5 sum: 0.9999999999999806\n",
      "topic: 6 sum: 0.999999999999993\n",
      "topic: 7 sum: 0.9999999999999393\n",
      "topic: 8 sum: 1.0000000000000984\n",
      "topic: 9 sum: 0.999999999999949\n",
      "topic: 10 sum: 1.0000000000000717\n",
      "topic: 11 sum: 0.9999999999999285\n",
      "*Topic 0\n",
      "- 链接 还有 京东 天猫 11 一起 活动 大牌 福利 搜索 超多 超级 这次 618 20 优惠 好物 赶紧 现在 30\n",
      "*Topic 1\n",
      "- 护肤 精华 皮肤 保湿 面霜 分享 肌肤 修护 好物 面膜 敏感 补水 换季 可以 使用 干燥 抗老 清爽 维稳 护肤品\n",
      "*Topic 2\n",
      "- 口红 底妆 粉底液 眼影 真的 分享 高级 妆容 颜色 适合 粉底 温柔 哑光 自然 持妆 质地 定妆 细腻 日常 搭配\n",
      "*Topic 3\n",
      "- 成分 肌肤 皮肤 效果 状态 吸收 质地 精华 修护 抗老 使用 熬夜 里面 添加 可以 紧致 容易 坚持 而且 真的\n",
      "*Topic 4\n",
      "- 味道 一起 香水 清新 喜欢 自己 香气 整个 生活 香味 感觉 系列 精致 舒适 身体 治愈 分享 香氛 一种 设计\n",
      "*Topic 5\n",
      "- 可以 90 到手 19 10 20 这个 不错 00 他家 12 29 39 小伙伴 30 14 好吃 可爱 喜欢 80\n",
      "*Topic 6\n",
      "- 计划 新年 完美 攻略 开春 好颜 大家 虎年 2022 过年 一年 新年好 美妆 今天 指南 春节 酒窝 冉冉 新潮 姐妹\n",
      "*Topic 7\n",
      "- plog 生活 vlog 日常 快乐 日记 记录 碎片 博主 真的 今天 你们 王国 潼话 最近 周末 开心 告别 一些 大家\n",
      "*Topic 8\n",
      "- 真的 这个 感觉 而且 可以 就是 但是 觉得 还是 不会 头发 所以 时候 那种 一样 喜欢 一点 起来 因为 之前\n",
      "*Topic 9\n",
      "- ootd 夏日 look 出游 春夏 春季 出街 春天 今天 氛围 今日 夏天 攻略 妆容 春日 时尚 搭配 美妆 真的 衣服\n",
      "*Topic 10\n",
      "- 妆容 美妆 今日 真的 化妆 今天 这个 姐妹 教程 眼妆 重点 可以 眼影 腮红 氛围 简单 睫毛 起来 好看 如何\n",
      "*Topic 11\n",
      "- 浪漫 心动 约会 时尚 礼物 氛围 限定 七夕 自己 可以 仪式 系列 生活 告白 情人节 美妆 什么 风格 今天 青年\n",
      "*Topic 12\n",
      "- 我们 自己 可以 就是 时候 一起 大家 真的 这个 没有 很多 一直 这次 现在 作为 还是 已经 工作 所以 觉得\n",
      "*Topic 13\n",
      "- 互动 大家 宝贝 粉丝 你们 评论 可以 私信 抽奖 日常 上榜 安排 多多 记得 美妆 感谢 宝子们 一起 继续 中奖\n",
      "*Topic 14\n",
      "- 分享 好物 种草 大家 你们 美妆 推荐 宝藏 今天 最近 看看 姐妹 彩妆 什么 购物 真的 一些 护肤 东西 抽奖\n",
      "*Topic 15\n",
      "- 护肤 今天 问题 大家 自己 姐妹 一定 如何 皮肤 分享 很多 选择 我们 正确 知道 就是 怎么 到底 重要 真的\n",
      "*Topic 16\n",
      "- 开箱 品牌 分享 礼盒 快乐 双十 大会 感谢 收到 一种 礼物 好物 美妆 各位 你们 抽奖 大家 pr 最近 生活\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(df_total[\"content_cutted\"])\n",
    "vocab=tf_vectorizer.get_feature_names_out()\n",
    "model = lda.LDA(n_topics=17, n_iter=100, random_state=1)  \n",
    "model.fit(tf)\n",
    "print('model done')\n",
    "\n",
    "#主题-单词（topic-word）分布\n",
    "topic_word = model.topic_word_ \n",
    "print(\"shape: {}\".format(topic_word.shape))\n",
    "print(vocab[:3])\n",
    "print(topic_word[:, :3])\n",
    "for n in range(12):\n",
    "    sum_pr = sum(topic_word[n,:])  \n",
    "    print(\"topic: {} sum: {}\".format(n, sum_pr))\n",
    "\n",
    "#计算各主题Top-N个单词\n",
    "import numpy as np\n",
    "n = 20\n",
    "for i, topic_dist in enumerate(topic_word):  \n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]  \n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be01f530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "INFO:lda:n_documents: 17631\n",
      "INFO:lda:vocab_size: 5000\n",
      "INFO:lda:n_words: 475044\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -5256449\n",
      "INFO:lda:<10> log likelihood: -4173781\n",
      "INFO:lda:<20> log likelihood: -3971641\n",
      "INFO:lda:<30> log likelihood: -3900621\n",
      "INFO:lda:<40> log likelihood: -3868589\n",
      "INFO:lda:<50> log likelihood: -3849814\n",
      "INFO:lda:<60> log likelihood: -3838489\n",
      "INFO:lda:<70> log likelihood: -3829150\n",
      "INFO:lda:<80> log likelihood: -3824608\n",
      "INFO:lda:<90> log likelihood: -3820943\n",
      "INFO:lda:<99> log likelihood: -3817735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model done\n",
      "shape: (10, 5000)\n",
      "['00' '01' '02']\n",
      "[[1.39157540e-07 1.39157540e-07 1.39157540e-07]\n",
      " [6.03582066e-03 3.91910958e-07 3.91910958e-07]\n",
      " [1.97804371e-07 6.52952230e-04 8.90317476e-04]\n",
      " [2.50187641e-07 2.50187641e-07 2.50187641e-07]\n",
      " [2.12071086e-07 2.12071086e-07 2.12071086e-07]\n",
      " [2.73064302e-03 6.51536785e-04 2.50494727e-07]\n",
      " [1.96394202e-07 2.55508857e-04 3.94752347e-05]\n",
      " [2.30451916e-07 2.30451916e-07 2.30451916e-07]\n",
      " [2.83792604e-07 2.83792604e-07 2.83792604e-07]\n",
      " [1.40807390e-07 1.40807390e-07 1.40807390e-07]]\n",
      "topic: 0 sum: 1.0000000000000324\n",
      "topic: 1 sum: 0.9999999999999368\n",
      "topic: 2 sum: 0.9999999999999579\n",
      "topic: 3 sum: 0.9999999999999616\n",
      "topic: 4 sum: 1.000000000000054\n",
      "topic: 5 sum: 0.9999999999999646\n",
      "topic: 6 sum: 1.0000000000000375\n",
      "topic: 7 sum: 1.0000000000000293\n",
      "topic: 8 sum: 1.0000000000001172\n",
      "topic: 9 sum: 0.9999999999999415\n",
      "*Topic 0\n",
      "- 我们 就是 自己 里面 现在 可以 链接 而且 不仅 作为\n",
      "*Topic 1\n",
      "- 可以 互动 粉丝 90 大家 10 私信 宝贝 到手 抽奖\n",
      "*Topic 2\n",
      "- 妆容 口红 眼影 氛围 真的 温柔 美妆 今日 高级 颜色\n",
      "*Topic 3\n",
      "- 生活 plog 日常 快乐 vlog 日记 记录 今天 今日 美妆\n",
      "*Topic 4\n",
      "- 今天 分享 夏日 look 变美 攻略 春季 出街 春夏 出游\n",
      "*Topic 5\n",
      "- 链接 还有 京东 一起 天猫 11 活动 可以 超级 大牌\n",
      "*Topic 6\n",
      "- 真的 不会 可以 而且 就是 这个 感觉 头发 好物 底妆\n",
      "*Topic 7\n",
      "- 计划 新年 完美 攻略 真的 开春 好颜 大家 自己 这个\n",
      "*Topic 8\n",
      "- 分享 好物 开箱 大家 美妆 快乐 礼盒 品牌 礼物 你们\n",
      "*Topic 9\n",
      "- 护肤 皮肤 精华 肌肤 修护 抗老 保湿 面霜 敏感 分享\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(df_total[\"content_cutted\"])\n",
    "vocab=tf_vectorizer.get_feature_names_out()\n",
    "model = lda.LDA(n_topics=10, n_iter=100, random_state=1)  \n",
    "model.fit(tf)\n",
    "print('model done')\n",
    "\n",
    "#主题-单词（topic-word）分布\n",
    "topic_word = model.topic_word_ \n",
    "print(\"shape: {}\".format(topic_word.shape))\n",
    "print(vocab[:3])\n",
    "print(topic_word[:, :3])\n",
    "for n in range(10):\n",
    "    sum_pr = sum(topic_word[n,:])  \n",
    "    print(\"topic: {} sum: {}\".format(n, sum_pr))\n",
    "\n",
    "#计算各主题Top-N个单词\n",
    "import numpy as np\n",
    "n = 10\n",
    "for i, topic_dist in enumerate(topic_word):  \n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]  \n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65dae9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# 周小仙yoo的微博视频', '# 周小仙yoo的微博', '#周小仙yoo的微博视频', '#周小仙yoo的微博', '周小仙yoo', '# 张进ZJZJ的微博视频', '# 张进ZJZJ的微博', '#张进ZJZJ的微博视频', '#张进ZJZJ的微博', '张进ZJZJ', '# 陈大皮儿的微博视频', '# 陈大皮儿的微博', '#陈大皮儿的微博视频', '#陈大皮儿的微博', '陈大皮儿', '# 马锐的微博视频', '# 马锐的微博', '#马锐的微博视频', '#马锐的微博', '马锐', '# Kiko成晨的微博视频', '# Kiko成晨的微博', '#Kiko成晨的微博视频', '#Kiko成晨的微博', 'Kiko成晨', '# 彤彤_sakura的微博视频', '# 彤彤_sakura的微博', '#彤彤_sakura的微博视频', '#彤彤_sakura的微博', '彤彤_sakura', '# 手边巴黎urruolan的微博视频', '# 手边巴黎urruolan的微博', '#手边巴黎urruolan的微博视频', '#手边巴黎urruolan的微博', '手边巴黎urruolan', '# 叫我大表哥好吗好的的微博视频', '# 叫我大表哥好吗好的的微博', '#叫我大表哥好吗好的的微博视频', '#叫我大表哥好吗好的的微博', '叫我大表哥好吗好的', '# 牛明昱的微博视频', '# 牛明昱的微博', '#牛明昱的微博视频', '#牛明昱的微博', '牛明昱', '# LookLana的微博视频', '# LookLana的微博', '#LookLana的微博视频', '#LookLana的微博', 'LookLana', '# 孙一玮Well的微博视频', '# 孙一玮Well的微博', '#孙一玮Well的微博视频', '#孙一玮Well的微博', '孙一玮Well', '# Hedy北北的微博视频', '# Hedy北北的微博', '#Hedy北北的微博视频', '#Hedy北北的微博', 'Hedy北北', '# Ruby幼熙的微博视频', '# Ruby幼熙的微博', '#Ruby幼熙的微博视频', '#Ruby幼熙的微博', 'Ruby幼熙', '# 蒲一雯iwen的微博视频', '# 蒲一雯iwen的微博', '#蒲一雯iwen的微博视频', '#蒲一雯iwen的微博', '蒲一雯iwen', '# 美少女毛容易的微博视频', '# 美少女毛容易的微博', '#美少女毛容易的微博视频', '#美少女毛容易的微博', '美少女毛容易', '# Joey土播鼠的微博视频', '# Joey土播鼠的微博', '#Joey土播鼠的微博视频', '#Joey土播鼠的微博', 'Joey土播鼠', '# 章解放_的微博视频', '# 章解放_的微博', '#章解放_的微博视频', '#章解放_的微博', '章解放_', '# 一枝南南的微博视频', '# 一枝南南的微博', '#一枝南南的微博视频', '#一枝南南的微博', '一枝南南', '# 潘白雪s的微博视频', '# 潘白雪s的微博', '#潘白雪s的微博视频', '#潘白雪s的微博', '潘白雪s', '# 一番fane的微博视频', '# 一番fane的微博', '#一番fane的微博视频', '#一番fane的微博', '一番fane', '# 美少女Lisa酱的微博视频', '# 美少女Lisa酱的微博', '#美少女Lisa酱的微博视频', '#美少女Lisa酱的微博', '美少女Lisa酱', '# 潘南竹呀的微博视频', '# 潘南竹呀的微博', '#潘南竹呀的微博视频', '#潘南竹呀的微博', '潘南竹呀', '# 小考拉Lake的微博视频', '# 小考拉Lake的微博', '#小考拉Lake的微博视频', '#小考拉Lake的微博', '小考拉Lake', '# 周姊伊的微博视频', '# 周姊伊的微博', '#周姊伊的微博视频', '#周姊伊的微博', '周姊伊', '# 种草达人绵绵酱的微博视频', '# 种草达人绵绵酱的微博', '#种草达人绵绵酱的微博视频', '#种草达人绵绵酱的微博', '种草达人绵绵酱', '# Yuki_优酱的微博视频', '# Yuki_优酱的微博', '#Yuki_优酱的微博视频', '#Yuki_优酱的微博', 'Yuki_优酱', '# 兰阿雯的微博视频', '# 兰阿雯的微博', '#兰阿雯的微博视频', '#兰阿雯的微博', '兰阿雯', '# 优莉Uli的微博视频', '# 优莉Uli的微博', '#优莉Uli的微博视频', '#优莉Uli的微博', '优莉Uli', '# Echo桃小小的微博视频', '# Echo桃小小的微博', '#Echo桃小小的微博视频', '#Echo桃小小的微博', 'Echo桃小小', '# 呦呦仔的微博视频', '# 呦呦仔的微博', '#呦呦仔的微博视频', '#呦呦仔的微博', '呦呦仔', '# 宋素雯的微博视频', '# 宋素雯的微博', '#宋素雯的微博视频', '#宋素雯的微博', '宋素雯', '# 月野皮皮的微博视频', '# 月野皮皮的微博', '#月野皮皮的微博视频', '#月野皮皮的微博', '月野皮皮', '# 美硕的成分测评的微博视频', '# 美硕的成分测评的微博', '#美硕的成分测评的微博视频', '#美硕的成分测评的微博', '美硕的成分测评', '# 桃子百莉的微博视频', '# 桃子百莉的微博', '#桃子百莉的微博视频', '#桃子百莉的微博', '桃子百莉', '# 苍口小梨涡的微博视频', '# 苍口小梨涡的微博', '#苍口小梨涡的微博视频', '#苍口小梨涡的微博', '苍口小梨涡', '# Fairy小默的微博视频', '# Fairy小默的微博', '#Fairy小默的微博视频', '#Fairy小默的微博', 'Fairy小默', '# 莓灵酱的微博视频', '# 莓灵酱的微博', '#莓灵酱的微博视频', '#莓灵酱的微博', '莓灵酱', '# 你Rui哥的微博视频', '# 你Rui哥的微博', '#你Rui哥的微博视频', '#你Rui哥的微博', '你Rui哥', '# 刘魔王大人-的微博视频', '# 刘魔王大人-的微博', '#刘魔王大人-的微博视频', '#刘魔王大人-的微博', '刘魔王大人-', '# 蟹阿文AWEN的微博视频', '# 蟹阿文AWEN的微博', '#蟹阿文AWEN的微博视频', '#蟹阿文AWEN的微博', '蟹阿文AWEN', '# 陈端端儿的微博视频', '# 陈端端儿的微博', '#陈端端儿的微博视频', '#陈端端儿的微博', '陈端端儿', '# 迟池Chichi的微博视频', '# 迟池Chichi的微博', '#迟池Chichi的微博视频', '#迟池Chichi的微博', '迟池Chichi', '# 叫我桃maymay的微博视频', '# 叫我桃maymay的微博', '#叫我桃maymay的微博视频', '#叫我桃maymay的微博', '叫我桃maymay', '# 种草颜究生的微博视频', '# 种草颜究生的微博', '#种草颜究生的微博视频', '#种草颜究生的微博', '种草颜究生']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "blogger_names = [\n",
    "\"周小仙yoo\", \"张进ZJZJ\", \"陈大皮儿\", \"马锐\", \n",
    "\"Kiko成晨\", \"彤彤_sakura\",\"手边巴黎urruolan\", \"叫我大表哥好吗好的\", \n",
    "\"牛明昱\", \"LookLana\",\"孙一玮Well\", \"Hedy北北\", \n",
    "\"Ruby幼熙\",\"蒲一雯iwen\", \"美少女毛容易\", \"Joey土播鼠\", \n",
    "\"章解放_\", \"一枝南南\", \"潘白雪s\",\"一番fane\",\n",
    "\"美少女Lisa酱\",\"潘南竹呀\",\"小考拉Lake\", \"周姊伊\",\n",
    "\"种草达人绵绵酱\", \"Yuki_优酱\", \"兰阿雯\", \"优莉Uli\",\n",
    "\"Echo桃小小\", \"呦呦仔\", \"宋素雯\", \"月野皮皮\", \n",
    "\"美硕的成分测评\", \"桃子百莉\",\"苍口小梨涡\", \"Fairy小默\", \n",
    "\"莓灵酱\", \"你Rui哥\", \"刘魔王大人-\", \"蟹阿文AWEN\",\n",
    "\"陈端端儿\", \"迟池Chichi\", \"叫我桃maymay\", \"种草颜究生\"\n",
    "]\n",
    "\n",
    "self_references = []\n",
    "\n",
    "for name in blogger_names:\n",
    "    reference1 = f\"# {name}的微博视频\"\n",
    "    reference2 = f\"# {name}的微博\"\n",
    "    reference3 = f\"#{name}的微博视频\"\n",
    "    reference4 = f\"#{name}的微博\"\n",
    "    reference5 = name\n",
    "    self_references.append(reference1)\n",
    "    self_references.append(reference2)\n",
    "    self_references.append(reference3)\n",
    "    self_references.append(reference4)\n",
    "    self_references.append(reference5)\n",
    "\n",
    "blogger_names_variants = [\n",
    "    \"周小仙yoo\", \"周小仙\", \"yoo\",\n",
    "    \"张进ZJZJ\", \"张进\", \"ZJZJ\", \"zjzj\",\n",
    "    \"陈大皮儿\",\"陈大皮\"\n",
    "    \"马锐\",\n",
    "    \n",
    "    \"Kiko成晨\", \"Kiko\", \"成晨\",\n",
    "    \"彤彤_sakura\", \"彤彤\", \"sakura\",\n",
    "    \"手边巴黎urruolan\", \"手边巴黎\", \"urruolan\",\n",
    "    \"叫我大表哥好吗好的\",\n",
    "    \n",
    "    \"牛明昱\",\n",
    "    \"LookLana\",\n",
    "    \"孙一玮Well\", \"孙一玮\", \"Well\",\"well\",\n",
    "    \"Hedy北北\", \"Hedy\", \"北北\",\"hedy\",\n",
    "    \n",
    "    \"Ruby幼熙\", \"Ruby\", \"幼熙\",\"ruby\",\n",
    "    \"蒲一雯iwen\", \"蒲一雯\", \"iwen\",\n",
    "    \"美少女毛容易\", \"毛容易\",\n",
    "    \"Joey土播鼠\", \"Joey\",\"joey\",\n",
    "    \n",
    "    \"章解放_\", \"章解放\",\n",
    "    \"一枝南南\", \"南南\", \"一枝\",\n",
    "    \"潘白雪s\", \"潘白雪\",\n",
    "    \"一番fane\",\"fane\",\"一番\",\n",
    "    \n",
    "    \"美少女Lisa酱\",\"Lisa酱\",\n",
    "    \"潘南竹呀\",\"潘南竹\",\"南竹\",\n",
    "    \"小考拉Lake\", \"小考拉\", \"Lake\",\"lake\",\n",
    "    \"周姊伊\",\n",
    "    \n",
    "    \"种草达人绵绵酱\", \"绵绵酱\",\n",
    "    \"Yuki_优酱\", \"Yuki\", \"优酱\",\n",
    "    \"兰阿雯\",\n",
    "    \"优莉Uli\", \"优莉\", \"Uli\",\"uli\",\n",
    "    \n",
    "    \"Echo桃小小\", \"Echo\", \"桃小小\",\"echo\",\n",
    "    \"呦呦仔\",\n",
    "    \"宋素雯\",\n",
    "    \"月野皮皮\", \"皮皮\", \"月野\",\n",
    "    \n",
    "    \"美硕的成分测评\", \"美硕\",\n",
    "    \"桃子百莉\", \"百莉\",\n",
    "    \"苍口小梨涡\",\n",
    "    \"Fairy小默\", \"Fairy\", \"小默\",\"fairy\",\n",
    "    \n",
    "    \"莓灵酱\",\n",
    "    \"你Rui哥\",\n",
    "    \"刘魔王大人-\",\n",
    "    \"蟹阿文AWEN\", \"蟹阿文\", \"AWEN\",\"awen\",\n",
    "    \n",
    "    \"陈端端儿\",\n",
    "    \"迟池Chichi\", \"迟池\", \"Chichi\",\"chi\",\"Chi\",\n",
    "    \"叫我桃maymay\", \"maymay\", \"may\",\"May\",\n",
    "    \"种草颜究生\"\n",
    "]\n",
    "\n",
    "\n",
    "print(self_references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b1d55f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['兰阿雯', '#周姊伊的微博视频', 'Lisa酱', 'joey', '#章解放_的微博', '#Echo桃小小的微博视频', '#Joey土播鼠的微博视频', '# 迟池Chichi的微博', '蒲一雯', '#陈大皮儿的微博视频', '#周小仙yoo的微博视频', '#陈大皮儿的微博', '宋素雯', '# 蒲一雯iwen的微博', '# Fairy小默的微博', '# 马锐的微博视频', '#美硕的成分测评的微博', '#宋素雯的微博视频', '#Kiko成晨的微博视频', '#美少女Lisa酱的微博', '# 蟹阿文AWEN的微博视频', '# 美硕的成分测评的微博视频', '周小仙', '马锐', '#种草颜究生的微博', '#潘南竹呀的微博', '成晨', '# 一番fane的微博', '呦呦仔', '#兰阿雯的微博视频', '# 呦呦仔的微博视频', 'hedy', 'Ruby幼熙', '幼熙', 'awen', '#美少女Lisa酱的微博视频', 'uli', '一番fane', '#牛明昱的微博视频', '一枝南南', '#一番fane的微博', '# 月野皮皮的微博', '# 桃子百莉的微博', '迟池Chichi', '手边巴黎', 'LookLana', '#桃子百莉的微博视频', '#莓灵酱的微博', '#Ruby幼熙的微博视频', '#优莉Uli的微博', 'iwen', '皮皮', '#潘白雪s的微博', '莓灵酱', '# 孙一玮Well的微博', '#周小仙yoo的微博', '彤彤_sakura', '# 种草颜究生的微博视频', '#马锐的微博视频', '# 莓灵酱的微博视频', '彤彤', '潘白雪', '张进', 'urruolan', '#陈端端儿的微博', '#种草达人绵绵酱的微博视频', '#Yuki_优酱的微博视频', 'Hedy北北', 'AWEN', '小默', '#一番fane的微博视频', '优莉Uli', '#孙一玮Well的微博视频', '#苍口小梨涡的微博视频', 'zjzj', '#优莉Uli的微博视频', '一番', '优莉', '# Echo桃小小的微博视频', '#你Rui哥的微博视频', '#Kiko成晨的微博', '# 优莉Uli的微博视频', 'ZJZJ', '#蟹阿文AWEN的微博', '#张进ZJZJ的微博视频', '# 张进ZJZJ的微博视频', '月野', '# 一番fane的微博视频', '你Rui哥', 'Fairy小默', '# 迟池Chichi的微博视频', '#LookLana的微博', '# 美硕的成分测评的微博', '#Hedy北北的微博', '# 叫我桃maymay的微博视频', '# 苍口小梨涡的微博', '# 手边巴黎urruolan的微博', '# 周姊伊的微博', '# 潘南竹呀的微博视频', '#月野皮皮的微博视频', '# 蟹阿文AWEN的微博', '桃小小', 'Yuki_优酱', '叫我大表哥好吗好的', '# 孙一玮Well的微博视频', '#莓灵酱的微博视频', '# 手边巴黎urruolan的微博视频', 'Echo', '刘魔王大人-', '# Kiko成晨的微博视频', '潘南竹呀', '# 周小仙yoo的微博视频', '# 优莉Uli的微博', '#美硕的成分测评的微博视频', '# 周姊伊的微博视频', '#迟池Chichi的微博视频', '苍口小梨涡', '# LookLana的微博', '#叫我桃maymay的微博', 'Ruby', '# 桃子百莉的微博视频', '种草颜究生', '#马锐的微博', '孙一玮Well', '# 一枝南南的微博视频', '#手边巴黎urruolan的微博视频', '#蒲一雯iwen的微博', '# 潘白雪s的微博视频', 'Chi', 'Yuki', 'echo', '# 月野皮皮的微博视频', '#章解放_的微博视频', '# 呦呦仔的微博', '周姊伊', '#Ruby幼熙的微博', '#月野皮皮的微博', '北北', 'Echo桃小小', '# Joey土播鼠的微博视频', 'fairy', '章解放_', 'Uli', '#美少女毛容易的微博视频', '# 彤彤_sakura的微博', '蟹阿文', '# 陈大皮儿的微博视频', '#Yuki_优酱的微博', '#一枝南南的微博视频', '#Joey土播鼠的微博', '# 小考拉Lake的微博', '# 苍口小梨涡的微博视频', 'sakura', '#Hedy北北的微博视频', '# 兰阿雯的微博', '# 小考拉Lake的微博视频', '# 一枝南南的微博', '#彤彤_sakura的微博', '#周姊伊的微博', 'Well', '南南', '# 种草颜究生的微博', '#苍口小梨涡的微博', '#小考拉Lake的微博', '# 莓灵酱的微博', '#刘魔王大人-的微博视频', '#牛明昱的微博', '# 蒲一雯iwen的微博视频', '# 陈大皮儿的微博', '# Joey土播鼠的微博', '# Yuki_优酱的微博视频', '#呦呦仔的微博视频', '牛明昱', '# 种草达人绵绵酱的微博视频', '# 刘魔王大人-的微博', '# 彤彤_sakura的微博视频', '# 种草达人绵绵酱的微博', '#呦呦仔的微博', '# Hedy北北的微博视频', '# 叫我大表哥好吗好的的微博视频', '桃子百莉', '毛容易', '小考拉Lake', '#孙一玮Well的微博', '种草达人绵绵酱', 'fane', '# 美少女毛容易的微博', '# LookLana的微博视频', '# 宋素雯的微博', '南竹', 'maymay', '#潘白雪s的微博视频', '#蒲一雯iwen的微博视频', '# 章解放_的微博', '#兰阿雯的微博', '# 周小仙yoo的微博', '# Echo桃小小的微博', 'Kiko', '#刘魔王大人-的微博', '陈大皮马锐', '# Kiko成晨的微博', '# 美少女Lisa酱的微博视频', '叫我桃maymay', '潘南竹', '# 你Rui哥的微博视频', '周小仙yoo', 'May', 'ruby', 'may', '#叫我桃maymay的微博视频', '#桃子百莉的微博', '张进ZJZJ', '# Hedy北北的微博', '# 潘白雪s的微博', '章解放', '一枝', '孙一玮', '# 你Rui哥的微博', '月野皮皮', '小考拉', 'Joey土播鼠', '美少女Lisa酱', '# 潘南竹呀的微博', '#手边巴黎urruolan的微博', 'yoo', 'Hedy', '# 美少女毛容易的微博视频', '#张进ZJZJ的微博', '# 叫我桃maymay的微博', '# Fairy小默的微博视频', '# 宋素雯的微博视频', '美少女毛容易', '绵绵酱', '迟池', 'chi', '# 美少女Lisa酱的微博', '#你Rui哥的微博', '#LookLana的微博视频', '# 牛明昱的微博', '# 张进ZJZJ的微博', '# 陈端端儿的微博', '# 兰阿雯的微博视频', '#一枝南南的微博', '#彤彤_sakura的微博视频', '#叫我大表哥好吗好的的微博', '#Fairy小默的微博', '陈端端儿', '优酱', '#美少女毛容易的微博', '#小考拉Lake的微博视频', 'Chichi', '#潘南竹呀的微博视频', '# 马锐的微博', '# 牛明昱的微博视频', '美硕', 'Fairy', '百莉', 'well', '# 刘魔王大人-的微博视频', '#陈端端儿的微博视频', 'lake', '#Echo桃小小的微博', '# 叫我大表哥好吗好的的微博', '# Ruby幼熙的微博', '#Fairy小默的微博视频', '#种草达人绵绵酱的微博', '#蟹阿文AWEN的微博视频', '潘白雪s', 'Kiko成晨', 'Joey', '# Ruby幼熙的微博视频', '陈大皮儿', '#迟池Chichi的微博', '#宋素雯的微博', '#种草颜究生的微博视频', '#叫我大表哥好吗好的的微博视频', '美硕的成分测评', 'Lake', '蟹阿文AWEN', '蒲一雯iwen', '# Yuki_优酱的微博', '# 章解放_的微博视频', '手边巴黎urruolan', '# 陈端端儿的微博视频']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "combined_blogger_names = list(set(self_references + blogger_names_variants))\n",
    "print(combined_blogger_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85a35725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                 【直播预告｜年度爱用盘点（下）】趁热打铁，下期来啦！🏃♀️清洁｜氨基酸洁面慕斯•口腔冲牙器滋...\n",
      "1                 最好的礼物从来不是礼物本身而是意料之中的温暖和出其不意的惊喜💫#新年礼物已就位##上新了新年...\n",
      "2                 2022年最后一天必须抽“红包”呀㊗️大家新年红红火火～平安健康～🧨留言里揪3⃣️0⃣️位哦...\n",
      "3                 妆容变化！3年的变美逆袭秘诀[哇][哇]几个美妆技巧 就可以改变妆容！底妆——眼妆——中庭—...\n",
      "4                 贵气美女的养成茂密的头发和好皮肤才是关键哦喜欢熬夜的妹妹们看看这里～#爱用##好物分享#  ...\n",
      "                                        ...                        \n",
      "17626                                              2022的第二天也美美出门啦！ \n",
      "17627                                                   2022 远离烦恼🥳 \n",
      "17628             救命！！🆘\\n这个下睫毛也太好看了吧！！\\n也是胖给我推荐的，之前买分段式小恶魔睫毛那家16...\n",
      "17629                            2021年的遗憾不开心都过去啦！！\\n2022很崭新，要快乐噢！！ \n",
      "content_cutted    0                 下   直播 防护 滋补 （ • 精华 爱 抽奖 的 次...\n",
      "Length: 17631, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print(df_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c6e59ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['抽奖 详情', '最好 礼物 从来不 礼物 意料之中 温暖 出其不意 惊喜 新年礼物 就位 新 新年', '年 最后 一天 必须 抽 红包 新年 红红火火 平安 健康 留言 里 揪 位 潼话 王国 抽奖 详情', '', '贵气 美女 养成 茂密 头发 皮肤 关键 喜欢 熬夜 妹妹 看看 爱用 好物 分享', '元 宇宙 未来派 Vlogchi 跨次 元版 时间 地点 任务 任务 道具 长效 保湿 X T 定妆 法 岛民 秘诀 AYAYI 妆容 pk 随性 酷飒 or 甜 野玫瑰 星河 绝色 碎 钻感 一眼 穿越 寻色 未来 潮次 元 UD 海南 免税 仿佛 开发 元 宇宙 DNA 现场 堪称 玩妆 乐园 没定 年终 出游 目 全文', '一年 金够 拜 在家 过节 仪式 感拉满 祝 快乐 不止 圣诞 圣诞 香香的 圣诞节 宅家 日常', '已经 年末 几天 迎来 新 一年 想 做 事情 赶紧 做 今年 留下 遗憾 ps 最近 zun 超 喜欢 粉粉 嫩嫩的 穿 搭 衣服 chanel 包包 Amazing song 兔子 红', '年度 爱用物 最后 一天 整理 一下 今年 爱用物 超级 好用 性价比 贼 高 明年 妆容 超越 今年 美妆 真 爱 粉底 镜 睫毛 富家 千金 香水 掰 遮 瑕 实用 超软 眼影 刷 按摩 梳 粉底 刷 首饰 低价 美貌 今年 爱用 发掘 明年 更 进步 爱用物 快来 安利 一下 完美 新年 计划', '做 天 香香 美女 motd', '抽奖 详情', '好久没 唠 前段时间 熬夜 太狠 感觉 老 几岁 烦恼 姐妹 明天 锁定 直播间 教 几招 k 老小 tips 记得 抽 礼物 直想 聊 抗衰 tips', '年货 节 list 咯 彩虹 屁 陪 度过 月 修护 好物 彩虹 屁 身体 快速 恢复 好吃 彩虹 屁 年前 快乐 一下 潼话 王国 年前 养好 皮肤', '有福同享 系列 咯 周末 礼物 姐妹 赶紧 看看 有没有 喜欢 评 点赞 抽送 最后 一张 之前 开奖 记录', '薄荷 铁 ootd', '迟池 Chichi 年末 氛围 跨次 元 一下 蹭 波元 宇宙 热度 紫色 灯光 未来 感好 足 池池 碎 碎念', 'halo 好久不见 好物 分享 栏目 断断续续 拍 爱用 物们 使用 九宫格 不够 小猫 合影 凑 喵 喵 羊羊们 好好 照顾 多喝水 补充 维生素 蛋白质 提高 免疫力 没有 宝贝 注意 防护 礼盒 分享 礼盒 开箱 快乐 全文', '一直 觉得 爱 一件 重要 事 临近 年末 工作 越来越 忙碌 脸色 暗淡无光 苹果 肌 凹陷 坍塌 法令 纹 细纹 跑 真的 累丑 抽出 时间 宠爱 快乐 一种 选择 送 一份 特别 新年礼物 雅诗兰黛 白金 系列 明星 产品 打算 帮 我养 回好 肌肤 过个 年 雅诗兰黛 白金 系列 明星 产品 白金 级 花菁萃 紧颜 精华 水 白金 级 花菁萃 紧颜 眼部 精华 霜 白金 级 花菁萃 紧颜 精华 面霜 雅诗兰黛 家 高端 护肤 系列 有着 出众 抗老 修护 能力 是因为 含有 雅诗兰黛 家 尖端科技 SIRT 核芯 抗老 科技 基因 层 激活 长寿 基因 强化 细胞 白金 级 花菁萃 紧颜 精华 水 肤感 精华 水般 精华 水般 凝露 质地 稳定 敏感 肌肤 平衡 皮脂 膜 完 之后 肤质 细腻 柔滑 水润 一整天 改善 肤质 效果 真的 满分 白金 级 花菁萃 紧颜 眼部 精华 霜 心头 抚平 眼周 细纹 重构 眼周 核心 三角 区域 淡化 黑眼圈 化妆 眼周 不再 需要 费劲 遮 瑕 爱 白金 级 花菁萃 紧颜 精华 面霜 尖端 工艺 采摘 极地 之花 极地 花 酷寒 强烈 紫外线 环境 生长 拥有 很强 自愈 能力 结合 SIRT 核芯 抗老 科技 封存 活性 自愈 能量 精准 提升 肌肤 抵御 能力 即时 修护 效果 真的 很强 云 锻 质地 掌心 温度 激活 脸 马上 吸收 忙碌 止不住 熬夜 脸 容易 下垂 特别 有用 坚持 下来 恢复 弹润 紧致 肌肤 状态 最近 每晚 白金 系列 护肤 时 会 留下 一段 放松 和疗 愈 时间 一整套 坚持 下来 脸上 细纹 法令 纹 淡化 很多 透出 细腻 光泽感 宝子们 保持 肌肤 状态 白金 系列 明星 产品 真的 速速 入手 先 滋养 状态 生活 开心 帮宝子们 找到 福利 最多 秘密 雅诗兰黛 免税 旗舰 程序 美旅星 愿圈 很多 礼享 信息 专享 礼物 打卡 做 任务 机会 获得 礼享 镜界 数字 藏品 计划 出行 姐妹 现场 打卡 免税 专柜 线下 礼享 奇境 快 闪店 参与 趣味 拍照 打卡 活动 逛累 很多 家 合作 酒店 推出 礼享 假日 下午茶 直接 cdf 海南 免税 官方 商城 在线 单 排队 宅家 坐等 收货 网页 链接 好礼以 黛 奇享 乐园', '近期 碎片 告别 充实 一年 想 四个 词来 总结 认真 热爱 坚持 悦己 一条 plog 告别', '妆容 新 年画 没 问题 motd', '年前 仪式 感 超好 身体 护理 单品 回家 前 好好 捣拾 一下 过年 闪亮 登场 头部 打造 锻感 秀发 洗护 三件套 身体 洗澡 愉悦感 山茶花 沐浴 泡泡 身体 纵享 肌肤 丝滑 蜜罐 身体 乳 霜 唇部 粉嘟嘟 果冻 唇膜 头发 护理 身体 护理 豆豆 Babe 抽奖 详情', '紫色 霓虹 X 未来 潮次 元 这场 活动 审美 call 美死 年尾 囤出 生活 ootd', '今天 第十一天 浅浅 算是 阳康 写 一点 几天 亲身经历 分享 全家 中招 尽量 分房 睡 房间 里 密度 太高 刚好 可能 反复 发烧 本来 第三天 下午 已经 退烧 隔天 Alice 爹 搬回 房间 睡 晚上 立刻 烧回 烧 整晚 好几个 朋友 老公 分房 睡后 热度 降下来 不复 烧 不信 这病 反复 搞 心态 真的 会 反复 发烧 网上 很多 无症状 烧一晚 退 现实 中 亲戚朋友 起码 烧 天 我家 妈咪 只烧 一晚 度外 Alice 爹 我爸 反复 烧 三天 前两天 更是 吃 度 少看 网上 少 吹 b 聊天 好好 养病 就行了 反复 真的 大部分 发烧 前 每天 洗澡 洗头 家里 收拾 干净 之后 发烧 几天 真的 没 力气 洗澡 会 疯狂 出汗 头发 油 混 汗 干不了 程度 苦涩 苦涩 烧后 建议 天再 洗头 其实 刚 退烧 虚 立刻 洗头 真心 容易 复烧 咳嗽 鼻塞 or 痰 吃 一点 不用 硬熬 特别 注意 扁桃体 不要 发炎 尽量 前期 控制 住 盯 bu 洛芬 感 康泰 诺散 利痛 效果 止咳 推荐 金喉 剑 宣肺 止咳 合剂 中 中 真的 特别 难受 感觉 好多年 没病 重了 主要 病 完 后人 特别 虚 没 精神 易 出汗 容易 累 太耗 精气神 没中 小伙伴 千万 预防 潼话 王国', '看到 这条 微博 如愿 一条 plog 告别 十一月', '最后 一个月 抓紧 续上 小美盒 有没有 发现 最近 眼部 状态 特别 好物 分享', 'Vlog Chichi 奇享 乐园 惊喜 日记 北方 孩子 永远 无法 拒绝 海边 单身 青蛙 岁末 忙里偷闲 姐妹 约 一场 浪漫 悦己 旅 三亚 限定 贵妇 生活 度假 仪式 感 抗老 奢级 护肤 cp 打卡 游乐园 里 免税 专柜 礼享 假日 下午茶 time 护肤 不止 追求 功效 更 享受 片刻 放松 毕竟 全文', '结束 心动 好物 月 里 大牌 会员 福利 不用 手 淘 搜索 品牌 年度 会员 日 元 抽 毛戈平 人气 腮红 元 赢 FILA 万元 礼包 超多 品牌 限定 会员 礼 宝子们 快 一起 年度 会员 日 上天 猫 你立 flag 实现', 'Vlog 日常生活 记录 篇 忙碌 最近 忘记 吴哥 结婚 两周年 差点 跑 错 活动 现场 沉迷在 最近 浓浓的 圣诞 氛围 里 ps 话 说 最近 大街 没什么 保护 噶 增强 抵抗力 兔子 红', '美食 探店 开了个 新 栏目 第一期 探店 长沙 必 打卡 一家 店 笨 萝卜 跨年 吃饭 选好 大片 牛肉 酸菜 炒 粉皮 碎 红椒 炒肉 心里 前 三 试试 好吃 店 快 安利 下期 安排 咯 探店 vlog', '新年 YSL 大约 听到 心声 全新 YSL 藏金 面霜', '异地 跨年 带些 出差 旅行 好物 推荐 今年 计划 出去 跨年 狠狠 期待 住 打包 时间 猪猪 浅 唠 一会儿 分享 一下 豆仔 出行 好物 大容量 行李箱 手持 蒸汽熨斗 多功能 烧 水壶 养生 茶壶 简易版 睡套 跨年 红红火火 装 私人 安心 百宝 袋 加热式 蒸汽 颈椎 帖 应急 渍 笔 次 抛 精华 一次性 马桶 贴 mini 装 香水 胶囊 卸妆油 旅行 好物 豆豆 Babe 抽奖 详情', 'belief 太给力 盲 盒里 有款 洁面 爱 死 哈哈哈 家 产品 适合 白天 用纯 补水 厚重 油皮 刚刚 欧莱雅 玻 尿酸 水 光 面膜 挤完 精华 一定 膜 布 吸收 吸收 反正 补水 好使 植村秀 送 口红 已经 拥有 几十支 家 色系 适合 秋冬 知道 做到 每支 超显白 分享 礼盒 开箱 快乐 礼盒', '今年 月 收到 源力 面膜 已经 默默 半年 最近 剩下 片 囤 不敢 怕 拍照 没 东西 揣手 揣手 今天 终于 正大光明 夸夸 激动 嘞 第一次 感觉 离不开 源力 面膜 夏天 新疆 玩 日晒 时间 太长 一直 坐车 烤 脸 疼 行 随便 敷 片面 膜 缓解 晒 天 以后 敷 面膜 脸 疼 敷 完 隔天 爆皮 爆皮 最后 发现 敷源力 面膜 or fu 美 缓解 这种 超长 日晒 高温 带来 肌底 干燥 当时 心里 已经 非常 impressive 最近 源力 面膜 更是 升级成 患难之交 老粉 知道 月 有点 惨 皮肤 折腾 泛红 粗糙 皮肤 底子 有种 烧干 感觉 干 油 爆 好多 红肿 痘 赶紧 片面 膜 敷 感觉 说 好像 凉水 泼 桑拿 石头 呲 一声 全 吸收 膜 布 软 糯 凹 造型 鼻子 下面 嘴角 这种 高烧 特别 干燥 部位 轻轻 扯 一扯 完整 覆盖 最近 皮肤 特别 差 喜欢 先用 源力 精华 厚涂 敷上 源力 面膜 瞬间 降火 保湿 感觉 精华 长期 维稳 加上 面膜 紧急 修复 搭配 刚刚 整张 膜 布以 玻 尿酸 base 添加 温和 修护 成分 皮肤 再烂 干 不会 疼 感觉 皮肤 吨 吨 吨 喝水 先 补水 补油 敷 两天 第三天 滴 精华 油 打个 底 敷上 源力 面膜 精华 油 油感 完全 消失 特别 适合 这种 状态 差到 底层 炎症 导致 大爆 痘 皮肤 今天 拍照 皮 回来 春游 家族 春游 家族 月 下旬 朋友 跑 我安 利源 力 面膜 没发 guang 没有 试用 品 没想到 我俩 默默 享用 型 hhhh 源力 系列 非常 安心 做 修护 皮肤 可能 觉得 保湿 修护 没什么 皮肤 泛红 干痒 皮 处于 min 感 状态 需要 急救 知道 高效 温和 皮肤 吃 进去 修复 舒缓 珍贵 彩虹 屁 彩虹 屁', '重要 通知 长沙 想见 亲爱 今年 终于 机会 线下 一起 玩 茶 颜悦色 小龙虾 一年一度 微博 超级 红人节 月 号 号 长沙 找 玩 购票 通道 戳 超级 红人节 立即 开抢', '三亚 出差 小记 怀念 小阴人', '怀着 热情 say Hi Hi 老婆 不知不觉 一年 复盘 时刻 年终总结 VS 年终总结 整理 今年 份 素材 随手 整合 转瞬即逝 碎片 突然 串联 出 轮廓 记录 意义 回顾 时 好像 重回 那一刻 年终总结 技巧 今年 全文', '相爱多年 依然 热恋 期 分享 一下 爱情 保鲜 秘诀 爱是 双向 双旦 这种 节日 精心 挑选 体香 产品 送给 现在 很多 男生 精致 哒 理容 产品 真是 送礼 佳选 天猫 超级 品类 日 月 日 日 宝子们 上天 猫 搜索 天猫 超级 品类 日 挑选 理想 男友 新理物 氛围 感 体香 类 男士 理容 套装 全文', '最近 忙碌 吃喝 生活 share 降温 不会 养 膘 叭 好物 分享', '爆爽 零食 分享 马上 元旦 元旦 打算 屯一波 好吃 零食 好好 犒劳 钢镚儿 冰 熔岩 焦绿 酥 麻辣 牛肉 亲测 绝绝 好吃 吃 赚 欢迎 宝子 分享 觉得 好吃 零食 评论 区 互相 种草 一下 新年新 开场', 'AWs makeup 微醺 热 红酒 妆 圣诞 来袭 画 暖暖的 微醺 热 红酒 妆 朋友 一起 聚会 motd', '精致 妆容 秘诀 年度 爱 彩妆 大赏 抽奖 详情', '顶 黑眼圈 圣诞节 每天 四点 睡觉 真的 快 熬不住 时差 今晚 一定 早点 睡 点 节目 快乐 宝贝 年尾 囤出 生活 兔子 红', '年 圣诞节 拼拼凑凑 年 仪式 感拉 冲天 彩虹 屁 彩虹 屁 潼话 王国', 'vlog 游乐园 一天 浅仿 一个 泡泡 玛特 灵感 妆 今年 没 出过 灵感 妆 妆容 想来 点 正好 最近 收到 雅顿 泡泡 玛特 联名 礼盒 灵感 浅 仿一 下去 杭州 乐园 玩吧 赶上 一波 年末 节日 气氛 雅顿 POP 庆典 雅顿 轻感 金胶 雅顿 粉 胶次 抛 a 醇 泡泡 玛特 vlog', '即无束 欢愉 闪耀 鎏金 之夜', '金 拜金 拜 岁 岁 平安 叮叮 互动 榜 是不是 应景 心动 超崽 兑换 迟池 助理 找 组织 铁粉 群 迟 池子 qq 空间 铁粉 进 陪 跑 群 迟池 陪 跑 团 全文', '甜蜜 记录 糖分 含量 有点 高 嘻嘻 重新 发 一下 希望 没有 甜 齁 情侣 日常 发现 生活 中 美好', '欢迎 来到 GIVENCHY 限时 书店 芮欧 百货 ps 快 打卡 出片 超多 新品 发现', '马上 跨年 新 一年 一定 多多 利用 积极 心理学 默念 值得 接纳 缺点 放大 优点 开启 全新 快乐 一年 买 几件 新年 裙子 到时候 分享 年货 节 一件 降 可不 买 新 装备 大好时机 哈哈哈 新年新 开场', '兔年 最近 收到 好多 新年礼物 雅诗兰黛 兔年 限定 礼盒 几乎 一眼 爱住 完全 俘获 芳心 里面 第七代 小棕瓶 小棕瓶 眼霜 更是 心头 爱 空瓶 无数 兔年 限定 包装 颜值 真的 高 粉色 小花 插画 设计 超有 新年 氛围 过年 带回家 送礼 超有 排面 他俩 真的 挚爱 修护 CP 以前 熬夜 压力 毛孔 容易 疯狂 出 油 皮肤 稳定 爱 过敏 泛红 高高挂起 熊猫眼 他俩 救回来 先 爱 棕瓶 大学 冲着 里面 二裂 酵母 透明质 酸去 当时 添加 珍贵 成分 护肤品 印象 里 棕瓶 总是 一个 收入 瓶 中 成分 追求 极致 一条 路 走 年 现在 棕瓶里 更 成分 专利 修护 成分 律波 肽 高效 优质 分子 肽 拔高 细胞 吸收 效能 拿捏 住 肌肤 夜间 修护 黄金 期 加速 修护 细胞 损伤 经常 熬夜 身体 生物钟 紊乱 肌肤 生物钟 小棕瓶 里 猴面包树 籽 提取物 拨 正 肌肤 紊乱 生物钟 监督 肌肤 细胞 早早 睡 始终保持 健康 状态 熬夜 脸 放心 交给 他俩 解决 棕瓶 大学 坚持 用到 现在 皮肤 水油 平衡 熬夜 不会 变身 油田 始终 水润 透亮 状态 现在 皮肤 爱 过敏 换季 稳稳当当 脸蛋 问题 解决 令人 头疼 黑眼圈 交给 小棕瓶 眼霜 同为 棕 家族 那律波 肽 自然 少不了 全新 自由基 智感 防御 科技 源头 阻击 自由基 损伤 有效 淡化 色 区 黑眼圈 淡 褪 顽固 眼纹 现在 看得出 熬 大夜 今年 雅诗兰黛 兔年 限定 礼盒 一定 会 C 位出 圈 高质量 修护 CP 会 爱 礼盒 里 开运 小兔 美愿 贺卡 选它 送礼 再也 不用 愁 美愿 绽 新生 京东 魔方 参与 京东 金 魔方 新品 日 get 同款 戳 链接 了解 更 优惠 网页 链接', '月 PR 礼物 开箱 一期 场面 护肤品 牌 做起 限定 周边 真的 太卷 全球 限量 瓶 香水 收到 编号 竟然 变 身 夸夸 队 队长 新品 彩妆 懂 女生 本期 真的 大饱眼福 猪猪 参考 新年礼物 选 开箱 豆豆 Babe 抽奖 详情', '没想到 两年 前 买 衣服 现在 穿 哈哈哈哈 最近 食欲 没 味觉 年尾 囤出 生活 兔子 红', '金钩 拜 金钩 拜 金钩 今年 分外 想要 平安 健康 快乐 潼话 王国', '粉金 小蛮 腰 温柔 颜色 迷糊', '私藏 抗衰 好物 公开 想要 知道 初 抗老 宝贝 快 来看 亲测 有效 抗初 老 产品 少花 冤枉钱 对症下药 新手 稳稳 上车 好物 分享 爱用', '迟池 Chichi 粉粉 白金 之旅 免税店 门口 咔 咔 一下 像不像 游乐园 三亚 季节 舒服 微凉 微风 微好 逛 池池 碎 碎念 三亚', '圣诞 新年 氛围 越来越 浓厚 最近 囤 很多 燕窝 开启 入冬 养生 新年礼物 送人 不错 子 冬日 自洽 时刻 plog 博主 日记', '不知不觉 已经 吴哥 结婚 两周年 周末 大吃大喝 一通 快落 ps 宝子们 快快 找到 另一半 冬天 希望 有人 捂 手手 motd 女孩子 出门 必备', '一组 口红 品宣 拍照 花絮 首饰 真的 瞬间 提升 妆造 精致 感 兔子 红', 'AWs sharing 年终 好物 年底 看看 今年 各大 品牌 好东东 好物 分享', '', '娇兰 蛮 仪式 感滴 每年 送 口红 植村秀 真 大方 今年 礼盒 送 六七次 彩虹 屁 揪 宝宝 送 同款 快乐 芭比 波朗 橘子 礼盒 真滴 可爱 用空 好几 罐 礼盒 分享 礼盒 开箱 快乐', '没想到 一波 周年 咯 能少 亲爱 索 老板 正好 刚要 出 品 没收 那种 热乎 立刻 薅 份 庆生 彩虹 屁 彩虹 屁 索 老板 名言 我要 做 潼话 王国里 除潼 潼外 最大 方 女伦 潼话 王国 抽奖 详情', '来来来 京东 美妆 最后 一波 作业 赶快 抄 京东 美妆 直球 价 两天 想 买 东西 没 抢 姐妹 速速 过来 整理 一份 爱 用品 清单 心头 京东 美妆 闭眼 不用 预售 不用 蹲点 抢 大牌 好货 不止 买 一赠 赠品 无敌 不想 物流 想 快点 直接 京东 买 戳 网页 链接 京东 生活 多点 实在 口碑 爆款 好价 跟着 买 绝对 不会 出错', '黑色 头发 突然 看不惯 存图 嘻嘻 ootd', '圣诞 月 开箱 不愧 喜欢 月 拆 礼物 嘎嘎 香 年底 品牌 卷 眼福 奥密克戎 苦 终于 吃 完 老婆 还好 池池 碎 碎念', '装修 日记 长辈 装 新家 现在 进入 软装 阶段 距离 家 分钟 车程 喜欢 窗外 山景 明年 夏天 想 搬 过来 住 节奏 美好 家居 打造 法 plog 博主 日记 家居', '秋冬 锁死 这套 底妆 搭配 真的 太太 太 无暇 持妆 Abyb charming 超级 红人节 超红扮 靓 现场', '十二月 有太多 盼头 盼 平安夜 圣诞 跨年 烟花 回家 歲 末 將至 祝 宝子 平安 喜樂 愿 不止 圣诞快乐 一天 兔子 红', '圣诞 月 快乐 SHUXUAN G ootd', '宅家 暖冬 好物 兼具 保暖 舒适度 猪猪 现在 几度 上海 已经 降温 零下 只能 老老实实 窝 家里 取暖 分享 近期 本豆 仔 宅家 四件套 穿着 睡 家居服 柔软 亲肤 抑 螨 冬日 必备 绒感 家居服 宽松 暖 糯可外 穿 洋气 百搭 红色 袜子 新年 穿 搭 加分 轻盈 保暖 鹅绒 冬 绒量 充足 不压身 居家 好物 豆豆 Babe 抽奖 详情', '苦涩 距离 上次 开箱 一年 攒 半个 月 忍住 没 开 今天 分享 一下 收到 礼盒 贴进 跨年 圣诞 知道 可能 一年 当中 最好 礼盒 哈哈哈哈 有点 技术含量 美力 定义 女性 美', '周年 生日 礼物 流星雨 继续 每次 开心 时刻 少不了 wuli 瑷 尔博士 春游 家族 份福袋 安排 今年过年 回家 咯 ps 先抽 几天 新年 抽 一次 潼话 王国 抽奖 详情', '昆仑 煮 雪 一闻 难忘 真的 喜欢 观夏 杭州 店 氛围 天目 里 拍照 出片 打卡 哈哈哈哈', '开盲 盒 太 快乐 圣诞节 马上 看看 圣诞 盲 盒里 春游 家族 春游 家族 真的 很大 爱用 好物 分享', '沸羊羊 日记 喜提 青青草原 入场券 一点点 子 病中 小心 池池 碎 碎念 生病 一分钟 难熬 挺 遭罪 单身 青蛙', '穿 搭 风格 生活 态度 保持 热爱 保持 舒适 自由 中 寻觅 真 新浪 时尚 风格 大赏 百 V 评审团 关注 风格 大赏 起来 探索 定义 时尚 风格', 'VLOG 记录 生活 忙碌 工作 姐妹 生日 短途 旅程 生活 应该 丰富多彩 最近 食欲 暴涨 新一轮 减肥 time 超级 红人节 超红扮 靓 现场', '咳嗽 一个多 礼拜 终于 身体健康 幸福 穿 新 买 衣服 拍 组 冷淡 潮酷 风 年尾 囤出 生活', 'GRWM 一起 夜间 护理 卸下 一天 疲惫 好好 享受 晚间 护肤 治愈 时刻 motd 好物 分享', '今日 份 换头 纯野 辣妹 妆 欢迎 走进 辣妹 一天 工作 拍摄 幕后 吃饭 日常 记录 BTW 妆 敲 好看 敲 上镜 画 好看 VLOG 妆容 分享 豆豆 Babe 抽奖 详情', '捉个 晴天 雾 霾 天 太阳 太难 幸好 摄影师 会 人造 太阳 哈哈哈 哈哈哈 太 难为人 美力 定义 女性 美', '周年 生日 礼物 流星雨 咯 皮宝儿 年底 终于 薅 上课 植物 派对 油敏 肌 礼盒 份 潼话 王国 网页 链接', 'w 误认为 女明星 妆 真的 上镜 精致 近期 爱 秋冬 雾面 毛绒 感 妆 氛围 细节 满满 睫毛 眉毛 重点 麦吉丽 精萃 平衡 水 麦吉丽 化妆 双十', '天气 突然 降温 产品 换换 新 最近 温差 皮肤 总是 过敏 泛红 搭配 兰嘉丝 汀 晚霜 霏 丝佳 乳液 一起 使用 皮肤 不适 缓解 好多 敏感 肌太 需要 每天 画妆 新 买 Anastasia 眼影 盘 近期 新宠 颜色 太美 适合 秋冬 爱用 好物 分享', 'OPPO 二代 折叠 屏 Find N 终于 官宣 轻薄 折叠 屏 真的 一眼 心动 重量 仅 g 刷新 折叠 屏 记录 做到 行业 轻 升级 超轻 铰链 更轻 更 窄 更 坚固 折叠 体积 缩为 原来 一半 黄金 尺寸 手 小女生 单手操作 压力 折叠 轻巧 可爱 任意 窗外 屏像 哆 A 梦 口袋 往外 推 放在 口袋 随身携带 还超 轻便 全文', '提前 拍 圣诞 宝们 造型 好看 一眼 看中 这套 明艳 圣诞 公主 整个 妆造 一种 热情 可爱 甜心 感 裙子 经典 圣诞红 抓人 眼球 头上 可爱 爱 蝴蝶结 妆 面上 bulingbuling 水钻 整个 俏皮 住 适配 度 今年 圣诞 公主 我超 满意 明年 宝子们 希望 全文', '月 中 快乐 一波 超级 红人节 超红扮 靓 现场', '寻找 温暖 接力 官 孩子 过冬 发愁 添 一件 衣服 孩子 感受 温暖 力量 成功 开通 微博 公益 赞赏 功能 冬天 赞赏 金额 自动 捐赠 壹 基金 温暖 包 公益 项目 一起 参与 接力 以微 博之力 寒冬 中 孩子 送 温暖 网页 链接', '冬日 白月光 底妆 组合 太 戳 冬天 想要 底妆 不卡粉 定妆 喷雾 一定 要用 CT 喷雾 宝子们 夸 多次 持妆 保湿 新品 白月光 粉饼 惊艳 粉质 细腻 轻薄 脸 白白净净 炒 鸡 自然 清透妆容 搭配 自然 系 腮红 黛珂 PK 温柔 气质 香芋 粉 很百搭 motd 好物 分享', '冬季 保暖 必杀技 羽绒服 合集 天气 越来越 冷 做好 保暖 感冒 冬季 保暖 必然 羽绒服 这期 羽绒 合集 挑 久 平价 小众 贵价 统统 简约 高级 娇俏 可爱 质感 百搭 快点 进 视频 看看 哪款 更 戳 羽绒服 冬季 穿 搭 豆豆 Babe 抽奖 详情', '说 看看 植村秀 美少女 战士 联名 快 闪店 巨好 拍 一定 一定 穿 带 闪片 衣服 闪光灯 一打 太太 好看 仔细 一下 现场 灯光 花 心思 那种 梦幻 光影 超有 代入 感', 'PLOG 精神 好点 赶紧 冒个 泡 潼话 王国', 'Plog 咯 清空 最近 手机 相册 真的 压箱底 图 找 月 月 真的 参加 很多 朋友 生日 聚会 天秤 天蝎 yyds 祝 周末 愉快 评论 区', '分享 最近 爱用 好物 发现 新 祛痘 宝藏 好物 分享 爱用', '雪地 精灵 冻伤 妆 美商 终于 更新 久违 玩妆 浅试 拽 灵 雪地 小精灵 顺便来 波 近期 秋冬 彩妆 开箱 本期 看点 冷暖 挑战 五彩斑斓 蓝 恶魔 三部曲 浓密 野生 毛发 感 海鸥 腮 黄 楚楚可怜 冻 伤感 圣 全文', '长沙 终于 进入 冬日 频道 降温 抵挡不住 探店 热爱 听说 IFS 新开 一家 爱 包围 快 闪店 来到 感受 满满的 爱意 有趣 主题 不能 错过 商场 发现 三诺 二十周年 特别 打造 天生 爱 漫游 局 快 闪店 三诺 Sinocare 整个 粉红色 系 布置 太 戳 简直 全文', '几天 使用 频率 超高 单品 不得不 说 一个 现在 原因 ps 猜猜 眼影 颜色 腮红 会 买 双十 一超 省钱 攻略 双十 一种 草 大会', '韩系 明星 画报 风 半扎 高 马尾 真的 贼 显颅 顶 高 姐妹 一定 试试 发现 耳饰 项链 加 持有 真的 会 整体 造型 分 SHOW 出 时髦 松弛 感 松弛 感 气质 妆容', '蟹 阿文 AWEN 粉丝 互动 榜 福利 恭喜 TOP TOP 宝贝 陆续 安排 老规矩 这条点 赞里 揪 个宝 奶茶 最后 一个月 一起 加油 抽奖 详情']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "\n",
    "# 加载停用词\n",
    "def load_stopwords():\n",
    "    url = \"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\"\n",
    "    return pd.read_csv(url, header=None, sep=\"\\t\", quoting=3, names=['stopword'], encoding='utf-8').stopword.tolist()\n",
    "\n",
    "# 获取停用词列表\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# 检查并删除df[0]中的重复数据\n",
    "df_total = df_total.drop_duplicates()\n",
    "\n",
    "\n",
    "# 假设的博主名字列表，如果你没有这个列表，可以将其定义为空列表\n",
    "combined_blogger_names = []\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 使用正则表达式去除“XX的微博视频”\n",
    "    text = re.sub(r'[^#]*的微博视频', '', text)\n",
    "    \n",
    "    # 去除特定无用词\n",
    "    useless_terms = [\"显示地图\", \"原图\", \"[超话]\", \"超话\"]\n",
    "    for term in useless_terms:\n",
    "        text = text.replace(term, '')\n",
    "    \n",
    "    # 去除自我指代\n",
    "    for name in combined_blogger_names:\n",
    "        text = text.replace(name, '')\n",
    "    \n",
    "    # 使用jieba进行分词\n",
    "    words = jieba.cut(text, cut_all=False)\n",
    "    \n",
    "    # 去除标点和特殊字符\n",
    "    words = [re.sub(r'[^a-zA-Z\\u4e00-\\u9fff]', '', word) for word in words if len(word) > 0]\n",
    "    \n",
    "    # 去除“组图”、“共”、“张”\n",
    "    words = [word for word in words if word not in [\"组图\", \"共\", \"张\"]]\n",
    "    \n",
    "    # 去除停用词\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    # 将连续的多个空格替换为一个空格\n",
    "    result = ' '.join(words)\n",
    "    result = re.sub(r'\\s+', ' ', result).strip()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 如果'content_cutted'存在于df_total中，则删除它\n",
    "if 'content_cutted' in df_total:\n",
    "    df_total = df_total.drop('content_cutted')\n",
    "\n",
    "# 然后应用预处理函数\n",
    "texts_cut = [preprocess_text(str(text)) for text in df_total]\n",
    "print(texts_cut[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ff5b2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  warnings.warn(\n",
      "INFO:lda:n_documents: 16804\n",
      "INFO:lda:vocab_size: 1000\n",
      "INFO:lda:n_words: 311161\n",
      "INFO:lda:n_topics: 15\n",
      "INFO:lda:n_iter: 1000\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -3197514\n",
      "INFO:lda:<10> log likelihood: -2285453\n",
      "INFO:lda:<20> log likelihood: -2151042\n",
      "INFO:lda:<30> log likelihood: -2111743\n",
      "INFO:lda:<40> log likelihood: -2091868\n",
      "INFO:lda:<50> log likelihood: -2080839\n",
      "INFO:lda:<60> log likelihood: -2076074\n",
      "INFO:lda:<70> log likelihood: -2072956\n",
      "INFO:lda:<80> log likelihood: -2069531\n",
      "INFO:lda:<90> log likelihood: -2066949\n",
      "INFO:lda:<100> log likelihood: -2065738\n",
      "INFO:lda:<110> log likelihood: -2064076\n",
      "INFO:lda:<120> log likelihood: -2063414\n",
      "INFO:lda:<130> log likelihood: -2063040\n",
      "INFO:lda:<140> log likelihood: -2062029\n",
      "INFO:lda:<150> log likelihood: -2060671\n",
      "INFO:lda:<160> log likelihood: -2060770\n",
      "INFO:lda:<170> log likelihood: -2059651\n",
      "INFO:lda:<180> log likelihood: -2059701\n",
      "INFO:lda:<190> log likelihood: -2059453\n",
      "INFO:lda:<200> log likelihood: -2057796\n",
      "INFO:lda:<210> log likelihood: -2058581\n",
      "INFO:lda:<220> log likelihood: -2058135\n",
      "INFO:lda:<230> log likelihood: -2058114\n",
      "INFO:lda:<240> log likelihood: -2057915\n",
      "INFO:lda:<250> log likelihood: -2057328\n",
      "INFO:lda:<260> log likelihood: -2056717\n",
      "INFO:lda:<270> log likelihood: -2056099\n",
      "INFO:lda:<280> log likelihood: -2055909\n",
      "INFO:lda:<290> log likelihood: -2055729\n",
      "INFO:lda:<300> log likelihood: -2056139\n",
      "INFO:lda:<310> log likelihood: -2054741\n",
      "INFO:lda:<320> log likelihood: -2055056\n",
      "INFO:lda:<330> log likelihood: -2055414\n",
      "INFO:lda:<340> log likelihood: -2053561\n",
      "INFO:lda:<350> log likelihood: -2053698\n",
      "INFO:lda:<360> log likelihood: -2053669\n",
      "INFO:lda:<370> log likelihood: -2053725\n",
      "INFO:lda:<380> log likelihood: -2052821\n",
      "INFO:lda:<390> log likelihood: -2052105\n",
      "INFO:lda:<400> log likelihood: -2053203\n",
      "INFO:lda:<410> log likelihood: -2051946\n",
      "INFO:lda:<420> log likelihood: -2050768\n",
      "INFO:lda:<430> log likelihood: -2051220\n",
      "INFO:lda:<440> log likelihood: -2052311\n",
      "INFO:lda:<450> log likelihood: -2051193\n",
      "INFO:lda:<460> log likelihood: -2050598\n",
      "INFO:lda:<470> log likelihood: -2051380\n",
      "INFO:lda:<480> log likelihood: -2050535\n",
      "INFO:lda:<490> log likelihood: -2051333\n",
      "INFO:lda:<500> log likelihood: -2050325\n",
      "INFO:lda:<510> log likelihood: -2051341\n",
      "INFO:lda:<520> log likelihood: -2049265\n",
      "INFO:lda:<530> log likelihood: -2050485\n",
      "INFO:lda:<540> log likelihood: -2050554\n",
      "INFO:lda:<550> log likelihood: -2050148\n",
      "INFO:lda:<560> log likelihood: -2049505\n",
      "INFO:lda:<570> log likelihood: -2049215\n",
      "INFO:lda:<580> log likelihood: -2050555\n",
      "INFO:lda:<590> log likelihood: -2049238\n",
      "INFO:lda:<600> log likelihood: -2049407\n",
      "INFO:lda:<610> log likelihood: -2048636\n",
      "INFO:lda:<620> log likelihood: -2049094\n",
      "INFO:lda:<630> log likelihood: -2049424\n",
      "INFO:lda:<640> log likelihood: -2049217\n",
      "INFO:lda:<650> log likelihood: -2048825\n",
      "INFO:lda:<660> log likelihood: -2048439\n",
      "INFO:lda:<670> log likelihood: -2049067\n",
      "INFO:lda:<680> log likelihood: -2048917\n",
      "INFO:lda:<690> log likelihood: -2049760\n",
      "INFO:lda:<700> log likelihood: -2049287\n",
      "INFO:lda:<710> log likelihood: -2049341\n",
      "INFO:lda:<720> log likelihood: -2050044\n",
      "INFO:lda:<730> log likelihood: -2048487\n",
      "INFO:lda:<740> log likelihood: -2049207\n",
      "INFO:lda:<750> log likelihood: -2048708\n",
      "INFO:lda:<760> log likelihood: -2048447\n",
      "INFO:lda:<770> log likelihood: -2048253\n",
      "INFO:lda:<780> log likelihood: -2048207\n",
      "INFO:lda:<790> log likelihood: -2048154\n",
      "INFO:lda:<800> log likelihood: -2048275\n",
      "INFO:lda:<810> log likelihood: -2048235\n",
      "INFO:lda:<820> log likelihood: -2047047\n",
      "INFO:lda:<830> log likelihood: -2047358\n",
      "INFO:lda:<840> log likelihood: -2047542\n",
      "INFO:lda:<850> log likelihood: -2047643\n",
      "INFO:lda:<860> log likelihood: -2047985\n",
      "INFO:lda:<870> log likelihood: -2048062\n",
      "INFO:lda:<880> log likelihood: -2047221\n",
      "INFO:lda:<890> log likelihood: -2047610\n",
      "INFO:lda:<900> log likelihood: -2048142\n",
      "INFO:lda:<910> log likelihood: -2048132\n",
      "INFO:lda:<920> log likelihood: -2047855\n",
      "INFO:lda:<930> log likelihood: -2048084\n",
      "INFO:lda:<940> log likelihood: -2047483\n",
      "INFO:lda:<950> log likelihood: -2047754\n",
      "INFO:lda:<960> log likelihood: -2047649\n",
      "INFO:lda:<970> log likelihood: -2047611\n",
      "INFO:lda:<980> log likelihood: -2048200\n",
      "INFO:lda:<990> log likelihood: -2048308\n",
      "INFO:lda:<999> log likelihood: -2048125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model done\n",
      "Topic #0:\n",
      "防晒 粉底液 底妆 质地 不会 定妆 非常 真的 油皮 粉底\n",
      "Topic #1:\n",
      "真的 感觉 一个 喜欢 觉得 姐妹 没有 不会 最近 现在\n",
      "Topic #2:\n",
      "抽奖 详情 化妆 视频 开学 变美 今天 姐妹 一个 豆豆\n",
      "Topic #3:\n",
      "头发 身体 头皮 护发 真的 护理 精油 欧莱雅 发质 洗发水\n",
      "Topic #4:\n",
      "生活 一个 一起 体验 时尚 免税 工作 全文 感受 真的\n",
      "Topic #5:\n",
      "春夏 出游 春天 look 春季 双十 攻略 出街 春日 氛围\n",
      "Topic #6:\n",
      "精华 皮肤 肌肤 修护 抗老 成分 护肤 面霜 真的 效果\n",
      "Topic #7:\n",
      "京东 链接 网页 天猫 一起 超级 活动 ml 福利 大牌\n",
      "Topic #8:\n",
      "妆容 今日 口红 眼影 美妆 腮红 真的 氛围 颜色 今天\n",
      "Topic #9:\n",
      "分享 好物 种草 美妆 推荐 彩妆 开箱 购物 最近 爱用物\n",
      "Topic #10:\n",
      "新年 计划 完美 攻略 开春 好颜 虎年 过年 微博 新年好\n",
      "Topic #11:\n",
      "护肤 皮肤 面膜 分享 精华 换季 敏感 清洁 卸妆 好物\n",
      "Topic #12:\n",
      "互动 感谢 品牌 快乐 微博 评论 宝贝 礼盒 礼物 粉丝\n",
      "Topic #13:\n",
      "生活 plog vlog 夏日 日常 快乐 日记 记录 碎片 ootd\n",
      "Topic #14:\n",
      "香水 心动 礼物 礼盒 搭配 约会 七夕 浪漫 系列 玫瑰\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (16804) does not match length of index (2235)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-585d49412210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# 6. 为每个文档分配主题\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdoc_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_topic_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_topic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# 7. 计算困惑度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3948\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3949\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3950\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3952\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4141\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4142\u001b[0m         \"\"\"\n\u001b[0;32m-> 4143\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4145\u001b[0m         if (\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4870\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4871\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \"\"\"\n\u001b[1;32m    575\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (16804) does not match length of index (2235)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# 3. 文本向量化\n",
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, stop_words=stopwords, max_df=0.5, min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(texts_cut)\n",
    "\n",
    "# 4. LDA模型训练\n",
    "n_topics = 15\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=1000, random_state=2)\n",
    "model.fit(tf)\n",
    "print('Model done')\n",
    "\n",
    "# 5. 打印每个主题的关键词\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names_out()\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# 6. 为每个文档分配主题\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = [topics.argmax() for topics in doc_topic]\n",
    "\n",
    "# 7. 计算困惑度\n",
    "log_likelihood = model.loglikelihood()\n",
    "n_total_words = tf.sum()\n",
    "perplexity = np.exp(-log_likelihood / n_total_words)\n",
    "print(\"Model Perplexity: \", perplexity)\n",
    "\n",
    "# # 8. 保存结果\n",
    "# df.to_excel('/Users/laihuiqian/Documents/weibo_0925/total2-0925_with_topics.xlsx', index=False)\n",
    "# print('Results saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f533cd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "INFO:lda:n_documents: 16804\n",
      "INFO:lda:vocab_size: 1000\n",
      "INFO:lda:n_words: 311161\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 1000\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -3004567\n",
      "INFO:lda:<10> log likelihood: -2253569\n",
      "INFO:lda:<20> log likelihood: -2149800\n",
      "INFO:lda:<30> log likelihood: -2117337\n",
      "INFO:lda:<40> log likelihood: -2098600\n",
      "INFO:lda:<50> log likelihood: -2087543\n",
      "INFO:lda:<60> log likelihood: -2079759\n",
      "INFO:lda:<70> log likelihood: -2076547\n",
      "INFO:lda:<80> log likelihood: -2075241\n",
      "INFO:lda:<90> log likelihood: -2071607\n",
      "INFO:lda:<100> log likelihood: -2068321\n",
      "INFO:lda:<110> log likelihood: -2066359\n",
      "INFO:lda:<120> log likelihood: -2065054\n",
      "INFO:lda:<130> log likelihood: -2062546\n",
      "INFO:lda:<140> log likelihood: -2062728\n",
      "INFO:lda:<150> log likelihood: -2060808\n",
      "INFO:lda:<160> log likelihood: -2060296\n",
      "INFO:lda:<170> log likelihood: -2060132\n",
      "INFO:lda:<180> log likelihood: -2058790\n",
      "INFO:lda:<190> log likelihood: -2059283\n",
      "INFO:lda:<200> log likelihood: -2058643\n",
      "INFO:lda:<210> log likelihood: -2057381\n",
      "INFO:lda:<220> log likelihood: -2057211\n",
      "INFO:lda:<230> log likelihood: -2056829\n",
      "INFO:lda:<240> log likelihood: -2055782\n",
      "INFO:lda:<250> log likelihood: -2055682\n",
      "INFO:lda:<260> log likelihood: -2054784\n",
      "INFO:lda:<270> log likelihood: -2054524\n",
      "INFO:lda:<280> log likelihood: -2054458\n",
      "INFO:lda:<290> log likelihood: -2053410\n",
      "INFO:lda:<300> log likelihood: -2053598\n",
      "INFO:lda:<310> log likelihood: -2052374\n",
      "INFO:lda:<320> log likelihood: -2052638\n",
      "INFO:lda:<330> log likelihood: -2051373\n",
      "INFO:lda:<340> log likelihood: -2052627\n",
      "INFO:lda:<350> log likelihood: -2050863\n",
      "INFO:lda:<360> log likelihood: -2051776\n",
      "INFO:lda:<370> log likelihood: -2051051\n",
      "INFO:lda:<380> log likelihood: -2050443\n",
      "INFO:lda:<390> log likelihood: -2050985\n",
      "INFO:lda:<400> log likelihood: -2050664\n",
      "INFO:lda:<410> log likelihood: -2049813\n",
      "INFO:lda:<420> log likelihood: -2050173\n",
      "INFO:lda:<430> log likelihood: -2050459\n",
      "INFO:lda:<440> log likelihood: -2051654\n",
      "INFO:lda:<450> log likelihood: -2049373\n",
      "INFO:lda:<460> log likelihood: -2050836\n",
      "INFO:lda:<470> log likelihood: -2050675\n",
      "INFO:lda:<480> log likelihood: -2049277\n",
      "INFO:lda:<490> log likelihood: -2050342\n",
      "INFO:lda:<500> log likelihood: -2049731\n",
      "INFO:lda:<510> log likelihood: -2050300\n",
      "INFO:lda:<520> log likelihood: -2049017\n",
      "INFO:lda:<530> log likelihood: -2049069\n",
      "INFO:lda:<540> log likelihood: -2048466\n",
      "INFO:lda:<550> log likelihood: -2048991\n",
      "INFO:lda:<560> log likelihood: -2048107\n",
      "INFO:lda:<570> log likelihood: -2048160\n",
      "INFO:lda:<580> log likelihood: -2049146\n",
      "INFO:lda:<590> log likelihood: -2049190\n",
      "INFO:lda:<600> log likelihood: -2047811\n",
      "INFO:lda:<610> log likelihood: -2048367\n",
      "INFO:lda:<620> log likelihood: -2048069\n",
      "INFO:lda:<630> log likelihood: -2047737\n",
      "INFO:lda:<640> log likelihood: -2048277\n",
      "INFO:lda:<650> log likelihood: -2048554\n",
      "INFO:lda:<660> log likelihood: -2046868\n",
      "INFO:lda:<670> log likelihood: -2047746\n",
      "INFO:lda:<680> log likelihood: -2046939\n",
      "INFO:lda:<690> log likelihood: -2049206\n",
      "INFO:lda:<700> log likelihood: -2048700\n",
      "INFO:lda:<710> log likelihood: -2048270\n",
      "INFO:lda:<720> log likelihood: -2047732\n",
      "INFO:lda:<730> log likelihood: -2048276\n",
      "INFO:lda:<740> log likelihood: -2048464\n",
      "INFO:lda:<750> log likelihood: -2050164\n",
      "INFO:lda:<760> log likelihood: -2048210\n",
      "INFO:lda:<770> log likelihood: -2049147\n",
      "INFO:lda:<780> log likelihood: -2048493\n",
      "INFO:lda:<790> log likelihood: -2048190\n",
      "INFO:lda:<800> log likelihood: -2046707\n",
      "INFO:lda:<810> log likelihood: -2048307\n",
      "INFO:lda:<820> log likelihood: -2047952\n",
      "INFO:lda:<830> log likelihood: -2048243\n",
      "INFO:lda:<840> log likelihood: -2048354\n",
      "INFO:lda:<850> log likelihood: -2047136\n",
      "INFO:lda:<860> log likelihood: -2048787\n",
      "INFO:lda:<870> log likelihood: -2047769\n",
      "INFO:lda:<880> log likelihood: -2046690\n",
      "INFO:lda:<890> log likelihood: -2047203\n",
      "INFO:lda:<900> log likelihood: -2048288\n",
      "INFO:lda:<910> log likelihood: -2047878\n",
      "INFO:lda:<920> log likelihood: -2047404\n",
      "INFO:lda:<930> log likelihood: -2048159\n",
      "INFO:lda:<940> log likelihood: -2047672\n",
      "INFO:lda:<950> log likelihood: -2048102\n",
      "INFO:lda:<960> log likelihood: -2047173\n",
      "INFO:lda:<970> log likelihood: -2046534\n",
      "INFO:lda:<980> log likelihood: -2047228\n",
      "INFO:lda:<990> log likelihood: -2047884\n",
      "INFO:lda:<999> log likelihood: -2047033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model done\n",
      "Topic #0:\n",
      "妆容 真的 口红 底妆 粉底液 眼影 腮红 化妆 适合 颜色\n",
      "Topic #1:\n",
      "防晒 真的 感觉 喜欢 香水 觉得 味道 没有 一个 不会\n",
      "Topic #2:\n",
      "互动 微博 礼盒 礼物 快乐 感谢 品牌 评论 宝贝 开箱\n",
      "Topic #3:\n",
      "京东 链接 网页 天猫 一起 礼盒 超级 生活 活动 礼物\n",
      "Topic #4:\n",
      "生活 plog vlog 日常 快乐 日记 记录 今天 碎片 博主\n",
      "Topic #5:\n",
      "头发 真的 身体 头皮 护发 护理 精油 欧莱雅 姐妹 发质\n",
      "Topic #6:\n",
      "夏日 妆容 ootd look 今日 氛围 今天 春天 春夏 出游\n",
      "Topic #7:\n",
      "分享 好物 护肤 美妆 种草 今天 推荐 最近 视频 彩妆\n",
      "Topic #8:\n",
      "皮肤 精华 肌肤 护肤 修护 抗老 成分 真的 面霜 效果\n",
      "Topic #9:\n",
      "新年 计划 完美 抽奖 详情 攻略 双十 开春 好颜 虎年\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (16804) does not match length of index (2235)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-2cd264c65155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# 6. 为每个文档分配主题\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdoc_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_topic_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_topic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# 7. 计算困惑度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3948\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3949\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3950\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3952\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4141\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4142\u001b[0m         \"\"\"\n\u001b[0;32m-> 4143\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4145\u001b[0m         if (\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4870\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4871\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \"\"\"\n\u001b[1;32m    575\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (16804) does not match length of index (2235)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# 3. 文本向量化\n",
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, stop_words=stopwords, max_df=0.5, min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(texts_cut)\n",
    "\n",
    "# 4. LDA模型训练\n",
    "n_topics = 10\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=1000, random_state=2)\n",
    "model.fit(tf)\n",
    "print('Model done')\n",
    "\n",
    "# 5. 打印每个主题的关键词\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names_out()\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# 6. 为每个文档分配主题\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = [topics.argmax() for topics in doc_topic]\n",
    "\n",
    "# 7. 计算困惑度\n",
    "log_likelihood = model.loglikelihood()\n",
    "n_total_words = tf.sum()\n",
    "perplexity = np.exp(-log_likelihood / n_total_words)\n",
    "print(\"Model Perplexity: \", perplexity)\n",
    "\n",
    "# # 8. 保存结果\n",
    "# df.to_excel('/Users/laihuiqian/Documents/weibo_0925/total2-0925_with_topics.xlsx', index=False)\n",
    "# print('Results saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f035da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
