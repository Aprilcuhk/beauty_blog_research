{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d1327e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        【2022·年度爱用物🏆彩妆篇】超长25min完整版！20大品类！100+件爱用精选！一路常...\n",
      "1        《云之羽》原班造型团队🎬      🌟#虞书欣云为衫妆造还原#㊙️虞书欣本剧御用化妆师@侍慧...\n",
      "2        谁懂！！😭欣欣把云为衫的剧组衣服借我穿了！！！#虞书欣云为衫妆造还原##微博变美手册# #仿...\n",
      "3            回家啦！饭后散步中~特别舒服的初秋上海！💨🍂#一枝南南[超话]#  [组图共6张] 原图 \n",
      "4        9月第一天！！水逆退散！！来接好运🍀🎁————固定栏目「pr礼物」上线～评论区许愿池开放✌🏻...\n",
      "                               ...                        \n",
      "13000    𝙉𝙞𝙘𝙚 𝙩𝙤 𝘾 𝙮𝙤𝙪现场做实验的一天🔅#美妆生活##motd##好物分享#  [组图共...\n",
      "13001    𝙄𝙉𝙏𝙊𝙉𝙀 𝙀𝘿𝙄𝙏𝙄𝙊𝙉_心慕与你色彩编辑部实习编辑上线💗#2023的她们##动静皆风尚...\n",
      "13002    最近天气太好啦～到处都是春日的信号🌿☁️#2023的她们##美妆生活##动静皆风尚#  [组...\n",
      "13003    啾咪！ʙʟᴜᴇ ʙʟᴜᴇ皮闪现💙今天打卡了科颜氏「超CHILL」保湿街区！超多的沉浸式体验场...\n",
      "13004    《关于相册里的囤图这件事》✎ 迟到的➌月ᴘʟᴏɢ请查收 ✓#2023的她们##日常碎片plo...\n",
      "Name: 微博正文, Length: 13005, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 指定文件路径\n",
    "file_path = '/Users/laihuiqian/Documents/weibo_0925/total1-0925.csv'\n",
    "\n",
    "# 读取.csv文件到DataFrame\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# 提取“微博正文”这一列\n",
    "weibo_content = df[\"微博正文\"]\n",
    "\n",
    "# 打印“微博正文”\n",
    "print(weibo_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "430511ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "INFO:lda:n_documents: 13005\n",
      "INFO:lda:vocab_size: 5000\n",
      "INFO:lda:n_words: 298022\n",
      "INFO:lda:n_topics: 17\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -3646656\n",
      "INFO:lda:<10> log likelihood: -2656670\n",
      "INFO:lda:<20> log likelihood: -2514413\n",
      "INFO:lda:<30> log likelihood: -2461563\n",
      "INFO:lda:<40> log likelihood: -2437192\n",
      "INFO:lda:<50> log likelihood: -2424579\n",
      "INFO:lda:<60> log likelihood: -2415868\n",
      "INFO:lda:<70> log likelihood: -2408600\n",
      "INFO:lda:<80> log likelihood: -2403769\n",
      "INFO:lda:<90> log likelihood: -2398470\n",
      "INFO:lda:<99> log likelihood: -2397472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model done\n",
      "shape: (17, 5000)\n",
      "['00' '01' '02']\n",
      "[[4.61169526e-07 4.61169526e-07 4.61169526e-07]\n",
      " [9.30578820e-07 9.30578820e-07 9.30578820e-07]\n",
      " [6.23519142e-07 6.23519142e-07 6.23519142e-07]\n",
      " [5.79619626e-03 5.63245555e-04 5.62682872e-07]\n",
      " [5.73789305e-07 5.74363094e-04 1.03339454e-03]\n",
      " [8.65276456e-07 8.65276456e-07 8.65276456e-07]\n",
      " [5.98909984e-07 5.98909984e-07 5.98909984e-07]\n",
      " [4.25387102e-07 4.25387102e-07 4.25387102e-07]\n",
      " [1.05418511e-06 1.05418511e-06 1.05418511e-06]\n",
      " [5.16368894e-07 5.16368894e-07 5.16368894e-07]\n",
      " [6.83854202e-07 6.83854202e-07 3.42610955e-04]\n",
      " [5.21975154e-07 5.21975154e-07 5.21975154e-07]\n",
      " [3.20728696e-07 3.20728696e-07 3.20728696e-07]\n",
      " [4.31685733e-07 4.31685733e-07 4.31685733e-07]\n",
      " [6.36294222e-07 6.36294222e-07 6.36294222e-07]\n",
      " [6.03427468e-07 6.03427468e-07 6.03427468e-07]\n",
      " [1.05875714e-03 7.05367849e-07 7.05367849e-07]]\n",
      "topic: 0 sum: 0.9999999999999561\n",
      "topic: 1 sum: 0.9999999999999183\n",
      "topic: 2 sum: 0.9999999999999251\n",
      "topic: 3 sum: 1.0000000000000449\n",
      "topic: 4 sum: 0.9999999999998892\n",
      "topic: 5 sum: 0.9999999999999654\n",
      "topic: 6 sum: 0.9999999999999983\n",
      "topic: 7 sum: 0.999999999999892\n",
      "topic: 8 sum: 0.9999999999999358\n",
      "topic: 9 sum: 1.0000000000000822\n",
      "topic: 10 sum: 0.9999999999999512\n",
      "topic: 11 sum: 1.0000000000000262\n",
      "*Topic 0\n",
      "- 妆容 今日 美妆 look ootd 氛围 今天 真的 春季 出街 春天 出游 春夏 眼妆 这个 适合 夏日 春日 就是 化妆\n",
      "*Topic 1\n",
      "- 互动 大家 粉丝 可以 评论 私信 宝贝 你们 日常 多多 福利 美少女 记得 抽奖 加入 一起 继续 上榜 随机 恭喜\n",
      "*Topic 2\n",
      "- 香水 味道 玫瑰 香气 温柔 清新 高级 浪漫 感觉 气息 适合 花香 木质 一款 氛围 香氛 系列 香味 喜欢 美好\n",
      "*Topic 3\n",
      "- 链接 地图 显示 一起 还有 京东 11 超级 活动 天猫 福利 巴黎 上海 搜索 品牌 超多 20 直播间 10 这次\n",
      "*Topic 4\n",
      "- 口红 完美 攻略 开春 分享 好颜 计划 新年 妆容 底妆 粉底液 眼影 美妆 好物 高级 适合 颜色 今天 腮红 日常\n",
      "*Topic 5\n",
      "- 分享 好物 美妆 品牌 感谢 开箱 礼盒 护肤 快乐 收到 最近 各位 motd 礼物 精华 一起 欧莱雅 阿文 彩妆 makeup\n",
      "*Topic 6\n",
      "- 真的 感觉 姐妹 可以 就是 最近 头发 这个 起来 夏天 一定 你们 简直 还是 精致 一样 时候 今天 喜欢 非常\n",
      "*Topic 7\n",
      "- 自己 真的 我们 没有 这个 可以 很多 就是 觉得 什么 知道 还是 你们 现在 但是 希望 其实 大家 不要 非常\n",
      "*Topic 8\n",
      "- 娱乐 迷妹 真的 美妆 美出 哈哈哈 演唱会 这是 这个 哈哈哈哈 姐姐 张杰 听说 怎么 喜欢 这么 明星 赵丽颖 暑期 变美逆袭\n",
      "*Topic 9\n",
      "- 分享 好物 护肤 今天 大家 种草 大会 变美 攻略 姐妹 推荐 你们 双十 美妆 一种 最近 怎么 自己 如何 看看\n",
      "*Topic 10\n",
      "- 新年 计划 大家 完美 礼物 快乐 礼盒 白雪 过年 心动 今日 2022 情人节 一年 开箱 限定 约会 虎年 收到 今天\n",
      "*Topic 11\n",
      "- 生活 日常 plog vlog ootd 日记 快乐 今天 最近 碎片 记录 真的 博主 你们 周末 什么 一天 今日 分享 一些\n",
      "*Topic 12\n",
      "- 护肤 皮肤 精华 肌肤 修护 抗老 真的 保湿 质地 可以 敏感 效果 面霜 熬夜 成分 吸收 换季 分享 状态 紧致\n",
      "*Topic 13\n",
      "- 而且 我们 同时 状态 现在 所以 就是 作为 里面 健康 除了 链接 一直 还是 知道 想要 保持 产品 没有 加上\n",
      "*Topic 14\n",
      "- 这次 真的 体验 可以 我们 还是 还有 大家 不仅 看到 一起 免税 这个 整个 自己 直接 就是 这么 很多 满满\n",
      "*Topic 15\n",
      "- 没有 不是 就是 知道 每日 什么 可以 一点 一香 还是 因为 这个 女人 那些 一瓶 世界 如果 不会 当然 这样\n",
      "*Topic 16\n",
      "- 生活 体验 自己 一起 时尚 挑战 感受 夏日 自由 喜欢 造型 一场 美好 风格 定义 世界 可以 艺术 设计 浪漫\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "def open_dict(Dict):\n",
    "    path = '%s.txt' % Dict\n",
    "    dictionary = open(path, 'r', encoding='utf-8')\n",
    "    dict = []\n",
    "    for word in dictionary:\n",
    "        word = word.strip('\\n')\n",
    "        dict.append(word)\n",
    "    return dict\n",
    "\n",
    "clearwords=open_dict('clearwords')\n",
    "\n",
    "#分词\n",
    "def chinese_word_cut(mytext):\n",
    "    tempcut=jieba.cut(str(mytext))\n",
    "    return \" \".join(set(tempcut)-set(clearwords))\n",
    "\n",
    "#打印前n_top_words关键词\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "# df = pd.read_excel(\"metoo20181221.xlsx\",'Sheet1',index_col=None,na_values=['NA'])\n",
    "# df.shape\n",
    "df[\"content_cutted\"] = df[\"微博正文\"].apply(chinese_word_cut)\n",
    "n_features = 5000\n",
    "\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "tf = tf_vectorizer.fit_transform(df.content_cutted)\n",
    "vocab=tf_vectorizer.get_feature_names_out()\n",
    "model = lda.LDA(n_topics=17, n_iter=100, random_state=1)  \n",
    "model.fit(tf)\n",
    "print('model done')\n",
    "\n",
    "#主题-单词（topic-word）分布\n",
    "topic_word = model.topic_word_ \n",
    "print(\"shape: {}\".format(topic_word.shape))\n",
    "print(vocab[:3])\n",
    "print(topic_word[:, :3])\n",
    "for n in range(12):\n",
    "    sum_pr = sum(topic_word[n,:])  \n",
    "    print(\"topic: {} sum: {}\".format(n, sum_pr))\n",
    "\n",
    "#计算各主题Top-N个单词\n",
    "import numpy as np\n",
    "n = 20\n",
    "for i, topic_dist in enumerate(topic_word):  \n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]  \n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3a92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
