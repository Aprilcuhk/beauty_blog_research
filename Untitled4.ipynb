{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2006d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        【2022·年度爱用物🏆彩妆篇】超长25min完整版！20大品类！100+件爱用精选！一路常...\n",
      "1        《云之羽》原班造型团队🎬      🌟#虞书欣云为衫妆造还原#㊙️虞书欣本剧御用化妆师@侍慧...\n",
      "2        谁懂！！😭欣欣把云为衫的剧组衣服借我穿了！！！#虞书欣云为衫妆造还原##微博变美手册# #仿...\n",
      "3            回家啦！饭后散步中~特别舒服的初秋上海！💨🍂#一枝南南[超话]#  [组图共6张] 原图 \n",
      "4        9月第一天！！水逆退散！！来接好运🍀🎁————固定栏目「pr礼物」上线～评论区许愿池开放✌🏻...\n",
      "                               ...                        \n",
      "13000    𝙉𝙞𝙘𝙚 𝙩𝙤 𝘾 𝙮𝙤𝙪现场做实验的一天🔅#美妆生活##motd##好物分享#  [组图共...\n",
      "13001    𝙄𝙉𝙏𝙊𝙉𝙀 𝙀𝘿𝙄𝙏𝙄𝙊𝙉_心慕与你色彩编辑部实习编辑上线💗#2023的她们##动静皆风尚...\n",
      "13002    最近天气太好啦～到处都是春日的信号🌿☁️#2023的她们##美妆生活##动静皆风尚#  [组...\n",
      "13003    啾咪！ʙʟᴜᴇ ʙʟᴜᴇ皮闪现💙今天打卡了科颜氏「超CHILL」保湿街区！超多的沉浸式体验场...\n",
      "13004    《关于相册里的囤图这件事》✎ 迟到的➌月ᴘʟᴏɢ请查收 ✓#2023的她们##日常碎片plo...\n",
      "Name: 微博正文, Length: 13005, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 指定文件路径\n",
    "file_path = '/Users/laihuiqian/Documents/weibo_0925/total1-0925.csv'\n",
    "\n",
    "# 读取.csv文件到DataFrame\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# 提取“微博正文”这一列\n",
    "weibo_content = df[\"微博正文\"]\n",
    "\n",
    "# 打印“微博正文”\n",
    "print(weibo_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "026024ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "INFO:lda:n_documents: 13005\n",
      "INFO:lda:vocab_size: 5000\n",
      "INFO:lda:n_words: 316403\n",
      "INFO:lda:n_topics: 17\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -3833775\n",
      "INFO:lda:<10> log likelihood: -2799798\n",
      "INFO:lda:<20> log likelihood: -2643432\n",
      "INFO:lda:<30> log likelihood: -2592128\n",
      "INFO:lda:<40> log likelihood: -2566627\n",
      "INFO:lda:<50> log likelihood: -2551125\n",
      "INFO:lda:<60> log likelihood: -2540266\n",
      "INFO:lda:<70> log likelihood: -2532301\n",
      "INFO:lda:<80> log likelihood: -2525819\n",
      "INFO:lda:<90> log likelihood: -2520767\n",
      "INFO:lda:<99> log likelihood: -2518051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model done\n",
      "shape: (17, 5000)\n",
      "['00' '01' '02']\n",
      "[[4.48752468e-07 5.38951714e-04 9.87704182e-04]\n",
      " [3.78114720e-07 3.78114720e-07 3.78114720e-07]\n",
      " [5.29969792e-07 5.29969792e-07 5.35269490e-05]\n",
      " [8.35212562e-07 8.35212562e-07 8.35212562e-07]\n",
      " [4.75669505e-07 4.75669505e-07 4.75669505e-07]\n",
      " [4.77805915e-07 4.77805915e-07 4.77805915e-07]\n",
      " [6.86294695e-07 6.86294695e-07 6.86294695e-07]\n",
      " [3.24091993e-03 3.60502273e-04 4.50065259e-07]\n",
      " [6.06980273e-07 6.06980273e-07 6.06980273e-07]\n",
      " [7.44324525e-07 7.44324525e-07 7.44324525e-07]\n",
      " [4.38865970e-07 4.38865970e-07 4.38865970e-07]\n",
      " [4.73666932e-05 4.68977161e-07 4.68977161e-07]\n",
      " [7.97575371e-07 7.97575371e-07 7.97575371e-07]\n",
      " [8.49978751e-07 8.49978751e-07 8.49978751e-07]\n",
      " [3.08346952e-07 3.08346952e-07 3.08346952e-07]\n",
      " [6.33151830e-07 6.33151830e-07 6.33151830e-07]\n",
      " [3.63247518e-03 8.07037366e-07 8.07037366e-07]]\n",
      "topic: 0 sum: 1.0000000000000697\n",
      "topic: 1 sum: 0.9999999999999135\n",
      "topic: 2 sum: 0.9999999999999875\n",
      "topic: 3 sum: 0.9999999999999106\n",
      "topic: 4 sum: 0.9999999999999313\n",
      "topic: 5 sum: 1.0000000000000628\n",
      "topic: 6 sum: 1.000000000000059\n",
      "topic: 7 sum: 1.0000000000000773\n",
      "topic: 8 sum: 1.0000000000000568\n",
      "topic: 9 sum: 0.9999999999999494\n",
      "topic: 10 sum: 1.0000000000000324\n",
      "topic: 11 sum: 0.999999999999922\n",
      "*Topic 0\n",
      "- 妆容 美妆 视频 真的 超话 口红 眼妆 微博 今日 这个 眼影 氛围 适合 腮红 拿捏 颜色 温柔 重点 今天 高级\n",
      "*Topic 1\n",
      "- 真的 还是 就是 现在 没有 可以 觉得 一个 这个 一定 所以 大家 很多 知道 因为 但是 其实 不会 这种 什么\n",
      "*Topic 2\n",
      "- 视频 微博 ootd 分享 今天 look 春天 今日 春季 妆容 出街 超话 攻略 最近 出游 夏日 大会 春夏 真的 什么\n",
      "*Topic 3\n",
      "- 互动 大家 超话 粉丝 可以 评论 你们 日常 多多 私信 宝贝 开学 微博 福利 手册 一起 上榜 加入 恭喜 继续\n",
      "*Topic 4\n",
      "- 视频 微博 护肤 分享 姐妹 皮肤 问题 自己 今天 大家 最近 起来 护理 如何 真的 就是 好物 一个 有效 我们\n",
      "*Topic 5\n",
      "- 香水 味道 没有 一个 香气 玫瑰 每日 什么 清新 就是 温柔 可以 一香 不是 知道 花香 一瓶 喜欢 女人 一点\n",
      "*Topic 6\n",
      "- 新年 计划 完美 大家 微博 攻略 开春 快乐 开箱 好颜 视频 礼盒 分享 品牌 过年 礼物 收到 好物 一年 你们\n",
      "*Topic 7\n",
      "- 链接 网页 还有 一起 京东 这次 天猫 全文 11 福利 活动 姐妹 准备 超多 官方 搜索 可以 惊喜 hedy 直播间\n",
      "*Topic 8\n",
      "- 微博 生活 视频 plog 日常 vlog 日记 碎片 今天 最近 快乐 记录 张进 博主 你们 一些 什么 ootd 一条 一下\n",
      "*Topic 9\n",
      "- 时尚 微博 超级 好看 巴黎 设计 今日 系列 风格 现场 手边 urruolan 今天 时装周 搭配 一起 可以 上海 灵感 这次\n",
      "*Topic 10\n",
      "- 分享 好物 护肤 精华 真的 最近 推荐 彩妆 粉底液 美妆 底妆 面霜 宝藏 全文 防晒 面膜 种草 爱用物 爱用 秋冬\n",
      "*Topic 11\n",
      "- 自己 我们 一个 真的 希望 这个 看到 就是 可以 没有 一直 这么 知道 生活 什么 时候 不是 全文 一起 永远\n",
      "*Topic 12\n",
      "- 微博 视频 娱乐 迷妹 真的 白雪 今日 words 一番 演唱会 fane 哈哈哈 周记 key 下周 可爱 喜欢 反差 超话 张杰\n",
      "*Topic 13\n",
      "- 微博 视频 超话 美妆 抽奖 详情 分享 一枝 kiko 本期 感谢 评论 幼熙 品牌 变美 南南 官方 快乐 化妆 一起\n",
      "*Topic 14\n",
      "- 皮肤 肌肤 精华 修护 护肤 质地 效果 可以 抗老 保湿 状态 成分 敏感 吸收 使用 熬夜 里面 紧致 细腻 感觉\n",
      "*Topic 15\n",
      "- 真的 感觉 头发 夏日 可以 精致 体验 喜欢 系列 氛围 浪漫 挑战 造型 搭配 这个 就是 自己 护发 夏天 起来\n",
      "*Topic 16\n",
      "- 地图 显示 上海 旅行 ootd 成都 感受 一起 超话 浪漫 生活 城市 超级 12 一场 直播 马锐 一个 治愈 10\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "def open_dict(Dict):\n",
    "    path = '%s.txt' % Dict\n",
    "    dictionary = open(path, 'r', encoding='utf-8')\n",
    "    dict = []\n",
    "    for word in dictionary:\n",
    "        word = word.strip('\\n')\n",
    "        dict.append(word)\n",
    "    return dict\n",
    "\n",
    "# clearwords=open_dict('clearwords')\n",
    "\n",
    "#分词\n",
    "# def chinese_word_cut(mytext):\n",
    "#     tempcut=jieba.cut(str(mytext))\n",
    "#     return \" \".join(set(tempcut)-set(clearwords))\n",
    "def chinese_word_cut(mytext):\n",
    "    tempcut=jieba.cut(str(mytext))\n",
    "    return \" \".join(set(tempcut))\n",
    "\n",
    "#打印前n_top_words关键词\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "# df = pd.read_excel(\"metoo20181221.xlsx\",'Sheet1',index_col=None,na_values=['NA'])\n",
    "# df.shape\n",
    "df[\"content_cutted\"] = df[\"微博正文\"].apply(chinese_word_cut)\n",
    "n_features = 5000\n",
    "\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "tf = tf_vectorizer.fit_transform(df.content_cutted)\n",
    "vocab=tf_vectorizer.get_feature_names_out()\n",
    "model = lda.LDA(n_topics=17, n_iter=100, random_state=1)  \n",
    "model.fit(tf)\n",
    "print('model done')\n",
    "\n",
    "#主题-单词（topic-word）分布\n",
    "topic_word = model.topic_word_ \n",
    "print(\"shape: {}\".format(topic_word.shape))\n",
    "print(vocab[:3])\n",
    "print(topic_word[:, :3])\n",
    "for n in range(12):\n",
    "    sum_pr = sum(topic_word[n,:])  \n",
    "    print(\"topic: {} sum: {}\".format(n, sum_pr))\n",
    "\n",
    "#计算各主题Top-N个单词\n",
    "import numpy as np\n",
    "n = 20\n",
    "for i, topic_dist in enumerate(topic_word):  \n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]  \n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f09d461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
