{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2006d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        ã€2022Â·å¹´åº¦çˆ±ç”¨ç‰©ğŸ†å½©å¦†ç¯‡ã€‘è¶…é•¿25minå®Œæ•´ç‰ˆï¼20å¤§å“ç±»ï¼100+ä»¶çˆ±ç”¨ç²¾é€‰ï¼ä¸€è·¯å¸¸...\n",
      "1        ã€Šäº‘ä¹‹ç¾½ã€‹åŸç­é€ å‹å›¢é˜ŸğŸ¬      ğŸŒŸ#è™ä¹¦æ¬£äº‘ä¸ºè¡«å¦†é€ è¿˜åŸ#ãŠ™ï¸è™ä¹¦æ¬£æœ¬å‰§å¾¡ç”¨åŒ–å¦†å¸ˆ@ä¾æ…§...\n",
      "2        è°æ‡‚ï¼ï¼ğŸ˜­æ¬£æ¬£æŠŠäº‘ä¸ºè¡«çš„å‰§ç»„è¡£æœå€Ÿæˆ‘ç©¿äº†ï¼ï¼ï¼#è™ä¹¦æ¬£äº‘ä¸ºè¡«å¦†é€ è¿˜åŸ##å¾®åšå˜ç¾æ‰‹å†Œ# #ä»¿...\n",
      "3            å›å®¶å•¦ï¼é¥­åæ•£æ­¥ä¸­~ç‰¹åˆ«èˆ’æœçš„åˆç§‹ä¸Šæµ·ï¼ğŸ’¨ğŸ‚#ä¸€æå—å—[è¶…è¯]# Â [ç»„å›¾å…±6å¼ ]Â åŸå›¾Â \n",
      "4        9æœˆç¬¬ä¸€å¤©ï¼ï¼æ°´é€†é€€æ•£ï¼ï¼æ¥æ¥å¥½è¿ğŸ€ğŸâ€”â€”â€”â€”å›ºå®šæ ç›®ã€Œprç¤¼ç‰©ã€ä¸Šçº¿ï½è¯„è®ºåŒºè®¸æ„¿æ± å¼€æ”¾âœŒğŸ»...\n",
      "                               ...                        \n",
      "13000    ğ™‰ğ™ğ™˜ğ™š ğ™©ğ™¤ ğ˜¾ ğ™®ğ™¤ğ™ªç°åœºåšå®éªŒçš„ä¸€å¤©ğŸ”…#ç¾å¦†ç”Ÿæ´»##motd##å¥½ç‰©åˆ†äº«# Â [ç»„å›¾å…±...\n",
      "13001    ğ™„ğ™‰ğ™ğ™Šğ™‰ğ™€ ğ™€ğ˜¿ğ™„ğ™ğ™„ğ™Šğ™‰_å¿ƒæ…•ä¸ä½ è‰²å½©ç¼–è¾‘éƒ¨å®ä¹ ç¼–è¾‘ä¸Šçº¿ğŸ’—#2023çš„å¥¹ä»¬##åŠ¨é™çš†é£å°š...\n",
      "13002    æœ€è¿‘å¤©æ°”å¤ªå¥½å•¦ï½åˆ°å¤„éƒ½æ˜¯æ˜¥æ—¥çš„ä¿¡å·ğŸŒ¿â˜ï¸#2023çš„å¥¹ä»¬##ç¾å¦†ç”Ÿæ´»##åŠ¨é™çš†é£å°š# Â [ç»„...\n",
      "13003    å•¾å’ªï¼Ê™ÊŸá´œá´‡ Ê™ÊŸá´œá´‡çš®é—ªç°ğŸ’™ä»Šå¤©æ‰“å¡äº†ç§‘é¢œæ°ã€Œè¶…CHILLã€ä¿æ¹¿è¡—åŒºï¼è¶…å¤šçš„æ²‰æµ¸å¼ä½“éªŒåœº...\n",
      "13004    ã€Šå…³äºç›¸å†Œé‡Œçš„å›¤å›¾è¿™ä»¶äº‹ã€‹âœ è¿Ÿåˆ°çš„âŒæœˆá´˜ÊŸá´É¢è¯·æŸ¥æ”¶ âœ“#2023çš„å¥¹ä»¬##æ—¥å¸¸ç¢ç‰‡plo...\n",
      "Name: å¾®åšæ­£æ–‡, Length: 13005, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# æŒ‡å®šæ–‡ä»¶è·¯å¾„\n",
    "file_path = '/Users/laihuiqian/Documents/weibo_0925/total1-0925.csv'\n",
    "\n",
    "# è¯»å–.csvæ–‡ä»¶åˆ°DataFrame\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# æå–â€œå¾®åšæ­£æ–‡â€è¿™ä¸€åˆ—\n",
    "weibo_content = df[\"å¾®åšæ­£æ–‡\"]\n",
    "\n",
    "# æ‰“å°â€œå¾®åšæ­£æ–‡â€\n",
    "print(weibo_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "026024ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "INFO:lda:n_documents: 13005\n",
      "INFO:lda:vocab_size: 5000\n",
      "INFO:lda:n_words: 316403\n",
      "INFO:lda:n_topics: 17\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -3833775\n",
      "INFO:lda:<10> log likelihood: -2799798\n",
      "INFO:lda:<20> log likelihood: -2643432\n",
      "INFO:lda:<30> log likelihood: -2592128\n",
      "INFO:lda:<40> log likelihood: -2566627\n",
      "INFO:lda:<50> log likelihood: -2551125\n",
      "INFO:lda:<60> log likelihood: -2540266\n",
      "INFO:lda:<70> log likelihood: -2532301\n",
      "INFO:lda:<80> log likelihood: -2525819\n",
      "INFO:lda:<90> log likelihood: -2520767\n",
      "INFO:lda:<99> log likelihood: -2518051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model done\n",
      "shape: (17, 5000)\n",
      "['00' '01' '02']\n",
      "[[4.48752468e-07 5.38951714e-04 9.87704182e-04]\n",
      " [3.78114720e-07 3.78114720e-07 3.78114720e-07]\n",
      " [5.29969792e-07 5.29969792e-07 5.35269490e-05]\n",
      " [8.35212562e-07 8.35212562e-07 8.35212562e-07]\n",
      " [4.75669505e-07 4.75669505e-07 4.75669505e-07]\n",
      " [4.77805915e-07 4.77805915e-07 4.77805915e-07]\n",
      " [6.86294695e-07 6.86294695e-07 6.86294695e-07]\n",
      " [3.24091993e-03 3.60502273e-04 4.50065259e-07]\n",
      " [6.06980273e-07 6.06980273e-07 6.06980273e-07]\n",
      " [7.44324525e-07 7.44324525e-07 7.44324525e-07]\n",
      " [4.38865970e-07 4.38865970e-07 4.38865970e-07]\n",
      " [4.73666932e-05 4.68977161e-07 4.68977161e-07]\n",
      " [7.97575371e-07 7.97575371e-07 7.97575371e-07]\n",
      " [8.49978751e-07 8.49978751e-07 8.49978751e-07]\n",
      " [3.08346952e-07 3.08346952e-07 3.08346952e-07]\n",
      " [6.33151830e-07 6.33151830e-07 6.33151830e-07]\n",
      " [3.63247518e-03 8.07037366e-07 8.07037366e-07]]\n",
      "topic: 0 sum: 1.0000000000000697\n",
      "topic: 1 sum: 0.9999999999999135\n",
      "topic: 2 sum: 0.9999999999999875\n",
      "topic: 3 sum: 0.9999999999999106\n",
      "topic: 4 sum: 0.9999999999999313\n",
      "topic: 5 sum: 1.0000000000000628\n",
      "topic: 6 sum: 1.000000000000059\n",
      "topic: 7 sum: 1.0000000000000773\n",
      "topic: 8 sum: 1.0000000000000568\n",
      "topic: 9 sum: 0.9999999999999494\n",
      "topic: 10 sum: 1.0000000000000324\n",
      "topic: 11 sum: 0.999999999999922\n",
      "*Topic 0\n",
      "- å¦†å®¹ ç¾å¦† è§†é¢‘ çœŸçš„ è¶…è¯ å£çº¢ çœ¼å¦† å¾®åš ä»Šæ—¥ è¿™ä¸ª çœ¼å½± æ°›å›´ é€‚åˆ è…®çº¢ æ‹¿æ é¢œè‰² æ¸©æŸ” é‡ç‚¹ ä»Šå¤© é«˜çº§\n",
      "*Topic 1\n",
      "- çœŸçš„ è¿˜æ˜¯ å°±æ˜¯ ç°åœ¨ æ²¡æœ‰ å¯ä»¥ è§‰å¾— ä¸€ä¸ª è¿™ä¸ª ä¸€å®š æ‰€ä»¥ å¤§å®¶ å¾ˆå¤š çŸ¥é“ å› ä¸º ä½†æ˜¯ å…¶å® ä¸ä¼š è¿™ç§ ä»€ä¹ˆ\n",
      "*Topic 2\n",
      "- è§†é¢‘ å¾®åš ootd åˆ†äº« ä»Šå¤© look æ˜¥å¤© ä»Šæ—¥ æ˜¥å­£ å¦†å®¹ å‡ºè¡— è¶…è¯ æ”»ç•¥ æœ€è¿‘ å‡ºæ¸¸ å¤æ—¥ å¤§ä¼š æ˜¥å¤ çœŸçš„ ä»€ä¹ˆ\n",
      "*Topic 3\n",
      "- äº’åŠ¨ å¤§å®¶ è¶…è¯ ç²‰ä¸ å¯ä»¥ è¯„è®º ä½ ä»¬ æ—¥å¸¸ å¤šå¤š ç§ä¿¡ å®è´ å¼€å­¦ å¾®åš ç¦åˆ© æ‰‹å†Œ ä¸€èµ· ä¸Šæ¦œ åŠ å…¥ æ­å–œ ç»§ç»­\n",
      "*Topic 4\n",
      "- è§†é¢‘ å¾®åš æŠ¤è‚¤ åˆ†äº« å§å¦¹ çš®è‚¤ é—®é¢˜ è‡ªå·± ä»Šå¤© å¤§å®¶ æœ€è¿‘ èµ·æ¥ æŠ¤ç† å¦‚ä½• çœŸçš„ å°±æ˜¯ å¥½ç‰© ä¸€ä¸ª æœ‰æ•ˆ æˆ‘ä»¬\n",
      "*Topic 5\n",
      "- é¦™æ°´ å‘³é“ æ²¡æœ‰ ä¸€ä¸ª é¦™æ°” ç«ç‘° æ¯æ—¥ ä»€ä¹ˆ æ¸…æ–° å°±æ˜¯ æ¸©æŸ” å¯ä»¥ ä¸€é¦™ ä¸æ˜¯ çŸ¥é“ èŠ±é¦™ ä¸€ç“¶ å–œæ¬¢ å¥³äºº ä¸€ç‚¹\n",
      "*Topic 6\n",
      "- æ–°å¹´ è®¡åˆ’ å®Œç¾ å¤§å®¶ å¾®åš æ”»ç•¥ å¼€æ˜¥ å¿«ä¹ å¼€ç®± å¥½é¢œ è§†é¢‘ ç¤¼ç›’ åˆ†äº« å“ç‰Œ è¿‡å¹´ ç¤¼ç‰© æ”¶åˆ° å¥½ç‰© ä¸€å¹´ ä½ ä»¬\n",
      "*Topic 7\n",
      "- é“¾æ¥ ç½‘é¡µ è¿˜æœ‰ ä¸€èµ· äº¬ä¸œ è¿™æ¬¡ å¤©çŒ« å…¨æ–‡ 11 ç¦åˆ© æ´»åŠ¨ å§å¦¹ å‡†å¤‡ è¶…å¤š å®˜æ–¹ æœç´¢ å¯ä»¥ æƒŠå–œ hedy ç›´æ’­é—´\n",
      "*Topic 8\n",
      "- å¾®åš ç”Ÿæ´» è§†é¢‘ plog æ—¥å¸¸ vlog æ—¥è®° ç¢ç‰‡ ä»Šå¤© æœ€è¿‘ å¿«ä¹ è®°å½• å¼ è¿› åšä¸» ä½ ä»¬ ä¸€äº› ä»€ä¹ˆ ootd ä¸€æ¡ ä¸€ä¸‹\n",
      "*Topic 9\n",
      "- æ—¶å°š å¾®åš è¶…çº§ å¥½çœ‹ å·´é» è®¾è®¡ ä»Šæ—¥ ç³»åˆ— é£æ ¼ ç°åœº æ‰‹è¾¹ urruolan ä»Šå¤© æ—¶è£…å‘¨ æ­é… ä¸€èµ· å¯ä»¥ ä¸Šæµ· çµæ„Ÿ è¿™æ¬¡\n",
      "*Topic 10\n",
      "- åˆ†äº« å¥½ç‰© æŠ¤è‚¤ ç²¾å çœŸçš„ æœ€è¿‘ æ¨è å½©å¦† ç²‰åº•æ¶² ç¾å¦† åº•å¦† é¢éœœ å®è— å…¨æ–‡ é˜²æ™’ é¢è†œ ç§è‰ çˆ±ç”¨ç‰© çˆ±ç”¨ ç§‹å†¬\n",
      "*Topic 11\n",
      "- è‡ªå·± æˆ‘ä»¬ ä¸€ä¸ª çœŸçš„ å¸Œæœ› è¿™ä¸ª çœ‹åˆ° å°±æ˜¯ å¯ä»¥ æ²¡æœ‰ ä¸€ç›´ è¿™ä¹ˆ çŸ¥é“ ç”Ÿæ´» ä»€ä¹ˆ æ—¶å€™ ä¸æ˜¯ å…¨æ–‡ ä¸€èµ· æ°¸è¿œ\n",
      "*Topic 12\n",
      "- å¾®åš è§†é¢‘ å¨±ä¹ è¿·å¦¹ çœŸçš„ ç™½é›ª ä»Šæ—¥ words ä¸€ç•ª æ¼”å”±ä¼š fane å“ˆå“ˆå“ˆ å‘¨è®° key ä¸‹å‘¨ å¯çˆ± å–œæ¬¢ åå·® è¶…è¯ å¼ æ°\n",
      "*Topic 13\n",
      "- å¾®åš è§†é¢‘ è¶…è¯ ç¾å¦† æŠ½å¥– è¯¦æƒ… åˆ†äº« ä¸€æ kiko æœ¬æœŸ æ„Ÿè°¢ è¯„è®º å¹¼ç†™ å“ç‰Œ å˜ç¾ å—å— å®˜æ–¹ å¿«ä¹ åŒ–å¦† ä¸€èµ·\n",
      "*Topic 14\n",
      "- çš®è‚¤ è‚Œè‚¤ ç²¾å ä¿®æŠ¤ æŠ¤è‚¤ è´¨åœ° æ•ˆæœ å¯ä»¥ æŠ—è€ ä¿æ¹¿ çŠ¶æ€ æˆåˆ† æ•æ„Ÿ å¸æ”¶ ä½¿ç”¨ ç†¬å¤œ é‡Œé¢ ç´§è‡´ ç»†è…» æ„Ÿè§‰\n",
      "*Topic 15\n",
      "- çœŸçš„ æ„Ÿè§‰ å¤´å‘ å¤æ—¥ å¯ä»¥ ç²¾è‡´ ä½“éªŒ å–œæ¬¢ ç³»åˆ— æ°›å›´ æµªæ¼« æŒ‘æˆ˜ é€ å‹ æ­é… è¿™ä¸ª å°±æ˜¯ è‡ªå·± æŠ¤å‘ å¤å¤© èµ·æ¥\n",
      "*Topic 16\n",
      "- åœ°å›¾ æ˜¾ç¤º ä¸Šæµ· æ—…è¡Œ ootd æˆéƒ½ æ„Ÿå— ä¸€èµ· è¶…è¯ æµªæ¼« ç”Ÿæ´» åŸå¸‚ è¶…çº§ 12 ä¸€åœº ç›´æ’­ é©¬é” ä¸€ä¸ª æ²»æ„ˆ 10\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "def open_dict(Dict):\n",
    "    path = '%s.txt' % Dict\n",
    "    dictionary = open(path, 'r', encoding='utf-8')\n",
    "    dict = []\n",
    "    for word in dictionary:\n",
    "        word = word.strip('\\n')\n",
    "        dict.append(word)\n",
    "    return dict\n",
    "\n",
    "# clearwords=open_dict('clearwords')\n",
    "\n",
    "#åˆ†è¯\n",
    "# def chinese_word_cut(mytext):\n",
    "#     tempcut=jieba.cut(str(mytext))\n",
    "#     return \" \".join(set(tempcut)-set(clearwords))\n",
    "def chinese_word_cut(mytext):\n",
    "    tempcut=jieba.cut(str(mytext))\n",
    "    return \" \".join(set(tempcut))\n",
    "\n",
    "#æ‰“å°å‰n_top_wordså…³é”®è¯\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "# df = pd.read_excel(\"metoo20181221.xlsx\",'Sheet1',index_col=None,na_values=['NA'])\n",
    "# df.shape\n",
    "df[\"content_cutted\"] = df[\"å¾®åšæ­£æ–‡\"].apply(chinese_word_cut)\n",
    "n_features = 5000\n",
    "\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "tf = tf_vectorizer.fit_transform(df.content_cutted)\n",
    "vocab=tf_vectorizer.get_feature_names_out()\n",
    "model = lda.LDA(n_topics=17, n_iter=100, random_state=1)  \n",
    "model.fit(tf)\n",
    "print('model done')\n",
    "\n",
    "#ä¸»é¢˜-å•è¯ï¼ˆtopic-wordï¼‰åˆ†å¸ƒ\n",
    "topic_word = model.topic_word_ \n",
    "print(\"shape: {}\".format(topic_word.shape))\n",
    "print(vocab[:3])\n",
    "print(topic_word[:, :3])\n",
    "for n in range(12):\n",
    "    sum_pr = sum(topic_word[n,:])  \n",
    "    print(\"topic: {} sum: {}\".format(n, sum_pr))\n",
    "\n",
    "#è®¡ç®—å„ä¸»é¢˜Top-Nä¸ªå•è¯\n",
    "import numpy as np\n",
    "n = 20\n",
    "for i, topic_dist in enumerate(topic_word):  \n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]  \n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f09d461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
