{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13424659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files merged and saved as total1-4.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_csvs_in_directory(directory_path, output_filename):\n",
    "    # åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶\n",
    "    files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "    # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªCSVæ–‡ä»¶\n",
    "    if not files:\n",
    "        print(\"No CSV files found in the specified directory.\")\n",
    "        return\n",
    "\n",
    "    # è¯»å–ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶\n",
    "    df_total = pd.read_csv(os.path.join(directory_path, files[0]), encoding='utf-8')\n",
    "\n",
    "    # è¯»å–å¹¶åˆå¹¶å…¶ä½™çš„CSVæ–‡ä»¶\n",
    "    for file in files[1:]:\n",
    "        df = pd.read_csv(os.path.join(directory_path, file), encoding='utf-8')\n",
    "        df_total = pd.concat([df_total, df], ignore_index=True)\n",
    "\n",
    "    # ä¿å­˜åˆå¹¶åçš„CSVæ–‡ä»¶\n",
    "    df_total.to_csv(os.path.join(directory_path, output_filename), index=False, encoding='utf-8')\n",
    "    print(f\"CSV files merged and saved as {output_filename}.\")\n",
    "\n",
    "# ä½¿ç”¨æ–¹æ³•\n",
    "directory_path = '/Users/laihuiqian/Documents/weibo_0918/'  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "output_filename = 'total1-4.csv'\n",
    "merge_csvs_in_directory(directory_path, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72b46426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files merged and saved as total1-4.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result['encoding']\n",
    "\n",
    "def merge_csvs_in_directory(directory_path, output_filename):\n",
    "    # åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶\n",
    "    files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "    # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªCSVæ–‡ä»¶\n",
    "    if not files:\n",
    "        print(\"No CSV files found in the specified directory.\")\n",
    "        return\n",
    "\n",
    "    # è¯»å–ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶\n",
    "    encoding = detect_encoding(os.path.join(directory_path, files[0]))\n",
    "    df_total = pd.read_csv(os.path.join(directory_path, files[0]), encoding=encoding)\n",
    "\n",
    "    # è¯»å–å¹¶åˆå¹¶å…¶ä½™çš„CSVæ–‡ä»¶\n",
    "    for file in files[1:]:\n",
    "        encoding = detect_encoding(os.path.join(directory_path, file))\n",
    "        df = pd.read_csv(os.path.join(directory_path, file), encoding=encoding)\n",
    "        df_total = pd.concat([df_total, df], ignore_index=True)\n",
    "\n",
    "    # ä¿å­˜åˆå¹¶åçš„CSVæ–‡ä»¶\n",
    "    df_total.to_csv(os.path.join(directory_path, output_filename), index=False, encoding='utf-8')\n",
    "    print(f\"CSV files merged and saved as {output_filename}.\")\n",
    "\n",
    "# ä½¿ç”¨æ–¹æ³•\n",
    "directory_path = '/Users/laihuiqian/Documents/weibo_0918/'  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "output_filename = 'total1-4.csv'\n",
    "merge_csvs_in_directory(directory_path, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff7af80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files merged and saved as total1-4.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result['encoding']\n",
    "\n",
    "def convert_to_utf8(file_path):\n",
    "    encoding = detect_encoding(file_path)\n",
    "    df = pd.read_csv(file_path, encoding=encoding)\n",
    "    df.to_csv(file_path, index=False, encoding='utf-8')\n",
    "\n",
    "def merge_csvs_in_directory(directory_path, output_filename):\n",
    "    # åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶\n",
    "    files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "    # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªCSVæ–‡ä»¶\n",
    "    if not files:\n",
    "        print(\"No CSV files found in the specified directory.\")\n",
    "        return\n",
    "\n",
    "    # è½¬æ¢æ¯ä¸ªCSVæ–‡ä»¶ä¸ºutf-8ç¼–ç \n",
    "    for file in files:\n",
    "        convert_to_utf8(os.path.join(directory_path, file))\n",
    "\n",
    "    # è¯»å–ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶\n",
    "    df_total = pd.read_csv(os.path.join(directory_path, files[0]), encoding='utf-8')\n",
    "\n",
    "    # è¯»å–å¹¶åˆå¹¶å…¶ä½™çš„CSVæ–‡ä»¶\n",
    "    for file in files[1:]:\n",
    "        df = pd.read_csv(os.path.join(directory_path, file), encoding='utf-8')\n",
    "        df_total = pd.concat([df_total, df], ignore_index=True)\n",
    "\n",
    "    # ä¿å­˜åˆå¹¶åçš„CSVæ–‡ä»¶\n",
    "    df_total.to_csv(os.path.join(directory_path, output_filename), index=False, encoding='utf-8')\n",
    "    print(f\"CSV files merged and saved as {output_filename}.\")\n",
    "\n",
    "# ä½¿ç”¨æ–¹æ³•\n",
    "directory_path = '/Users/laihuiqian/Documents/weibo_0918/'  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "output_filename = 'total1-4.csv'\n",
    "merge_csvs_in_directory(directory_path, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7795fa6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 4: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-90e4de15067c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mfirst_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdf_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# åœ¨Jupyter Notebookä¸­æ‰“å°DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8d in position 4: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result['encoding']\n",
    "\n",
    "# æŒ‡å®šä½ çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "directory_path = '/Users/laihuiqian/Documents/weibo_0918/'  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "\n",
    "# è·å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶\n",
    "files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "# ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªCSVæ–‡ä»¶\n",
    "if not files:\n",
    "    print(\"No CSV files found in the specified directory.\")\n",
    "else:\n",
    "    # è¯»å–ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶\n",
    "    first_file_path = os.path.join(directory_path, files[0])\n",
    "    encoding = detect_encoding(first_file_path)\n",
    "    df_first = pd.read_csv(first_file_path, encoding=encoding)\n",
    "\n",
    "    # åœ¨Jupyter Notebookä¸­æ‰“å°DataFrame\n",
    "    display(df_first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98daeb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>å¾®åšid</th>\n",
       "      <th>å¾®åšæ­£æ–‡</th>\n",
       "      <th>å¤´æ¡æ–‡ç« url</th>\n",
       "      <th>åŸå§‹å›¾ç‰‡url</th>\n",
       "      <th>å¾®åšè§†é¢‘url</th>\n",
       "      <th>å‘å¸ƒä½ç½®</th>\n",
       "      <th>å‘å¸ƒæ—¶é—´</th>\n",
       "      <th>å‘å¸ƒå·¥å…·</th>\n",
       "      <th>ç‚¹èµæ•°</th>\n",
       "      <th>è½¬å‘æ•°</th>\n",
       "      <th>è¯„è®ºæ•°</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Njql4rGWs</td>\n",
       "      <td>èº«ä½“æŠ¤ç†å¥½ç‰©åˆ†äº« å§å¦¹ä»¬ï¼Œèº«ä½“å¤´çš®çš„æŠ¤ç†å’Œæˆ‘ä»¬é¢éƒ¨ä¸€æ ·é‡è¦å“¦ï¼ä»å¤´åˆ°è„šéƒ½ä¸èƒ½æ”¾è¿‡ï¼ æœ€è¿‘ä¸€ä¸ª...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/006tjdECly1hhxf7go...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>æ— </td>\n",
       "      <td>2023-09-15 18:00</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>4258</td>\n",
       "      <td>204</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NjdSIx7mr</td>\n",
       "      <td>æ°¸è¿œè‡ªç”±å¦‚é£~#ä»Šå¤©ç©¿ä»€ä¹ˆ##ootd# #å¾®åšå˜ç¾æ‰‹å†Œ# #ç§è‰# åšå°”å¡”æ‹‰Â·èµ›é‡Œæœ¨æ¹– Â ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww1.sinaimg.cn/large/006tjdECly1hhvw7vp...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>åšå°”å¡”æ‹‰Â·èµ›é‡Œæœ¨æ¹–</td>\n",
       "      <td>2023-09-14 10:17</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>4911</td>\n",
       "      <td>104</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NjdQKvWzj</td>\n",
       "      <td>PIPIs SHARING | æ—©ç§‹æŠ¤è‚¤åˆ†äº«ç§‹å¤©çš„ç¬¬ä¸€ä»½æŠ¤è‚¤å“è¯·æŸ¥æ”¶ï½#å¥½ç‰©åˆ†äº«##å¥½ç‰©æ¨è...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>æ— </td>\n",
       "      <td>https://f.video.weibocdn.com/o0/ZJkVnEb8lx088A...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>2023-09-14 10:13</td>\n",
       "      <td>å¾®åšè§†é¢‘å·</td>\n",
       "      <td>6650</td>\n",
       "      <td>326</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NiMJzE4VZ</td>\n",
       "      <td>å°busä¹Ÿå¤ªå¯çˆ±äº†å§ï¼#å¥½ç‰©æ¨è##å¥½ç‰©åˆ†äº«# Â [ç»„å›¾å…±9å¼ ]Â åŸå›¾</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww2.sinaimg.cn/large/006tjdECly1hhsjzmd...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>æ— </td>\n",
       "      <td>2023-09-11 13:11</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>14711</td>\n",
       "      <td>107</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Niv5EnIuE</td>\n",
       "      <td>ä¸ç”Ÿå‘½åŠ›å¯¹è¯#OOTD##ä»Šå¤©ç©¿ä»€ä¹ˆ# #å¾®åšå˜ç¾æ‰‹å†Œ# #ç§è‰# Â [ç»„å›¾å…±9å¼ ]Â åŸå›¾</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/006tjdECly1hhqeg01...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>æ— </td>\n",
       "      <td>2023-09-09 16:16</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>14683</td>\n",
       "      <td>108</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>L9egkogzS</td>\n",
       "      <td>åˆæ˜¯ç²‰ç²‰çš„ä¸€å¤©ğŸ’—#è™å¹´å¼€è¿ç©¿æ­# Â [ç»„å›¾å…±9å¼ ]Â åŸå›¾</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/006tjdECly1gy1tx17...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>æ— </td>\n",
       "      <td>2022-01-04 19:24</td>\n",
       "      <td>æ— </td>\n",
       "      <td>36022</td>\n",
       "      <td>107</td>\n",
       "      <td>832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>L8Wj6l13P</td>\n",
       "      <td>å®¶äººä»¬ï¼Œæ–°å¹´å¿«ä¹ï¼ Â [ç»„å›¾å…±6å¼ ]Â åŸå›¾</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww2.sinaimg.cn/large/006tjdECly1gxznsb2...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>æ— </td>\n",
       "      <td>2022-01-02 21:41</td>\n",
       "      <td>æ— </td>\n",
       "      <td>30324</td>\n",
       "      <td>511</td>\n",
       "      <td>1181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>L8T3dhl9n</td>\n",
       "      <td>è¿‡å¹´äº†ğŸ§¨Thxsâ¤ï¸ Â [ç»„å›¾å…±18å¼ ]Â åŸå›¾</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww2.sinaimg.cn/large/006tjdECly1gxz95ld...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>æ— </td>\n",
       "      <td>2022-01-02 13:24</td>\n",
       "      <td>æ— </td>\n",
       "      <td>5879</td>\n",
       "      <td>310</td>\n",
       "      <td>931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>L8Nc9gDC5</td>\n",
       "      <td>12æœˆçš„ä¸‰äºšï¼Œæ»¡è¶³äº†æˆ‘å¯¹æµ·å²›çš„ä¸€åˆ‡å¹»æƒ³ï¼å‰å‡ å¤©ç»„å›¢å’Œå§å¦¹ä¸€èµ·å»ä¸‰äºšè¿‡å†¬å•¦ï¼Œæ¯æ¬¡æ¥å¿…é€›çš„â€œæ™¯ç‚¹...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww3.sinaimg.cn/large/0060cLBFly8gxyg3f1...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>æ— </td>\n",
       "      <td>2022-01-01 22:30</td>\n",
       "      <td>å¾®åš  weibo.com</td>\n",
       "      <td>31299</td>\n",
       "      <td>203</td>\n",
       "      <td>1127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>L8HTBd99L</td>\n",
       "      <td>#é™ˆå¤§çš®å„¿[è¶…è¯]#æ–°å¹´å¿«ä¹å®è´ä»¬ï¼ï¼ï¼[è¯ç­’]åäºŒæœˆPIPIç²‰ä¸äº’åŠ¨æ¦œå’Œè¶…è¯ç²‰ç¾¤çœ¼ç†Ÿæ¦œæ–°é²œ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww2.sinaimg.cn/large/006tjdECly1gxxw2dk...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>æ— </td>\n",
       "      <td>2022-01-01 09:00</td>\n",
       "      <td>é™ˆå¤§çš®å„¿è¶…è¯</td>\n",
       "      <td>33399</td>\n",
       "      <td>114</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>308 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          å¾®åšid                                               å¾®åšæ­£æ–‡  å¤´æ¡æ–‡ç« url  \\\n",
       "0    Njql4rGWs  èº«ä½“æŠ¤ç†å¥½ç‰©åˆ†äº« å§å¦¹ä»¬ï¼Œèº«ä½“å¤´çš®çš„æŠ¤ç†å’Œæˆ‘ä»¬é¢éƒ¨ä¸€æ ·é‡è¦å“¦ï¼ä»å¤´åˆ°è„šéƒ½ä¸èƒ½æ”¾è¿‡ï¼ æœ€è¿‘ä¸€ä¸ª...      NaN   \n",
       "1    NjdSIx7mr  æ°¸è¿œè‡ªç”±å¦‚é£~#ä»Šå¤©ç©¿ä»€ä¹ˆ##ootd# #å¾®åšå˜ç¾æ‰‹å†Œ# #ç§è‰# åšå°”å¡”æ‹‰Â·èµ›é‡Œæœ¨æ¹– Â ...      NaN   \n",
       "2    NjdQKvWzj  PIPIs SHARING | æ—©ç§‹æŠ¤è‚¤åˆ†äº«ç§‹å¤©çš„ç¬¬ä¸€ä»½æŠ¤è‚¤å“è¯·æŸ¥æ”¶ï½#å¥½ç‰©åˆ†äº«##å¥½ç‰©æ¨è...      NaN   \n",
       "3    NiMJzE4VZ               å°busä¹Ÿå¤ªå¯çˆ±äº†å§ï¼#å¥½ç‰©æ¨è##å¥½ç‰©åˆ†äº«# Â [ç»„å›¾å…±9å¼ ]Â åŸå›¾Â       NaN   \n",
       "4    Niv5EnIuE     ä¸ç”Ÿå‘½åŠ›å¯¹è¯#OOTD##ä»Šå¤©ç©¿ä»€ä¹ˆ# #å¾®åšå˜ç¾æ‰‹å†Œ# #ç§è‰# Â [ç»„å›¾å…±9å¼ ]Â åŸå›¾Â       NaN   \n",
       "..         ...                                                ...      ...   \n",
       "303  L9egkogzS                      åˆæ˜¯ç²‰ç²‰çš„ä¸€å¤©ğŸ’—#è™å¹´å¼€è¿ç©¿æ­# Â [ç»„å›¾å…±9å¼ ]Â åŸå›¾Â       NaN   \n",
       "304  L8Wj6l13P                             å®¶äººä»¬ï¼Œæ–°å¹´å¿«ä¹ï¼ Â [ç»„å›¾å…±6å¼ ]Â åŸå›¾Â       NaN   \n",
       "305  L8T3dhl9n                           è¿‡å¹´äº†ğŸ§¨Thxsâ¤ï¸ Â [ç»„å›¾å…±18å¼ ]Â åŸå›¾Â       NaN   \n",
       "306  L8Nc9gDC5  12æœˆçš„ä¸‰äºšï¼Œæ»¡è¶³äº†æˆ‘å¯¹æµ·å²›çš„ä¸€åˆ‡å¹»æƒ³ï¼å‰å‡ å¤©ç»„å›¢å’Œå§å¦¹ä¸€èµ·å»ä¸‰äºšè¿‡å†¬å•¦ï¼Œæ¯æ¬¡æ¥å¿…é€›çš„â€œæ™¯ç‚¹...      NaN   \n",
       "307  L8HTBd99L  #é™ˆå¤§çš®å„¿[è¶…è¯]#æ–°å¹´å¿«ä¹å®è´ä»¬ï¼ï¼ï¼[è¯ç­’]åäºŒæœˆPIPIç²‰ä¸äº’åŠ¨æ¦œå’Œè¶…è¯ç²‰ç¾¤çœ¼ç†Ÿæ¦œæ–°é²œ...      NaN   \n",
       "\n",
       "                                               åŸå§‹å›¾ç‰‡url  \\\n",
       "0    http://ww4.sinaimg.cn/large/006tjdECly1hhxf7go...   \n",
       "1    http://ww1.sinaimg.cn/large/006tjdECly1hhvw7vp...   \n",
       "2                                                    æ—    \n",
       "3    http://ww2.sinaimg.cn/large/006tjdECly1hhsjzmd...   \n",
       "4    http://ww4.sinaimg.cn/large/006tjdECly1hhqeg01...   \n",
       "..                                                 ...   \n",
       "303  http://ww4.sinaimg.cn/large/006tjdECly1gy1tx17...   \n",
       "304  http://ww2.sinaimg.cn/large/006tjdECly1gxznsb2...   \n",
       "305  http://ww2.sinaimg.cn/large/006tjdECly1gxz95ld...   \n",
       "306  http://ww3.sinaimg.cn/large/0060cLBFly8gxyg3f1...   \n",
       "307  http://ww2.sinaimg.cn/large/006tjdECly1gxxw2dk...   \n",
       "\n",
       "                                               å¾®åšè§†é¢‘url       å‘å¸ƒä½ç½®  \\\n",
       "0                                                    æ—           æ—    \n",
       "1                                                    æ—   åšå°”å¡”æ‹‰Â·èµ›é‡Œæœ¨æ¹–   \n",
       "2    https://f.video.weibocdn.com/o0/ZJkVnEb8lx088A...          æ—    \n",
       "3                                                    æ—           æ—    \n",
       "4                                                    æ—           æ—    \n",
       "..                                                 ...        ...   \n",
       "303                                                  æ—           æ—    \n",
       "304                                                  æ—           æ—    \n",
       "305                                                  æ—           æ—    \n",
       "306                                                  æ—           æ—    \n",
       "307                                                  æ—           æ—    \n",
       "\n",
       "                 å‘å¸ƒæ—¶é—´           å‘å¸ƒå·¥å…·    ç‚¹èµæ•°  è½¬å‘æ•°   è¯„è®ºæ•°  \n",
       "0    2023-09-15 18:00      iPhone 13   4258  204   433  \n",
       "1    2023-09-14 10:17      iPhone 13   4911  104   347  \n",
       "2    2023-09-14 10:13          å¾®åšè§†é¢‘å·   6650  326   421  \n",
       "3    2023-09-11 13:11      iPhone 13  14711  107   342  \n",
       "4    2023-09-09 16:16      iPhone 13  14683  108   341  \n",
       "..                ...            ...    ...  ...   ...  \n",
       "303  2022-01-04 19:24              æ—   36022  107   832  \n",
       "304  2022-01-02 21:41              æ—   30324  511  1181  \n",
       "305  2022-01-02 13:24              æ—    5879  310   931  \n",
       "306  2022-01-01 22:30  å¾®åš  weibo.com  31299  203  1127  \n",
       "307  2022-01-01 09:00         é™ˆå¤§çš®å„¿è¶…è¯  33399  114   630  \n",
       "\n",
       "[308 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# æŒ‡å®šä½ çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "directory_path = '/Users/laihuiqian/Documents/weibo_0918/'  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "\n",
    "# è·å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶\n",
    "files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "# ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªCSVæ–‡ä»¶\n",
    "if not files:\n",
    "    print(\"No CSV files found in the specified directory.\")\n",
    "else:\n",
    "    # è¯»å–ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶\n",
    "    first_file_path = os.path.join(directory_path, files[0])\n",
    "    \n",
    "    encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']\n",
    "    df_first = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            df_first = pd.read_csv(first_file_path, encoding=enc)\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    if df_first is not None:\n",
    "        # åœ¨Jupyter Notebookä¸­æ‰“å°DataFrame\n",
    "        display(df_first)\n",
    "    else:\n",
    "        print(\"Failed to read the CSV file with the provided encodings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ef9b4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>å¾®åšid</th>\n",
       "      <th>å¾®åšæ­£æ–‡</th>\n",
       "      <th>å¤´æ¡æ–‡ç« url</th>\n",
       "      <th>åŸå§‹å›¾ç‰‡url</th>\n",
       "      <th>å¾®åšè§†é¢‘url</th>\n",
       "      <th>å‘å¸ƒä½ç½®</th>\n",
       "      <th>å‘å¸ƒæ—¶é—´</th>\n",
       "      <th>å‘å¸ƒå·¥å…·</th>\n",
       "      <th>ç‚¹èµæ•°</th>\n",
       "      <th>è½¬å‘æ•°</th>\n",
       "      <th>è¯„è®ºæ•°</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Njql4rGWs</td>\n",
       "      <td>èº«ä½“æŠ¤ç†å¥½ç‰©åˆ†äº« å§å¦¹ä»¬ï¼Œèº«ä½“å¤´çš®çš„æŠ¤ç†å’Œæˆ‘ä»¬é¢éƒ¨ä¸€æ ·é‡è¦å“¦ï¼ä»å¤´åˆ°è„šéƒ½ä¸èƒ½æ”¾è¿‡ï¼ æœ€è¿‘ä¸€ä¸ª...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/006tjdECly1hhxf7go...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>æ— </td>\n",
       "      <td>2023-09-15 18:00</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>4258</td>\n",
       "      <td>204</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NjdSIx7mr</td>\n",
       "      <td>æ°¸è¿œè‡ªç”±å¦‚é£~#ä»Šå¤©ç©¿ä»€ä¹ˆ##ootd# #å¾®åšå˜ç¾æ‰‹å†Œ# #ç§è‰# åšå°”å¡”æ‹‰Â·èµ›é‡Œæœ¨æ¹– Â ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww1.sinaimg.cn/large/006tjdECly1hhvw7vp...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>åšå°”å¡”æ‹‰Â·èµ›é‡Œæœ¨æ¹–</td>\n",
       "      <td>2023-09-14 10:17</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>4911</td>\n",
       "      <td>104</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NjdQKvWzj</td>\n",
       "      <td>PIPIs SHARING | æ—©ç§‹æŠ¤è‚¤åˆ†äº«ç§‹å¤©çš„ç¬¬ä¸€ä»½æŠ¤è‚¤å“è¯·æŸ¥æ”¶ï½#å¥½ç‰©åˆ†äº«##å¥½ç‰©æ¨è...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>æ— </td>\n",
       "      <td>https://f.video.weibocdn.com/o0/ZJkVnEb8lx088A...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>2023-09-14 10:13</td>\n",
       "      <td>å¾®åšè§†é¢‘å·</td>\n",
       "      <td>6650</td>\n",
       "      <td>326</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NiMJzE4VZ</td>\n",
       "      <td>å°busä¹Ÿå¤ªå¯çˆ±äº†å§ï¼#å¥½ç‰©æ¨è##å¥½ç‰©åˆ†äº«# Â [ç»„å›¾å…±9å¼ ]Â åŸå›¾</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww2.sinaimg.cn/large/006tjdECly1hhsjzmd...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>æ— </td>\n",
       "      <td>2023-09-11 13:11</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>14711</td>\n",
       "      <td>107</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Niv5EnIuE</td>\n",
       "      <td>ä¸ç”Ÿå‘½åŠ›å¯¹è¯#OOTD##ä»Šå¤©ç©¿ä»€ä¹ˆ# #å¾®åšå˜ç¾æ‰‹å†Œ# #ç§è‰# Â [ç»„å›¾å…±9å¼ ]Â åŸå›¾</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/006tjdECly1hhqeg01...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>æ— </td>\n",
       "      <td>2023-09-09 16:16</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>14683</td>\n",
       "      <td>108</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>LaejswP9y</td>\n",
       "      <td>#å¤©å¤©ä¸æƒ³ä¸Šç­å¯èƒ½ä¸æ˜¯å› ä¸ºæ‡’#â—ï¸å¦‚ä½•ä¿æŒä¸Šç­çš„æ¿€æƒ…ï¼Ÿâ—ï¸ä¸Šç­å•çº¯ä¸ºäº†ç”Ÿè®¡å—ï¼Ÿâ—ï¸ä¸æƒ³ä¸Šç­çš„...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>æ— </td>\n",
       "      <td>https://f.video.weibocdn.com/o0/YlgaIPJJlx07SS...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>2022-01-11 09:22</td>\n",
       "      <td>å¾®åšè§†é¢‘å·</td>\n",
       "      <td>5033</td>\n",
       "      <td>1156</td>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>L9QaVq586</td>\n",
       "      <td>#å¸¦ç€å¾®åšå»æ—…è¡Œ# ä¸ºæœŸå°†è¿‘10å¤©çš„æ—…è¡Œç»“æŸäº†ğŸ”šå¼€å¿ƒ é‡åˆ°äº†è€æœ‹å‹ ï¼Œè®¤è¯†äº†æ–°æœ‹å‹â€¦åŠªåŠ›å·¥ä½œ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/001lWunQgy1gy6i6mz...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>ä¸‰äºšÂ·ä¸‰äºšå‡¤å‡°å›½é™…æœºåœº</td>\n",
       "      <td>2022-01-08 19:55</td>\n",
       "      <td>iPhone 12 Pro</td>\n",
       "      <td>4970</td>\n",
       "      <td>1069</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>L91KHrjCW</td>\n",
       "      <td>#å…ƒæ—¦å‡æœŸæœ€åä¸€å¤©# é£é¾™åœ¨å¤©â€¦å—å±±å¯ºçš„å¤©ç©ºçŠ¹å¦‚ç¥æ¥ä¹‹ç¬”æ—§å²è¾äº‘å»ï¼Œæ—¦å§‹è¿æœé˜³ã€‚å–„è¡Œä¸–é—´äº‹ï¼Œ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww1.sinaimg.cn/large/001lWunQgy1gy0bruw...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>ä¸‰äºšÂ·å—å±±æ–‡åŒ–æ—…æ¸¸åŒº</td>\n",
       "      <td>2022-01-03 11:33</td>\n",
       "      <td>iPhone 12 Pro</td>\n",
       "      <td>4608</td>\n",
       "      <td>1197</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>L8WH2iHTj</td>\n",
       "      <td>#æˆ‘çš„æ½®æµå…³é”®è¯#  éšæ‹æ—¥å¸¸â€¦ ä¸‰äºš Â æ˜¾ç¤ºåœ°å›¾Â [ç»„å›¾å…±10å¼ ]Â åŸå›¾</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww1.sinaimg.cn/large/001lWunQgy1gxzpg9j...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>ä¸‰äºš</td>\n",
       "      <td>2022-01-02 22:40</td>\n",
       "      <td>iPhone 12 Pro</td>\n",
       "      <td>4840</td>\n",
       "      <td>1097</td>\n",
       "      <td>1147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>L8MqwDJJm</td>\n",
       "      <td>#2022ç¬¬ä¸€å¤©##æ— æ•°è¿½æ¢¦äººè¿˜åœ¨å¥‹æ–—å¥‰çŒ®#å¹³å®‰ å¥åº· å’Œå¼€å¿ƒ 2022 çœŸå¿ƒçš„æ–°å¹´æ„¿æœ›å°±æ˜¯...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://wx3.sinaimg.cn/large/001lWunQgy1gxyg3p...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>ä¸‰äºš</td>\n",
       "      <td>2022-01-01 20:32</td>\n",
       "      <td>iPhone 12 mini</td>\n",
       "      <td>4223</td>\n",
       "      <td>1015</td>\n",
       "      <td>1082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1432 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           å¾®åšid                                               å¾®åšæ­£æ–‡ å¤´æ¡æ–‡ç« url  \\\n",
       "0     Njql4rGWs  èº«ä½“æŠ¤ç†å¥½ç‰©åˆ†äº« å§å¦¹ä»¬ï¼Œèº«ä½“å¤´çš®çš„æŠ¤ç†å’Œæˆ‘ä»¬é¢éƒ¨ä¸€æ ·é‡è¦å“¦ï¼ä»å¤´åˆ°è„šéƒ½ä¸èƒ½æ”¾è¿‡ï¼ æœ€è¿‘ä¸€ä¸ª...     NaN   \n",
       "1     NjdSIx7mr  æ°¸è¿œè‡ªç”±å¦‚é£~#ä»Šå¤©ç©¿ä»€ä¹ˆ##ootd# #å¾®åšå˜ç¾æ‰‹å†Œ# #ç§è‰# åšå°”å¡”æ‹‰Â·èµ›é‡Œæœ¨æ¹– Â ...     NaN   \n",
       "2     NjdQKvWzj  PIPIs SHARING | æ—©ç§‹æŠ¤è‚¤åˆ†äº«ç§‹å¤©çš„ç¬¬ä¸€ä»½æŠ¤è‚¤å“è¯·æŸ¥æ”¶ï½#å¥½ç‰©åˆ†äº«##å¥½ç‰©æ¨è...     NaN   \n",
       "3     NiMJzE4VZ               å°busä¹Ÿå¤ªå¯çˆ±äº†å§ï¼#å¥½ç‰©æ¨è##å¥½ç‰©åˆ†äº«# Â [ç»„å›¾å…±9å¼ ]Â åŸå›¾Â      NaN   \n",
       "4     Niv5EnIuE     ä¸ç”Ÿå‘½åŠ›å¯¹è¯#OOTD##ä»Šå¤©ç©¿ä»€ä¹ˆ# #å¾®åšå˜ç¾æ‰‹å†Œ# #ç§è‰# Â [ç»„å›¾å…±9å¼ ]Â åŸå›¾Â      NaN   \n",
       "...         ...                                                ...     ...   \n",
       "1427  LaejswP9y  #å¤©å¤©ä¸æƒ³ä¸Šç­å¯èƒ½ä¸æ˜¯å› ä¸ºæ‡’#â—ï¸å¦‚ä½•ä¿æŒä¸Šç­çš„æ¿€æƒ…ï¼Ÿâ—ï¸ä¸Šç­å•çº¯ä¸ºäº†ç”Ÿè®¡å—ï¼Ÿâ—ï¸ä¸æƒ³ä¸Šç­çš„...     NaN   \n",
       "1428  L9QaVq586  #å¸¦ç€å¾®åšå»æ—…è¡Œ# ä¸ºæœŸå°†è¿‘10å¤©çš„æ—…è¡Œç»“æŸäº†ğŸ”šå¼€å¿ƒ é‡åˆ°äº†è€æœ‹å‹ ï¼Œè®¤è¯†äº†æ–°æœ‹å‹â€¦åŠªåŠ›å·¥ä½œ...     NaN   \n",
       "1429  L91KHrjCW  #å…ƒæ—¦å‡æœŸæœ€åä¸€å¤©# é£é¾™åœ¨å¤©â€¦å—å±±å¯ºçš„å¤©ç©ºçŠ¹å¦‚ç¥æ¥ä¹‹ç¬”æ—§å²è¾äº‘å»ï¼Œæ—¦å§‹è¿æœé˜³ã€‚å–„è¡Œä¸–é—´äº‹ï¼Œ...     NaN   \n",
       "1430  L8WH2iHTj             #æˆ‘çš„æ½®æµå…³é”®è¯#  éšæ‹æ—¥å¸¸â€¦ ä¸‰äºš Â æ˜¾ç¤ºåœ°å›¾Â [ç»„å›¾å…±10å¼ ]Â åŸå›¾Â      NaN   \n",
       "1431  L8MqwDJJm  #2022ç¬¬ä¸€å¤©##æ— æ•°è¿½æ¢¦äººè¿˜åœ¨å¥‹æ–—å¥‰çŒ®#å¹³å®‰ å¥åº· å’Œå¼€å¿ƒ 2022 çœŸå¿ƒçš„æ–°å¹´æ„¿æœ›å°±æ˜¯...     NaN   \n",
       "\n",
       "                                                åŸå§‹å›¾ç‰‡url  \\\n",
       "0     http://ww4.sinaimg.cn/large/006tjdECly1hhxf7go...   \n",
       "1     http://ww1.sinaimg.cn/large/006tjdECly1hhvw7vp...   \n",
       "2                                                     æ—    \n",
       "3     http://ww2.sinaimg.cn/large/006tjdECly1hhsjzmd...   \n",
       "4     http://ww4.sinaimg.cn/large/006tjdECly1hhqeg01...   \n",
       "...                                                 ...   \n",
       "1427                                                  æ—    \n",
       "1428  http://ww4.sinaimg.cn/large/001lWunQgy1gy6i6mz...   \n",
       "1429  http://ww1.sinaimg.cn/large/001lWunQgy1gy0bruw...   \n",
       "1430  http://ww1.sinaimg.cn/large/001lWunQgy1gxzpg9j...   \n",
       "1431  https://wx3.sinaimg.cn/large/001lWunQgy1gxyg3p...   \n",
       "\n",
       "                                                å¾®åšè§†é¢‘url         å‘å¸ƒä½ç½®  \\\n",
       "0                                                     æ—             æ—    \n",
       "1                                                     æ—     åšå°”å¡”æ‹‰Â·èµ›é‡Œæœ¨æ¹–   \n",
       "2     https://f.video.weibocdn.com/o0/ZJkVnEb8lx088A...            æ—    \n",
       "3                                                     æ—             æ—    \n",
       "4                                                     æ—             æ—    \n",
       "...                                                 ...          ...   \n",
       "1427  https://f.video.weibocdn.com/o0/YlgaIPJJlx07SS...            æ—    \n",
       "1428                                                  æ—   ä¸‰äºšÂ·ä¸‰äºšå‡¤å‡°å›½é™…æœºåœº   \n",
       "1429                                                  æ—    ä¸‰äºšÂ·å—å±±æ–‡åŒ–æ—…æ¸¸åŒº   \n",
       "1430                                                  æ—            ä¸‰äºš   \n",
       "1431                                                  æ—            ä¸‰äºš   \n",
       "\n",
       "                  å‘å¸ƒæ—¶é—´            å‘å¸ƒå·¥å…·    ç‚¹èµæ•°   è½¬å‘æ•°   è¯„è®ºæ•°  \n",
       "0     2023-09-15 18:00       iPhone 13   4258   204   433  \n",
       "1     2023-09-14 10:17       iPhone 13   4911   104   347  \n",
       "2     2023-09-14 10:13           å¾®åšè§†é¢‘å·   6650   326   421  \n",
       "3     2023-09-11 13:11       iPhone 13  14711   107   342  \n",
       "4     2023-09-09 16:16       iPhone 13  14683   108   341  \n",
       "...                ...             ...    ...   ...   ...  \n",
       "1427  2022-01-11 09:22           å¾®åšè§†é¢‘å·   5033  1156  1177  \n",
       "1428  2022-01-08 19:55   iPhone 12 Pro   4970  1069  1142  \n",
       "1429  2022-01-03 11:33   iPhone 12 Pro   4608  1197  1225  \n",
       "1430  2022-01-02 22:40   iPhone 12 Pro   4840  1097  1147  \n",
       "1431  2022-01-01 20:32  iPhone 12 mini   4223  1015  1082  \n",
       "\n",
       "[1432 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the displayed DataFrame look correct? (yes/no): yes\n",
      "DataFrame saved as total1-4.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# æŒ‡å®šä½ çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "directory_path = '/Users/laihuiqian/Documents/weibo_0918/'  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "\n",
    "# è·å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶\n",
    "files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "# ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªCSVæ–‡ä»¶\n",
    "if not files:\n",
    "    print(\"No CSV files found in the specified directory.\")\n",
    "else:\n",
    "    encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']\n",
    "    dfs = []\n",
    "\n",
    "    # å°è¯•ä½¿ç”¨å¤šç§ç¼–ç æ¥è¯»å–æ¯ä¸ªCSVæ–‡ä»¶\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        df = None\n",
    "        for enc in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=enc)\n",
    "                dfs.append(df)\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "\n",
    "    # åˆå¹¶æ‰€æœ‰çš„DataFrames\n",
    "    if dfs:\n",
    "        df_total = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # åœ¨Jupyter Notebookä¸­æ˜¾ç¤ºåˆå¹¶åçš„DataFrame\n",
    "        display(df_total)\n",
    "\n",
    "        # è¯¢é—®ç”¨æˆ·æ˜¯å¦ä¿å­˜ä¸ºCSV\n",
    "        save = input(\"Does the displayed DataFrame look correct? (yes/no): \")\n",
    "        if save.lower() == 'yes':\n",
    "            df_total.to_csv(os.path.join(directory_path, 'total1-4.csv'), index=False, encoding='utf-8')\n",
    "            print(\"DataFrame saved as total1-4.csv.\")\n",
    "    else:\n",
    "        print(\"Failed to read the CSV files with the provided encodings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2778483f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>å¾®åšid</th>\n",
       "      <th>å¾®åšæ­£æ–‡</th>\n",
       "      <th>å¤´æ¡æ–‡ç« url</th>\n",
       "      <th>åŸå§‹å›¾ç‰‡url</th>\n",
       "      <th>å¾®åšè§†é¢‘url</th>\n",
       "      <th>å‘å¸ƒä½ç½®</th>\n",
       "      <th>å‘å¸ƒæ—¶é—´</th>\n",
       "      <th>å‘å¸ƒå·¥å…·</th>\n",
       "      <th>ç‚¹èµæ•°</th>\n",
       "      <th>è½¬å‘æ•°</th>\n",
       "      <th>è¯„è®ºæ•°</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Njql4rGWs</td>\n",
       "      <td>èº«ä½“æŠ¤ç†å¥½ç‰©åˆ†äº« å§å¦¹ä»¬ï¼Œèº«ä½“å¤´çš®çš„æŠ¤ç†å’Œæˆ‘ä»¬é¢éƒ¨ä¸€æ ·é‡è¦å“¦ï¼ä»å¤´åˆ°è„šéƒ½ä¸èƒ½æ”¾è¿‡ï¼ æœ€è¿‘ä¸€ä¸ª...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/006tjdECly1hhxf7go...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>æ— </td>\n",
       "      <td>2023-09-15 18:00</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>4258</td>\n",
       "      <td>204</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NjdSIx7mr</td>\n",
       "      <td>æ°¸è¿œè‡ªç”±å¦‚é£~#ä»Šå¤©ç©¿ä»€ä¹ˆ##ootd# #å¾®åšå˜ç¾æ‰‹å†Œ# #ç§è‰# åšå°”å¡”æ‹‰Â·èµ›é‡Œæœ¨æ¹– Â ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww1.sinaimg.cn/large/006tjdECly1hhvw7vp...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>åšå°”å¡”æ‹‰Â·èµ›é‡Œæœ¨æ¹–</td>\n",
       "      <td>2023-09-14 10:17</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>4911</td>\n",
       "      <td>104</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NjdQKvWzj</td>\n",
       "      <td>PIPIs SHARING | æ—©ç§‹æŠ¤è‚¤åˆ†äº«ç§‹å¤©çš„ç¬¬ä¸€ä»½æŠ¤è‚¤å“è¯·æŸ¥æ”¶ï½#å¥½ç‰©åˆ†äº«##å¥½ç‰©æ¨è...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>æ— </td>\n",
       "      <td>https://f.video.weibocdn.com/o0/ZJkVnEb8lx088A...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>2023-09-14 10:13</td>\n",
       "      <td>å¾®åšè§†é¢‘å·</td>\n",
       "      <td>6650</td>\n",
       "      <td>326</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NiMJzE4VZ</td>\n",
       "      <td>å°busä¹Ÿå¤ªå¯çˆ±äº†å§ï¼#å¥½ç‰©æ¨è##å¥½ç‰©åˆ†äº«# Â [ç»„å›¾å…±9å¼ ]Â åŸå›¾</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww2.sinaimg.cn/large/006tjdECly1hhsjzmd...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>æ— </td>\n",
       "      <td>2023-09-11 13:11</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>14711</td>\n",
       "      <td>107</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Niv5EnIuE</td>\n",
       "      <td>ä¸ç”Ÿå‘½åŠ›å¯¹è¯#OOTD##ä»Šå¤©ç©¿ä»€ä¹ˆ# #å¾®åšå˜ç¾æ‰‹å†Œ# #ç§è‰# Â [ç»„å›¾å…±9å¼ ]Â åŸå›¾</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/006tjdECly1hhqeg01...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>æ— </td>\n",
       "      <td>2023-09-09 16:16</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>14683</td>\n",
       "      <td>108</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>LaejswP9y</td>\n",
       "      <td>#å¤©å¤©ä¸æƒ³ä¸Šç­å¯èƒ½ä¸æ˜¯å› ä¸ºæ‡’#â—ï¸å¦‚ä½•ä¿æŒä¸Šç­çš„æ¿€æƒ…ï¼Ÿâ—ï¸ä¸Šç­å•çº¯ä¸ºäº†ç”Ÿè®¡å—ï¼Ÿâ—ï¸ä¸æƒ³ä¸Šç­çš„...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>æ— </td>\n",
       "      <td>https://f.video.weibocdn.com/o0/YlgaIPJJlx07SS...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>2022-01-11 09:22</td>\n",
       "      <td>å¾®åšè§†é¢‘å·</td>\n",
       "      <td>5033</td>\n",
       "      <td>1156</td>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>L9QaVq586</td>\n",
       "      <td>#å¸¦ç€å¾®åšå»æ—…è¡Œ# ä¸ºæœŸå°†è¿‘10å¤©çš„æ—…è¡Œç»“æŸäº†ğŸ”šå¼€å¿ƒ é‡åˆ°äº†è€æœ‹å‹ ï¼Œè®¤è¯†äº†æ–°æœ‹å‹â€¦åŠªåŠ›å·¥ä½œ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/001lWunQgy1gy6i6mz...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>ä¸‰äºšÂ·ä¸‰äºšå‡¤å‡°å›½é™…æœºåœº</td>\n",
       "      <td>2022-01-08 19:55</td>\n",
       "      <td>iPhone 12 Pro</td>\n",
       "      <td>4970</td>\n",
       "      <td>1069</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>L91KHrjCW</td>\n",
       "      <td>#å…ƒæ—¦å‡æœŸæœ€åä¸€å¤©# é£é¾™åœ¨å¤©â€¦å—å±±å¯ºçš„å¤©ç©ºçŠ¹å¦‚ç¥æ¥ä¹‹ç¬”æ—§å²è¾äº‘å»ï¼Œæ—¦å§‹è¿æœé˜³ã€‚å–„è¡Œä¸–é—´äº‹ï¼Œ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww1.sinaimg.cn/large/001lWunQgy1gy0bruw...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>ä¸‰äºšÂ·å—å±±æ–‡åŒ–æ—…æ¸¸åŒº</td>\n",
       "      <td>2022-01-03 11:33</td>\n",
       "      <td>iPhone 12 Pro</td>\n",
       "      <td>4608</td>\n",
       "      <td>1197</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862</th>\n",
       "      <td>L8WH2iHTj</td>\n",
       "      <td>#æˆ‘çš„æ½®æµå…³é”®è¯#  éšæ‹æ—¥å¸¸â€¦ ä¸‰äºš Â æ˜¾ç¤ºåœ°å›¾Â [ç»„å›¾å…±10å¼ ]Â åŸå›¾</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww1.sinaimg.cn/large/001lWunQgy1gxzpg9j...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>ä¸‰äºš</td>\n",
       "      <td>2022-01-02 22:40</td>\n",
       "      <td>iPhone 12 Pro</td>\n",
       "      <td>4840</td>\n",
       "      <td>1097</td>\n",
       "      <td>1147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863</th>\n",
       "      <td>L8MqwDJJm</td>\n",
       "      <td>#2022ç¬¬ä¸€å¤©##æ— æ•°è¿½æ¢¦äººè¿˜åœ¨å¥‹æ–—å¥‰çŒ®#å¹³å®‰ å¥åº· å’Œå¼€å¿ƒ 2022 çœŸå¿ƒçš„æ–°å¹´æ„¿æœ›å°±æ˜¯...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://wx3.sinaimg.cn/large/001lWunQgy1gxyg3p...</td>\n",
       "      <td>æ— </td>\n",
       "      <td>ä¸‰äºš</td>\n",
       "      <td>2022-01-01 20:32</td>\n",
       "      <td>iPhone 12 mini</td>\n",
       "      <td>4223</td>\n",
       "      <td>1015</td>\n",
       "      <td>1082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2864 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           å¾®åšid                                               å¾®åšæ­£æ–‡  å¤´æ¡æ–‡ç« url  \\\n",
       "0     Njql4rGWs  èº«ä½“æŠ¤ç†å¥½ç‰©åˆ†äº« å§å¦¹ä»¬ï¼Œèº«ä½“å¤´çš®çš„æŠ¤ç†å’Œæˆ‘ä»¬é¢éƒ¨ä¸€æ ·é‡è¦å“¦ï¼ä»å¤´åˆ°è„šéƒ½ä¸èƒ½æ”¾è¿‡ï¼ æœ€è¿‘ä¸€ä¸ª...      NaN   \n",
       "1     NjdSIx7mr  æ°¸è¿œè‡ªç”±å¦‚é£~#ä»Šå¤©ç©¿ä»€ä¹ˆ##ootd# #å¾®åšå˜ç¾æ‰‹å†Œ# #ç§è‰# åšå°”å¡”æ‹‰Â·èµ›é‡Œæœ¨æ¹– Â ...      NaN   \n",
       "2     NjdQKvWzj  PIPIs SHARING | æ—©ç§‹æŠ¤è‚¤åˆ†äº«ç§‹å¤©çš„ç¬¬ä¸€ä»½æŠ¤è‚¤å“è¯·æŸ¥æ”¶ï½#å¥½ç‰©åˆ†äº«##å¥½ç‰©æ¨è...      NaN   \n",
       "3     NiMJzE4VZ               å°busä¹Ÿå¤ªå¯çˆ±äº†å§ï¼#å¥½ç‰©æ¨è##å¥½ç‰©åˆ†äº«# Â [ç»„å›¾å…±9å¼ ]Â åŸå›¾Â       NaN   \n",
       "4     Niv5EnIuE     ä¸ç”Ÿå‘½åŠ›å¯¹è¯#OOTD##ä»Šå¤©ç©¿ä»€ä¹ˆ# #å¾®åšå˜ç¾æ‰‹å†Œ# #ç§è‰# Â [ç»„å›¾å…±9å¼ ]Â åŸå›¾Â       NaN   \n",
       "...         ...                                                ...      ...   \n",
       "2859  LaejswP9y  #å¤©å¤©ä¸æƒ³ä¸Šç­å¯èƒ½ä¸æ˜¯å› ä¸ºæ‡’#â—ï¸å¦‚ä½•ä¿æŒä¸Šç­çš„æ¿€æƒ…ï¼Ÿâ—ï¸ä¸Šç­å•çº¯ä¸ºäº†ç”Ÿè®¡å—ï¼Ÿâ—ï¸ä¸æƒ³ä¸Šç­çš„...      NaN   \n",
       "2860  L9QaVq586  #å¸¦ç€å¾®åšå»æ—…è¡Œ# ä¸ºæœŸå°†è¿‘10å¤©çš„æ—…è¡Œç»“æŸäº†ğŸ”šå¼€å¿ƒ é‡åˆ°äº†è€æœ‹å‹ ï¼Œè®¤è¯†äº†æ–°æœ‹å‹â€¦åŠªåŠ›å·¥ä½œ...      NaN   \n",
       "2861  L91KHrjCW  #å…ƒæ—¦å‡æœŸæœ€åä¸€å¤©# é£é¾™åœ¨å¤©â€¦å—å±±å¯ºçš„å¤©ç©ºçŠ¹å¦‚ç¥æ¥ä¹‹ç¬”æ—§å²è¾äº‘å»ï¼Œæ—¦å§‹è¿æœé˜³ã€‚å–„è¡Œä¸–é—´äº‹ï¼Œ...      NaN   \n",
       "2862  L8WH2iHTj             #æˆ‘çš„æ½®æµå…³é”®è¯#  éšæ‹æ—¥å¸¸â€¦ ä¸‰äºš Â æ˜¾ç¤ºåœ°å›¾Â [ç»„å›¾å…±10å¼ ]Â åŸå›¾Â       NaN   \n",
       "2863  L8MqwDJJm  #2022ç¬¬ä¸€å¤©##æ— æ•°è¿½æ¢¦äººè¿˜åœ¨å¥‹æ–—å¥‰çŒ®#å¹³å®‰ å¥åº· å’Œå¼€å¿ƒ 2022 çœŸå¿ƒçš„æ–°å¹´æ„¿æœ›å°±æ˜¯...      NaN   \n",
       "\n",
       "                                                åŸå§‹å›¾ç‰‡url  \\\n",
       "0     http://ww4.sinaimg.cn/large/006tjdECly1hhxf7go...   \n",
       "1     http://ww1.sinaimg.cn/large/006tjdECly1hhvw7vp...   \n",
       "2                                                     æ—    \n",
       "3     http://ww2.sinaimg.cn/large/006tjdECly1hhsjzmd...   \n",
       "4     http://ww4.sinaimg.cn/large/006tjdECly1hhqeg01...   \n",
       "...                                                 ...   \n",
       "2859                                                  æ—    \n",
       "2860  http://ww4.sinaimg.cn/large/001lWunQgy1gy6i6mz...   \n",
       "2861  http://ww1.sinaimg.cn/large/001lWunQgy1gy0bruw...   \n",
       "2862  http://ww1.sinaimg.cn/large/001lWunQgy1gxzpg9j...   \n",
       "2863  https://wx3.sinaimg.cn/large/001lWunQgy1gxyg3p...   \n",
       "\n",
       "                                                å¾®åšè§†é¢‘url         å‘å¸ƒä½ç½®  \\\n",
       "0                                                     æ—             æ—    \n",
       "1                                                     æ—     åšå°”å¡”æ‹‰Â·èµ›é‡Œæœ¨æ¹–   \n",
       "2     https://f.video.weibocdn.com/o0/ZJkVnEb8lx088A...            æ—    \n",
       "3                                                     æ—             æ—    \n",
       "4                                                     æ—             æ—    \n",
       "...                                                 ...          ...   \n",
       "2859  https://f.video.weibocdn.com/o0/YlgaIPJJlx07SS...            æ—    \n",
       "2860                                                  æ—   ä¸‰äºšÂ·ä¸‰äºšå‡¤å‡°å›½é™…æœºåœº   \n",
       "2861                                                  æ—    ä¸‰äºšÂ·å—å±±æ–‡åŒ–æ—…æ¸¸åŒº   \n",
       "2862                                                  æ—            ä¸‰äºš   \n",
       "2863                                                  æ—            ä¸‰äºš   \n",
       "\n",
       "                  å‘å¸ƒæ—¶é—´            å‘å¸ƒå·¥å…·    ç‚¹èµæ•°   è½¬å‘æ•°   è¯„è®ºæ•°  \n",
       "0     2023-09-15 18:00       iPhone 13   4258   204   433  \n",
       "1     2023-09-14 10:17       iPhone 13   4911   104   347  \n",
       "2     2023-09-14 10:13           å¾®åšè§†é¢‘å·   6650   326   421  \n",
       "3     2023-09-11 13:11       iPhone 13  14711   107   342  \n",
       "4     2023-09-09 16:16       iPhone 13  14683   108   341  \n",
       "...                ...             ...    ...   ...   ...  \n",
       "2859  2022-01-11 09:22           å¾®åšè§†é¢‘å·   5033  1156  1177  \n",
       "2860  2022-01-08 19:55   iPhone 12 Pro   4970  1069  1142  \n",
       "2861  2022-01-03 11:33   iPhone 12 Pro   4608  1197  1225  \n",
       "2862  2022-01-02 22:40   iPhone 12 Pro   4840  1097  1147  \n",
       "2863  2022-01-01 20:32  iPhone 12 mini   4223  1015  1082  \n",
       "\n",
       "[2864 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the displayed DataFrame look correct? (yes/no): yes\n",
      "DataFrame saved as total1-4.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# æŒ‡å®šä½ çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "directory_path = '/Users/laihuiqian/Documents/weibo_0918/'\n",
    "\n",
    "# è·å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶\n",
    "files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "# ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªCSVæ–‡ä»¶\n",
    "if not files:\n",
    "    print(\"No CSV files found in the specified directory.\")\n",
    "else:\n",
    "    encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']\n",
    "    dfs = []\n",
    "\n",
    "    # å°è¯•ä½¿ç”¨å¤šç§ç¼–ç æ¥è¯»å–æ¯ä¸ªCSVæ–‡ä»¶\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        df = None\n",
    "        for enc in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=enc)\n",
    "                dfs.append(df)\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "\n",
    "    # åˆå¹¶æ‰€æœ‰çš„DataFrames\n",
    "    if dfs:\n",
    "        df_total = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # åœ¨Jupyter Notebookä¸­æ˜¾ç¤ºåˆå¹¶åçš„DataFrame\n",
    "        display(df_total)\n",
    "\n",
    "        # è¯¢é—®ç”¨æˆ·æ˜¯å¦ä¿å­˜ä¸ºExcelæ–‡ä»¶\n",
    "        save = input(\"Does the displayed DataFrame look correct? (yes/no): \")\n",
    "        if save.lower() == 'yes':\n",
    "            df_total.to_excel(os.path.join(directory_path, 'total1-4.xlsx'), index=False, engine='openpyxl')\n",
    "            print(\"DataFrame saved as total1-4.xlsx.\")\n",
    "    else:\n",
    "        print(\"Failed to read the CSV files with the provided encodings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dc36e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/tx/327gmf1d3vd2fzt8k0972vz00000gn/T/jieba.cache\n",
      "Loading model cost 0.634 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "INFO:lda:n_documents: 2864\n",
      "INFO:lda:vocab_size: 1834\n",
      "INFO:lda:n_words: 74820\n",
      "INFO:lda:n_topics: 15\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -815870\n",
      "INFO:lda:<10> log likelihood: -564061\n",
      "INFO:lda:<20> log likelihood: -538229\n",
      "INFO:lda:<30> log likelihood: -528372\n",
      "INFO:lda:<40> log likelihood: -523706\n",
      "INFO:lda:<50> log likelihood: -519468\n",
      "INFO:lda:<60> log likelihood: -517493\n",
      "INFO:lda:<70> log likelihood: -515754\n",
      "INFO:lda:<80> log likelihood: -513831\n",
      "INFO:lda:<90> log likelihood: -512958\n",
      "INFO:lda:<99> log likelihood: -511896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model done\n",
      "Topic #0:\n",
      "äº¬ä¸œ é©¬é” é“¾æ¥ ç½‘é¡µ ç›´æ’­ 00 11 ä¸€èµ· ä¸è§ä¸æ•£ å¥åº·\n",
      "Topic #1:\n",
      "åˆ†äº« å¥½ç‰© è§†é¢‘ å¾®åš å¼€ç®± å¿«ä¹ ä½ ä»¬ æ¨è zjzj å½©å¦†\n",
      "Topic #2:\n",
      "å¦†å®¹ ä»Šæ—¥ æ—¶å°š ç¾å¦† æ°›å›´ å¾®åš é£æ ¼ æ­é… look æ•´ä¸ª\n",
      "Topic #3:\n",
      "yoo å‘¨å°ä»™ æ—¥å¸¸ è§†é¢‘ å¾®åš po å°ä»™ vlog ä½ ä»¬ plog\n",
      "Topic #4:\n",
      "è§†é¢‘ å¾®åš å¼ è¿› zjzj vlog å¦†å®¹ åŒ–å¦† ä»Šå¤© å¤§å®¶ å¦‚ä½•\n",
      "Topic #5:\n",
      "é˜²æ™’ çœŸçš„ å¯ä»¥ æŠ¤è‚¤ å–œæ¬¢ æ„Ÿè§‰ è¿˜æœ‰ å¤´å‘ ä¿æ¹¿ ä¸€ä¸ª\n",
      "Topic #6:\n",
      "é©¬é” å¾®åš è¶…è¯ è§†é¢‘ æ˜æ˜Ÿ ç¬”è®° åŒ–å¦†å¸ˆ æ˜¾ç¤º ä¸€ä¸ª åœ°å›¾\n",
      "Topic #7:\n",
      "è‡ªå·± æˆ‘ä»¬ ä¸€ä¸ª æ²¡æœ‰ åŠªåŠ› çœŸçš„ å¸Œæœ› å¯ä»¥ ç”Ÿæ´» å°±æ˜¯\n",
      "Topic #8:\n",
      "çš®è‚¤ æŠ¤è‚¤ çŠ¶æ€ è‡ªå·± ä¸€å®š ä¸è¦ æ•æ„Ÿ é©¬é” è‚Œè‚¤ æ‰€ä»¥\n",
      "Topic #9:\n",
      "è§†é¢‘ å¾®åš çš®å„¿ é™ˆå¤§ åˆ†äº« pipis sharing å¥½ç‰© åŒå æŠ¤è‚¤\n",
      "Topic #10:\n",
      "è¶…è¯ äº’åŠ¨ å¤§çš® ç²‰ä¸ å¯ä»¥ è¿‘æœŸ å„ä½ ç”Ÿæ´» ä¸Šæ¦œ åˆ†äº«\n",
      "Topic #11:\n",
      "æ–°å¹´ æ°›å›´ ç¤¼ç›’ å®Œç¾ è®¡åˆ’ å•å“ å¿ƒåŠ¨ æ¸…æ–° é™å®š æ‹¿æ\n",
      "Topic #12:\n",
      "å¼ è¿› çƒ­å·´ åŒ–å¦† è¿ªä¸½ å¦†å‘ åº•å¦† æ‘„å½± ç²‰åº•æ¶² å®‹ä½³ è¿›å‘\n",
      "Topic #13:\n",
      "ç²¾å çš®è‚¤ ä¿®æŠ¤ è‚Œè‚¤ æŠ—è€ è´¨åœ° çœŸçš„ ç†¬å¤œ é¢éœœ é›…è¯—å…°é»›\n",
      "Topic #14:\n",
      "ä¸€èµ· æ—…è¡Œ ä½“éªŒ å…ç¨ ä¸‰äºš å¯ä»¥ å‘³é“ è¿™æ¬¡ å›½é™… cdf\n",
      "\n",
      "Results saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import lda\n",
    "\n",
    "# 1. è¯»å–æ•°æ®\n",
    "df = pd.read_excel(\"/Users/laihuiqian/Documents/weibo_0918/total1-4.xlsx\")\n",
    "# print(df)\n",
    "texts = df['å¾®åšæ­£æ–‡'].astype(str)  # å‡è®¾'B'åˆ—æ˜¯å¾®åšæ­£æ–‡\n",
    "\n",
    "# 2. ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯\n",
    "texts_cut = [\" \".join(jieba.cut(text)) for text in texts]\n",
    "\n",
    "# 3. æ–‡æœ¬å‘é‡åŒ–\n",
    "n_features = 5000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, stop_words='english', max_df=0.5, min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(texts_cut)\n",
    "\n",
    "# 4. LDAæ¨¡å‹è®­ç»ƒ\n",
    "n_topics = 15\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=100, random_state=1)\n",
    "model.fit(tf)\n",
    "print('Model done')\n",
    "\n",
    "# 5. æ‰“å°æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# 6. ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ†é…ä¸»é¢˜\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = [topics.argmax() for topics in doc_topic]\n",
    "\n",
    "# 7. ä¿å­˜ç»“æœ\n",
    "df.to_excel('/Users/laihuiqian/Documents/weibo_0918/total1-4_with_topics.xlsx', index=False)\n",
    "print('Results saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01d0c7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 2864\n",
      "INFO:lda:vocab_size: 1799\n",
      "INFO:lda:n_words: 73526\n",
      "INFO:lda:n_topics: 15\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -831346\n",
      "INFO:lda:<10> log likelihood: -548199\n",
      "INFO:lda:<20> log likelihood: -524687\n",
      "INFO:lda:<30> log likelihood: -516162\n",
      "INFO:lda:<40> log likelihood: -511796\n",
      "INFO:lda:<50> log likelihood: -508639\n",
      "INFO:lda:<60> log likelihood: -506839\n",
      "INFO:lda:<70> log likelihood: -505462\n",
      "INFO:lda:<80> log likelihood: -503499\n",
      "INFO:lda:<90> log likelihood: -502360\n",
      "INFO:lda:<99> log likelihood: -501703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model done\n",
      "Topic #0:\n",
      "è‡ªå·± ä¸€ä¸ª å–œæ¬¢ æ²¡æœ‰ ç”Ÿæ´» æ—¶å€™ å°±æ˜¯ æ—¶é—´ ä»€ä¹ˆ ä¸€æ ·\n",
      "Topic #1:\n",
      "è¶…è¯ äº’åŠ¨ å¤§çš® ç²‰ä¸ å¯ä»¥ è¿‘æœŸ å„ä½ å¹¸è¿ ç”Ÿæ´» ä¸Šæ¦œ\n",
      "Topic #2:\n",
      "çš®è‚¤ çœŸçš„ è‚Œè‚¤ é˜²æ™’ ç²¾å æŠ¤è‚¤ æˆåˆ† ç†¬å¤œ æ•æ„Ÿ å°±æ˜¯\n",
      "Topic #3:\n",
      "åˆ†äº« è§†é¢‘ å¾®åš çš®å„¿ é™ˆå¤§ å¥½ç‰© pipis sharing æ¨è ä»Šå¤©\n",
      "Topic #4:\n",
      "è‡ªå·± æˆ‘ä»¬ å¯ä»¥ ä¸€ä¸ª å¥åº· é€‰æ‹© åŠªåŠ› çœŸçš„ å°±æ˜¯ ç°åœ¨\n",
      "Topic #5:\n",
      "å‘¨å°ä»™ yoo è§†é¢‘ æ—¥å¸¸ å¾®åš å°ä»™ po å¼€ç®± ä½ ä»¬ åˆ†äº«\n",
      "Topic #6:\n",
      "æ–°å¹´ æ°›å›´ å¦†å®¹ ä»Šå¤© ootd ä»Šæ—¥ å®Œç¾ å¤å¤© å£çº¢ å¤æ—¥\n",
      "Topic #7:\n",
      "å¾®åš è§†é¢‘ vlog å¤§å®¶ ä½ ä»¬ å¿«ä¹ zjzj å¼ è¿› ä¸€èµ· ä»€ä¹ˆ\n",
      "Topic #8:\n",
      "é©¬é” è§†é¢‘ å¾®åš è¶…è¯ æ˜æ˜Ÿ ç¬”è®° ç›´æ’­ åŒ–å¦†å¸ˆ åŒå èŒåœº\n",
      "Topic #9:\n",
      "äº¬ä¸œ ä¿®æŠ¤ ç²¾å æŠ¤è‚¤ è¿˜æœ‰ é“¾æ¥ è´¨åœ° ç½‘é¡µ ç³»åˆ— é¢éœœ\n",
      "Topic #10:\n",
      "ä¸€èµ· æ—…è¡Œ ä½“éªŒ å¤§å®¶ å¯ä»¥ å…ç¨ å¼€å­¦ ç¾ä¸½ æˆ‘ä»¬ ä½ ä»¬\n",
      "Topic #11:\n",
      "å¼ è¿› çƒ­å·´ è¿ªä¸½ åŒ–å¦† å¦†å‘ æ˜¾ç¤º åœ°å›¾ æ‘„å½± é€ å‹ å®‹ä½³\n",
      "Topic #12:\n",
      "è§†é¢‘ å¾®åš zjzj å¼ è¿› ç¾å¦† å¥½ç‰© åŒ–å¦† å½©å¦† è…®çº¢ æŠ½å¥–\n",
      "Topic #13:\n",
      "çœŸçš„ ç¤¼ç›’ è¿˜æœ‰ è¿™æ¬¡ å‘³é“ é«˜çº§ æ„Ÿè§‰ å–œæ¬¢ ç«ç‘° å¤´å‘\n",
      "Topic #14:\n",
      "ä¸è¦ è‡ªå·± æ²¡æœ‰ å› ä¸º ä¸€å®š ä¸€ä¸ª å¯ä»¥ å¦‚æœ æˆ‘ä»¬ å¾ˆå¤š\n",
      "\n",
      "Results saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import lda\n",
    "\n",
    "# 1. è¯»å–æ•°æ®\n",
    "df = pd.read_excel(\"/Users/laihuiqian/Documents/weibo_0918/total1-4.xlsx\")\n",
    "# print(df)\n",
    "texts = df['å¾®åšæ­£æ–‡'].astype(str)  # å‡è®¾'B'åˆ—æ˜¯å¾®åšæ­£æ–‡\n",
    "\n",
    "# 2. ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯\n",
    "# texts_cut = [\" \".join(jieba.cut(text)) for text in texts]\n",
    "\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯\n",
    "    words = jieba.cut(text)\n",
    "    \n",
    "    # å»é™¤æ ‡ç‚¹å’Œç‰¹æ®Šå­—ç¬¦\n",
    "    words = [re.sub(r'[^a-zA-Z\\u4e00-\\u9fff]', '', word) for word in words]\n",
    "    \n",
    "    # å»é™¤ç‰¹å®šæ— ç”¨è¯\n",
    "    useless_terms = [\"æ˜¾ç¤ºåœ°å›¾\", \"åŸå›¾\", \"\\[ç»„å›¾å…±\\då¼ \\]\"]\n",
    "    words = [word for word in words if word not in useless_terms and len(word) > 0]\n",
    "    \n",
    "    # å»é™¤åœç”¨è¯ (è¿™é‡Œéœ€è¦ä¸€ä¸ªä¸­æ–‡çš„åœç”¨è¯åˆ—è¡¨)\n",
    "    # stopwords = set(your_stopwords_list)\n",
    "    # words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# åº”ç”¨é¢„å¤„ç†å‡½æ•°\n",
    "texts_cut = [preprocess_text(text) for text in df['å¾®åšæ­£æ–‡']]\n",
    "\n",
    "\n",
    "# 3. æ–‡æœ¬å‘é‡åŒ–\n",
    "n_features = 5000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, stop_words='english', max_df=0.5, min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(texts_cut)\n",
    "\n",
    "# 4. LDAæ¨¡å‹è®­ç»ƒ\n",
    "n_topics = 15\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=100, random_state=1)\n",
    "model.fit(tf)\n",
    "print('Model done')\n",
    "\n",
    "# 5. æ‰“å°æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# 6. ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ†é…ä¸»é¢˜\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = [topics.argmax() for topics in doc_topic]\n",
    "\n",
    "# 7. ä¿å­˜ç»“æœ\n",
    "df.to_excel('/Users/laihuiqian/Documents/weibo_0918/total1-4_with_topics.xlsx', index=False)\n",
    "print('Results saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcc6f1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 2864\n",
      "INFO:lda:vocab_size: 1799\n",
      "INFO:lda:n_words: 73526\n",
      "INFO:lda:n_topics: 15\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -831346\n",
      "INFO:lda:<10> log likelihood: -548199\n",
      "INFO:lda:<20> log likelihood: -524687\n",
      "INFO:lda:<30> log likelihood: -516162\n",
      "INFO:lda:<40> log likelihood: -511796\n",
      "INFO:lda:<50> log likelihood: -508639\n",
      "INFO:lda:<60> log likelihood: -506839\n",
      "INFO:lda:<70> log likelihood: -505462\n",
      "INFO:lda:<80> log likelihood: -503499\n",
      "INFO:lda:<90> log likelihood: -502360\n",
      "INFO:lda:<99> log likelihood: -501703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model done\n",
      "Topic #0:\n",
      "è‡ªå·± ä¸€ä¸ª å–œæ¬¢ æ²¡æœ‰ ç”Ÿæ´» æ—¶å€™ å°±æ˜¯ æ—¶é—´ ä»€ä¹ˆ ä¸€æ ·\n",
      "Topic #1:\n",
      "è¶…è¯ äº’åŠ¨ å¤§çš® ç²‰ä¸ å¯ä»¥ è¿‘æœŸ å„ä½ å¹¸è¿ ç”Ÿæ´» ä¸Šæ¦œ\n",
      "Topic #2:\n",
      "çš®è‚¤ çœŸçš„ è‚Œè‚¤ é˜²æ™’ ç²¾å æŠ¤è‚¤ æˆåˆ† ç†¬å¤œ æ•æ„Ÿ å°±æ˜¯\n",
      "Topic #3:\n",
      "åˆ†äº« è§†é¢‘ å¾®åš çš®å„¿ é™ˆå¤§ å¥½ç‰© pipis sharing æ¨è ä»Šå¤©\n",
      "Topic #4:\n",
      "è‡ªå·± æˆ‘ä»¬ å¯ä»¥ ä¸€ä¸ª å¥åº· é€‰æ‹© åŠªåŠ› çœŸçš„ å°±æ˜¯ ç°åœ¨\n",
      "Topic #5:\n",
      "å‘¨å°ä»™ yoo è§†é¢‘ æ—¥å¸¸ å¾®åš å°ä»™ po å¼€ç®± ä½ ä»¬ åˆ†äº«\n",
      "Topic #6:\n",
      "æ–°å¹´ æ°›å›´ å¦†å®¹ ä»Šå¤© ootd ä»Šæ—¥ å®Œç¾ å¤å¤© å£çº¢ å¤æ—¥\n",
      "Topic #7:\n",
      "å¾®åš è§†é¢‘ vlog å¤§å®¶ ä½ ä»¬ å¿«ä¹ zjzj å¼ è¿› ä¸€èµ· ä»€ä¹ˆ\n",
      "Topic #8:\n",
      "é©¬é” è§†é¢‘ å¾®åš è¶…è¯ æ˜æ˜Ÿ ç¬”è®° ç›´æ’­ åŒ–å¦†å¸ˆ åŒå èŒåœº\n",
      "Topic #9:\n",
      "äº¬ä¸œ ä¿®æŠ¤ ç²¾å æŠ¤è‚¤ è¿˜æœ‰ é“¾æ¥ è´¨åœ° ç½‘é¡µ ç³»åˆ— é¢éœœ\n",
      "Topic #10:\n",
      "ä¸€èµ· æ—…è¡Œ ä½“éªŒ å¤§å®¶ å¯ä»¥ å…ç¨ å¼€å­¦ ç¾ä¸½ æˆ‘ä»¬ ä½ ä»¬\n",
      "Topic #11:\n",
      "å¼ è¿› çƒ­å·´ è¿ªä¸½ åŒ–å¦† å¦†å‘ æ˜¾ç¤º åœ°å›¾ æ‘„å½± é€ å‹ å®‹ä½³\n",
      "Topic #12:\n",
      "è§†é¢‘ å¾®åš zjzj å¼ è¿› ç¾å¦† å¥½ç‰© åŒ–å¦† å½©å¦† è…®çº¢ æŠ½å¥–\n",
      "Topic #13:\n",
      "çœŸçš„ ç¤¼ç›’ è¿˜æœ‰ è¿™æ¬¡ å‘³é“ é«˜çº§ æ„Ÿè§‰ å–œæ¬¢ ç«ç‘° å¤´å‘\n",
      "Topic #14:\n",
      "ä¸è¦ è‡ªå·± æ²¡æœ‰ å› ä¸º ä¸€å®š ä¸€ä¸ª å¯ä»¥ å¦‚æœ æˆ‘ä»¬ å¾ˆå¤š\n",
      "\n",
      "Results saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import lda\n",
    "\n",
    "# 1. è¯»å–æ•°æ®\n",
    "df = pd.read_excel(\"/Users/laihuiqian/Documents/weibo_0918/total1-4.xlsx\")\n",
    "# print(df)\n",
    "texts = df['å¾®åšæ­£æ–‡'].astype(str)  # å‡è®¾'B'åˆ—æ˜¯å¾®åšæ­£æ–‡\n",
    "\n",
    "# 2. ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯\n",
    "# texts_cut = [\" \".join(jieba.cut(text)) for text in texts]\n",
    "\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯\n",
    "    words = jieba.cut(text)\n",
    "    \n",
    "    # å»é™¤æ ‡ç‚¹å’Œç‰¹æ®Šå­—ç¬¦\n",
    "    words = [re.sub(r'[^a-zA-Z\\u4e00-\\u9fff]', '', word) for word in words]\n",
    "    \n",
    "    # å»é™¤ç‰¹å®šæ— ç”¨è¯\n",
    "    useless_terms = [\"æ˜¾ç¤ºåœ°å›¾\", \"åŸå›¾\", \"\\[ç»„å›¾å…±\\då¼ \\]\"]\n",
    "    words = [word for word in words if word not in useless_terms and len(word) > 0]\n",
    "    \n",
    "    # å»é™¤åœç”¨è¯ (è¿™é‡Œéœ€è¦ä¸€ä¸ªä¸­æ–‡çš„åœç”¨è¯åˆ—è¡¨)\n",
    "    # stopwords = set(your_stopwords_list)\n",
    "    # words = [word for word in words if word not in stopwords]\n",
    "    with open('baidu_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "        stopwords = set([line.strip() for line in f])\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# åº”ç”¨é¢„å¤„ç†å‡½æ•°\n",
    "texts_cut = [preprocess_text(text) for text in df['å¾®åšæ­£æ–‡']]\n",
    "\n",
    "\n",
    "# 3. æ–‡æœ¬å‘é‡åŒ–\n",
    "n_features = 5000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, stop_words='english', max_df=0.5, min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(texts_cut)\n",
    "\n",
    "# 4. LDAæ¨¡å‹è®­ç»ƒ\n",
    "n_topics = 15\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=100, random_state=1)\n",
    "model.fit(tf)\n",
    "print('Model done')\n",
    "\n",
    "# 5. æ‰“å°æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# 6. ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ†é…ä¸»é¢˜\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = [topics.argmax() for topics in doc_topic]\n",
    "\n",
    "# 7. ä¿å­˜ç»“æœ\n",
    "df.to_excel('/Users/laihuiqian/Documents/weibo_0918/total1-4_with_topics_2.xlsx', index=False)\n",
    "print('Results saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5709777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['è‹¥æœ'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "INFO:lda:n_documents: 2864\n",
      "INFO:lda:vocab_size: 1724\n",
      "INFO:lda:n_words: 66408\n",
      "INFO:lda:n_topics: 17\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -760839\n",
      "INFO:lda:<10> log likelihood: -495089\n",
      "INFO:lda:<20> log likelihood: -472508\n",
      "INFO:lda:<30> log likelihood: -464254\n",
      "INFO:lda:<40> log likelihood: -459505\n",
      "INFO:lda:<50> log likelihood: -456637\n",
      "INFO:lda:<60> log likelihood: -454355\n",
      "INFO:lda:<70> log likelihood: -452068\n",
      "INFO:lda:<80> log likelihood: -451091\n",
      "INFO:lda:<90> log likelihood: -450118\n",
      "INFO:lda:<99> log likelihood: -449195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model done\n",
      "Topic #0:\n",
      "vlog å¾®åš æ—¥å¸¸ ç”Ÿæ´» è§†é¢‘ æ˜¾ç¤º åœ°å›¾ plog å¿«ä¹ æ—¥è®° zjzj åšä¸» ä¸€å¤© ç»ˆäº æ­å· ç¾å‡º æ˜¥å¤© å¼ è¿› ç¢ç‰‡ po\n",
      "Topic #1:\n",
      "è§†é¢‘ å¾®åš åˆ†äº« çš®å„¿ é™ˆå¤§ å¥½ç‰© pipis sharing åŒå æ¨è å¤§ä¼š æŠ¤è‚¤ ç”Ÿæ´» èº«ä½“ ä¸€ä¸ª é©¬é” ä¸€ç§ å§å¦¹ çœŸçš„ ä»Šå¤©\n",
      "Topic #2:\n",
      "ä¸€ä¸ª åŠªåŠ› å¸Œæœ› æ²¡æœ‰ ä¸è¦ ç”Ÿæ´» å·¥ä½œ æ—¶é—´ é€‰æ‹© é‡åˆ° çœŸçš„ å–œæ¬¢ æˆä¸º ç”µå½± äººç”Ÿ å¼€å¿ƒ å¯èƒ½ é©¬é” è¶…è¯ æ°¸è¿œ\n",
      "Topic #3:\n",
      "æ°›å›´ é«˜çº§ è…®çº¢ çœŸçš„ å£çº¢ æ­é… æ‹¿æ æ•´ä¸ª è®¾è®¡ çœ¼å½± å•å“ get ä¸€ç‚¹ å¤å¤© ç¤¼ç›’ å®å­ä»¬ é™å®š ç®€ç›´ å¥½çœ‹ åˆ†äº«\n",
      "Topic #4:\n",
      "å‘¨å°ä»™ yoo è§†é¢‘ å¾®åš æ—¥å¸¸ ä»Šæ—¥ å°ä»™ po ä»Šå¤© ootd å¦†å®¹ æ”»ç•¥ å¼€æ˜¥ å–œæ¬¢ å® ç²‰ å¥½é¢œ è®°å¾— ä¸€ä¸‹ æ–°ç–† è¾£å¦¹\n",
      "Topic #5:\n",
      "æ–°å¹´ å®Œç¾ å¥åº· è®¡åˆ’ å¿«ä¹ ç¤¼ç›’ 2022 ç¤¼ç‰© 2023 ä¸€å¹´ è™å¹´ è¿‡å¹´ ç¥ç¦ å“ç‰Œ åŠ æ²¹ ä¸ƒå¤• å¹¸ç¦ ä¸€èµ· 520 å¯çˆ±\n",
      "Topic #6:\n",
      "å¾®åš æ˜æ˜Ÿ é©¬é” è§†é¢‘ åŒ–å¦†å¸ˆ ç¬”è®° åˆ°åº• å¤´å‘ ç¾å¦† é€‰æ‹© ç¾å¦†çº¢äºº ç™¾å¤§ å¤´çš® æŠ½å¥– è¶…è¯ å­¦ä¹  å¾®åšç¾å¦† å‡†å¤‡ ä¿®å®¹ é€‚åˆ\n",
      "Topic #7:\n",
      "è¶…è¯ å¤§çš® äº’åŠ¨ ç²‰ä¸ è¿‘æœŸ å¹¸è¿ ä¸Šæ¦œ ç”Ÿæ´» åˆ†äº« æ—¶é—´ å‘å¸ƒ æ—¥å¸¸ åŠ å…¥ ç§ä¿¡ è‡ªæ‹ é¢†å– å…‘å¥– æ²¡æœ‰ çœ¼ç†Ÿ å®å­\n",
      "Topic #8:\n",
      "å¦†å®¹ å¾®åš è§†é¢‘ åŒ–å¦† å¼ è¿› zjzj é€ å‹ ä»Šæ—¥ ä»Šå¤© ç²‰åº•æ¶² ç¾å¦† å¿ƒåŠ¨ æ‰“é€  èµ·æ¥ ä¸ç»’ çœ¼å¦† åº•å¦† ç«‹ä½“ ä¸€èµ· å¥¢å…‰\n",
      "Topic #9:\n",
      "æ²¡æœ‰ ä¸€å®š æœ‹å‹ ä¸€ä¸ª çœŸçš„ ç°åœ¨ å“ç‰Œ è§‰å¾— ç‰¹åˆ« æœ€è¿‘ è¿åŠ¨ å°è¯• å›½è´§ ä¸­å›½ ä¸€èµ· éå¸¸ æœ€å çŸ¥é“ çœ‹åˆ° å…¶å®\n",
      "Topic #10:\n",
      "å¼ è¿› çƒ­å·´ è¿ªä¸½ åŒ–å¦† å¦†å‘ æ‘„å½± å®‹ä½³ è¿›å‘ å°é¢ ä»£è¨€äºº å¹´åº¦ å“ç‰Œ å½©å¦† å¤§ç‰‡ é›ªäº® æå¿—è¾‰ å…¨çƒ æ˜¥æ—¥ å¼ å©§ä»ª ç›´æ’­\n",
      "Topic #11:\n",
      "ä¿®æŠ¤ çœŸçš„ é˜²æ™’ ç²¾å è´¨åœ° ç†¬å¤œ é¢éœœ æ•ˆæœ ä¸‹æ¥ çš®è‚¤ æŠ—è€ ç°åœ¨ é¢è†œ æ•´ä¸ª ç›´æ¥ ç»†è…» ä¿æ¹¿ ä¸€ä¸ª å–œæ¬¢ å¸æ”¶\n",
      "Topic #12:\n",
      "ä¸€èµ· å¤æ—¥ å–œæ¬¢ å‘³é“ ç«ç‘° åä¸º è‡ªç„¶ çœŸçš„ é£æ ¼ æ„Ÿå— é¦™æ°´ ç¾å¥½ ootd æ¸…æ–° æ„Ÿè§‰ æ—¶å°š look ä¸åŒ é¦™æ°› è§£é”\n",
      "Topic #13:\n",
      "è§†é¢‘ åˆ†äº« å¾®åš å¥½ç‰© å¼€ç®± zjzj å¼ è¿› yoo æ˜¥å¤ å‘¨å°ä»™ æŠ½å¥– æŠ¤è‚¤ ä¸€èµ· è¯¦æƒ… å¿«ä¹ ç§è‰ ä»Šå¤© å¤å¤© çœ‹çœ‹ å¤æ—¥\n",
      "Topic #14:\n",
      "é©¬é” è§†é¢‘ è¶…è¯ å¾®åš ç›´æ’­ èŒåœº 00 ä¸è§ä¸æ•£ å¼€å­¦ å˜ç¾ å«è§† ç®€å• ä»Šå¤© 21 20 ä»Šæ™š ç„¦è™‘ å¤©æ´¥ 10 å“ˆå“ˆå“ˆ\n",
      "Topic #15:\n",
      "çš®è‚¤ æŠ¤è‚¤ è‚Œè‚¤ ç²¾å çŠ¶æ€ æ•æ„Ÿ æŠ—è€ æˆåˆ† ä½“éªŒ åšæŒ é‡Œé¢ èµ·æ¥ äº§å“ ç¾ç™½ æŠ¤è‚¤å“ ä¸€ç›´ å¥åº· å®¹æ˜“ æå‡ é€äº®\n",
      "Topic #16:\n",
      "äº¬ä¸œ é“¾æ¥ ä¸€èµ· ç½‘é¡µ è¶…çº§ å…ç¨ ç”Ÿæ´» å¼€å¯ ä¸‰äºš æ´»åŠ¨ 11 ç¾å¦† çº¢äººèŠ‚ æƒŠå–œ å¤©çŒ« cdf è¶…å¤š å›½é™… 618 äº«å—\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# åŠ è½½åœç”¨è¯\n",
    "def load_stopwords():\n",
    "    # è¿™é‡Œä½¿ç”¨äº†æˆ‘ä¹‹å‰æä¾›çš„åœç”¨è¯é“¾æ¥ï¼Œä½ å¯ä»¥æ›¿æ¢ä¸ºè‡ªå·±çš„é“¾æ¥æˆ–æœ¬åœ°è·¯å¾„\n",
    "    url = \"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\"\n",
    "    return pd.read_csv(url, header=None, sep=\"\\t\", quoting=3, names=['stopword'], encoding='utf-8').stopword.tolist()\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# åˆ†è¯å¹¶å»é™¤åœç”¨è¯\n",
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join([word for word in jieba.cut(mytext) if word not in stopwords])\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "df = pd.read_excel(\"/Users/laihuiqian/Documents/weibo_0918/total1-4.xlsx\")\n",
    "df[\"content_cutted\"] = df['å¾®åšæ­£æ–‡'].astype(str).apply(chinese_word_cut)\n",
    "\n",
    "n_features = 5000\n",
    "\n",
    "tf_vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords,\n",
    "                                max_df=0.5,\n",
    "                                min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(df.content_cutted)\n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDAå»ºæ¨¡\n",
    "model = lda.LDA(n_topics=17, n_iter=100, random_state=1)\n",
    "model.fit(tf)\n",
    "print('model done')\n",
    "\n",
    "# æ‰“å°æ¯ä¸ªä¸»é¢˜çš„å‰20ä¸ªå…³é”®è¯\n",
    "def print_top_words(model, feature_names, n_top_words=20):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# å°†æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒä¿å­˜åˆ°Excel\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = doc_topic.argmax(axis=1)\n",
    "df.to_excel('/Users/laihuiqian/Documents/weibo_0918/total1-4_topics.xlsx', sheet_name='Sheet1')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68c0ccae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:collected 13222 word types from a corpus of 193660 raw words and 2864 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 13222 unique words (100.00% of original 13222, drops 0)', 'datetime': '2023-09-18T14:34:11.377585', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 193660 word corpus (100.00% of original 193660, drops 0)', 'datetime': '2023-09-18T14:34:11.378314', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 13222 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 19 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 139445.79549075436 word corpus (72.0%% of prior 193660)', 'datetime': '2023-09-18T14:34:11.443229', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 13222 words and 100 dimensions: 17188600 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-09-18T14:34:11.551766', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 4 workers on 13222 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-09-18T14:34:11.552503', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 193660 raw words (139530 effective words) took 0.1s, 2276904 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 193660 raw words (139424 effective words) took 0.1s, 2398712 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 193660 raw words (139510 effective words) took 0.1s, 2406548 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 193660 raw words (139399 effective words) took 0.1s, 2434729 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 193660 raw words (139517 effective words) took 0.1s, 2307762 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 968300 raw words (697380 effective words) took 0.3s, 2241517 effective words/s', 'datetime': '2023-09-18T14:34:11.864024', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=13222, vector_size=100, alpha=0.025>', 'datetime': '2023-09-18T14:34:11.864442', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# åŠ è½½åœç”¨è¯\n",
    "def load_stopwords():\n",
    "    url = \"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\"\n",
    "    return pd.read_csv(url, header=None, sep=\"\\t\", quoting=3, names=['stopword'], encoding='utf-8').stopword.tolist()\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# åˆ†è¯å¹¶å»é™¤åœç”¨è¯\n",
    "def chinese_word_cut(mytext):\n",
    "    return [word for word in jieba.cut(mytext) if word not in stopwords]\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "df = pd.read_excel(\"/Users/laihuiqian/Documents/weibo_0918/total1-4.xlsx\")\n",
    "df[\"content_cutted\"] = df['å¾®åšæ­£æ–‡'].astype(str).apply(chinese_word_cut)\n",
    "\n",
    "# ä½¿ç”¨Word2Vecç”ŸæˆåµŒå…¥å‘é‡\n",
    "model_w2v = Word2Vec(df[\"content_cutted\"], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "# å°†å¾®åšæ­£æ–‡è½¬æ¢ä¸ºå‘é‡\n",
    "def document_vector(doc):\n",
    "    return np.mean([model_w2v.wv[word] for word in doc if word in model_w2v.wv.key_to_index], axis=0)\n",
    "\n",
    "df['vector'] = df[\"content_cutted\"].apply(document_vector)\n",
    "\n",
    "\n",
    "# ä½¿ç”¨KMeansè¿›è¡Œèšç±»\n",
    "num_clusters = 17\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=1)\n",
    "df['topic'] = kmeans.fit_predict(list(df['vector']))\n",
    "\n",
    "# ä¿å­˜åˆ°Excel\n",
    "df.to_excel('/Users/laihuiqian/Documents/weibo_0918/total1-4_topics_w2v.xlsx', sheet_name='Sheet1')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76fb91a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:collected 13222 word types from a corpus of 193660 raw words and 2864 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 13222 unique words (100.00% of original 13222, drops 0)', 'datetime': '2023-09-18T14:36:06.551803', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 193660 word corpus (100.00% of original 193660, drops 0)', 'datetime': '2023-09-18T14:36:06.552465', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 13222 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 19 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 139445.79549075436 word corpus (72.0%% of prior 193660)', 'datetime': '2023-09-18T14:36:06.618854', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 13222 words and 100 dimensions: 17188600 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-09-18T14:36:06.723475', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 4 workers on 13222 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-09-18T14:36:06.724135', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 193660 raw words (139439 effective words) took 0.1s, 2291894 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 193660 raw words (139441 effective words) took 0.1s, 2225543 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 193660 raw words (139380 effective words) took 0.1s, 2231632 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 193660 raw words (139420 effective words) took 0.1s, 2155766 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 193660 raw words (139419 effective words) took 0.1s, 2197305 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 968300 raw words (697099 effective words) took 0.3s, 2106905 effective words/s', 'datetime': '2023-09-18T14:36:07.055400', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=13222, vector_size=100, alpha=0.025>', 'datetime': '2023-09-18T14:36:07.055829', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Â  ç»„å›¾ æ·±æµ· [ åŒ–å¦† å…± ] å¦†å‘ å¼  åŸå›¾\n",
      "Topic 1: ä¸è§ä¸æ•£   æ±‚èŒ â€¦ ç‹å¿ƒå‡Œ å¦†å‘ æŠ–éŸ³ ï½ Â  ç”œ\n",
      "Topic 2: æ±‚èŒ ï½ ç”œ æŠ–éŸ³ ä¸è§ä¸æ•£ ğŸŒŒ å‘å‹ é«˜æ‰‹ æ‰¾é©¬é” â€¦\n",
      "Topic 3: Â  å¦†å‘ åŒ–å¦† æ·±æµ· ç»„å›¾ [ èµµä¸½é¢– å…± ] æ‘„å½±\n",
      "Topic 4: ~ ä¹‹å¤œ ï½ å¿«ä¹ è´´ æˆ›çº³ ç›´æ’­ é«˜æ‰‹ å½• å†°é›ª\n",
      "Topic 5: èµµä¸½é¢– [ â€¦ å¦†å‘ Â  åŒ–å¦† ç»„å›¾ æ·±æµ· å­£ å¿«ä¹\n",
      "Topic 6: èµµä¸½é¢– å¦†å‘ åŒ–å¦† â€¦ æ‘„å½± Â  [ ç»„å›¾ æ·±æµ· æ—¶è£…å‘¨\n",
      "Topic 7: Â  [ ç»„å›¾ æ·±æµ· åŒ–å¦† å¦†å‘ å…± ] èµµä¸½é¢– å§œé€š\n",
      "Topic 8: ç›´æ’­ â€¦ å¼ è¿› å¿«ä¹ ï½ å¸¦ ä¸è§ä¸æ•£ æ±‚èŒ æŠ–éŸ³ å­£\n",
      "Topic 9: [ èµµä¸½é¢– ç»„å›¾ Â  åŒ–å¦† æ·±æµ· å¦†å‘ â€¦ å­£ å…±\n",
      "Topic 10: å¼ å©§ä»ª å‘å‹ å¤§ç‰‡ åŒ—äº¬ ç”œ åŠ æ²¹ ç”µå½± 11 é«˜æ‰‹ ç”Ÿæ—¥å¿«ä¹\n",
      "Topic 11: å¼ è¿› â€¦ ç›´æ’­ ä¸è§ä¸æ•£ èµµä¸½é¢– å¾®åš æŠ–éŸ³ æ±‚èŒ ï½ å¿«ä¹\n",
      "Topic 12: ç›´æ’­ ï½ å¿«ä¹ æ±‚èŒ ~ â€¦ æŠ–éŸ³ å¸¦ å½• ä¹‹å¤œ\n",
      "Topic 13: å¦†å‘ Â  åŒ–å¦† æ·±æµ· ç»„å›¾ [ èµµä¸½é¢– æ‘„å½± å…± ]\n",
      "Topic 14: Â  å¦†å‘   æ·±æµ· ä¸è§ä¸æ•£ â€¦ åŒ–å¦† [ ç»„å›¾ ç‹å¿ƒå‡Œ\n",
      "Topic 15: ä¸è§ä¸æ•£ æ±‚èŒ ï½ æŠ–éŸ³ â€¦ ç”œ ç‹å¿ƒå‡Œ å¦†å‘ ç›´æ’­ ğŸŒŒ\n",
      "Topic 16: èµµä¸½é¢– æ‘„å½± â€¦ å¿«ä¹ æƒ³è§ å¦†å‘ TOMFORDBEAUTY æ—¶è£…å‘¨ åŒ–å¦† ç”œ\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# åŠ è½½åœç”¨è¯\n",
    "def load_stopwords():\n",
    "    url = \"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\"\n",
    "    return pd.read_csv(url, header=None, sep=\"\\t\", quoting=3, names=['stopword'], encoding='utf-8').stopword.tolist()\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# åˆ†è¯å¹¶å»é™¤åœç”¨è¯\n",
    "def chinese_word_cut(mytext):\n",
    "    return [word for word in jieba.cut(mytext) if word not in stopwords]\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "df = pd.read_excel(\"/Users/laihuiqian/Documents/weibo_0918/total1-4.xlsx\")\n",
    "df[\"content_cutted\"] = df['å¾®åšæ­£æ–‡'].astype(str).apply(chinese_word_cut)\n",
    "\n",
    "# ä½¿ç”¨Word2Vecç”ŸæˆåµŒå…¥å‘é‡\n",
    "model_w2v = Word2Vec(sentences=df[\"content_cutted\"], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# å°†å¾®åšæ­£æ–‡è½¬æ¢ä¸ºå‘é‡\n",
    "def document_vector(doc):\n",
    "    return np.mean([model_w2v.wv[word] for word in doc if word in model_w2v.wv.key_to_index], axis=0)\n",
    "\n",
    "df['vector'] = df[\"content_cutted\"].apply(document_vector)\n",
    "\n",
    "# ä½¿ç”¨KMeansè¿›è¡Œèšç±»\n",
    "num_clusters = 17\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=1)\n",
    "df['topic'] = kmeans.fit_predict(list(df['vector']))\n",
    "\n",
    "# æ‰“å°æ¯ä¸ªä¸»é¢˜çš„æè¿°\n",
    "for i in range(num_clusters):\n",
    "    center = kmeans.cluster_centers_[i]\n",
    "    most_similar = model_w2v.wv.most_similar(positive=[center], topn=10)\n",
    "    words = [word[0] for word in most_similar]\n",
    "    print(f\"Topic {i}: {' '.join(words)}\")\n",
    "\n",
    "# ä¿å­˜åˆ°Excel\n",
    "df.to_excel('/Users/laihuiqian/Documents/weibo_0918/total1-4_topics_w2v.xlsx', sheet_name='Sheet1')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "672f66a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['è‹¥æœ'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "å‘¨å°ä»™ yoo è§†é¢‘ å¾®åš è¶…è¯ å‘¨å¯† çº¢äººèŠ‚ å¼€ç®± è¶…çº§ éšæ‰‹\n",
      "Topic #1:\n",
      "ç»„å›¾ ç¾å¥½ ç¾å‡º æœ€è¿‘ å¿«ä¹ ç”Ÿæ—¥å¿«ä¹ æ˜¥å¤© å¯çˆ± å¤§ä¼š è¶…è¯\n",
      "Topic #2:\n",
      "çƒ­å·´ è¿ªä¸½ å¼ è¿› åŒ–å¦† æå¿—è¾‰ ä»£è¨€äºº ç›´æ’­é—´ ç›´æ’­ å¦†å‘ è¿›å‘\n",
      "Topic #3:\n",
      "é©¬é” è¶…è¯ ç›´æ’­ æ˜æ˜Ÿ è§†é¢‘ ç¬”è®° å¾®åš åŒ–å¦†å¸ˆ å˜ç¾ ä¸è§ä¸æ•£\n",
      "Topic #4:\n",
      "po æ—¥å¸¸ å°ä»™ æ—¥è®° åšä¸» æ°›å›´ plog æ„Ÿå‡º éšæœº å–œæ¬¢\n",
      "Topic #5:\n",
      "å¥½ç‰© åˆ†äº« æ¨è å¼€ç®± æœ€è¿‘ è´­ç‰© æ”¶åˆ° è¯¦æƒ… æŠ½å¥– å®è´\n",
      "Topic #6:\n",
      "å¦†å‘ å¼ è¿› å®‹ä½³ å“ç‰Œ å¼ å©§ä»ª ä»£è¨€äºº æ‘„å½± ç»„å›¾ å…¨çƒ ä¼˜é›…\n",
      "Topic #7:\n",
      "zjzj å¼ è¿› è§†é¢‘ å¾®åš è¯¦æƒ… æŠ½å¥– è¿›å® ç¾å¦† åŒ–å¦† æ•™ç¨‹\n",
      "Topic #8:\n",
      "çš®è‚¤ æŠ¤è‚¤ ç²¾å çœŸçš„ è‚Œè‚¤ ä¿®æŠ¤ äº¬ä¸œ æŠ—è€ ä¸€ä¸ª çŠ¶æ€\n",
      "Topic #9:\n",
      "vlog å¿«ä¹ ç”Ÿæ´» ä¸€å¤© ä¸€èµ· å·¥ä½œ ä¸€ä¸ª è®°å½• å¤§èµ› å¸Œæœ›\n",
      "Topic #10:\n",
      "æ˜¾ç¤º åœ°å›¾ æ­å· å¹¿å· ä¸‰äºš åŒ—äº¬ å›½é™… æ³•å›½ æ½®æµ ç»„å›¾\n",
      "Topic #11:\n",
      "æ–°å¹´ å®Œç¾ è®¡åˆ’ å¼€ç®± å¥½é¢œ å¼€æ˜¥ æ–°å¹´å¥½ æ”»ç•¥ è™å¹´ å¼€è¿\n",
      "Topic #12:\n",
      "çš®å„¿ é™ˆå¤§ pipis sharing è§†é¢‘ å¾®åš ç¾å‡º å˜ç¾ åˆ†äº« å§å¦¹\n",
      "Topic #13:\n",
      "å¦†å®¹ ä»Šæ—¥ ç¾å¦† è…®çº¢ æ—¶å°š å†°é›ª å¤å¤ motd ä»Šå¤© åº•å¦†\n",
      "Topic #14:\n",
      "ootd ä»Šå¤© å¤æ—¥ è¾£å¦¹ ä¸€ä¸‹ å¤å¤© look æ€’æ”¾ æ˜¥å¤© å‡ºè¡—\n",
      "Topic #15:\n",
      "plog ä¸€æ¡ å‘Šåˆ« ç¢ç‰‡ é—ªå…‰ å¯»æ‰¾ ç»„å›¾ æ—¥å¸¸ åšä¸» æ—¥è®°\n",
      "Topic #16:\n",
      "åŒ–å¦† å°é¢ è¿›å‘ æ‘„å½± é›ªäº® å¤§ç‰‡ å®‹ä½³ æå¿—è¾‰ ç»„å›¾ å‘å‹\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# åŠ è½½åœç”¨è¯\n",
    "def load_stopwords():\n",
    "    url = \"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\"\n",
    "    return pd.read_csv(url, header=None, sep=\"\\t\", quoting=3, names=['stopword'], encoding='utf-8').stopword.tolist()\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# åˆ†è¯å¹¶å»é™¤åœç”¨è¯\n",
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join([word for word in jieba.cut(mytext) if word not in stopwords])\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "df = pd.read_excel(\"/Users/laihuiqian/Documents/weibo_0918/total1-4.xlsx\")\n",
    "# df[\"content_cutted\"] = df['å¾®åšæ­£æ–‡'].astype(str).apply(chinese_word_cut)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯\n",
    "    words = jieba.cut(text)\n",
    "    \n",
    "    # å»é™¤æ ‡ç‚¹å’Œç‰¹æ®Šå­—ç¬¦\n",
    "    words = [re.sub(r'[^a-zA-Z\\u4e00-\\u9fff]', '', word) for word in words]\n",
    "    \n",
    "    # å»é™¤ç‰¹å®šæ— ç”¨è¯\n",
    "    useless_terms = [\"æ˜¾ç¤ºåœ°å›¾\", \"åŸå›¾\", \"\\[ç»„å›¾å…±\\då¼ \\]\"]\n",
    "    words = [word for word in words if word not in useless_terms and len(word) > 0]\n",
    "    \n",
    "    # å»é™¤åœç”¨è¯ (è¿™é‡Œéœ€è¦ä¸€ä¸ªä¸­æ–‡çš„åœç”¨è¯åˆ—è¡¨)\n",
    "    # stopwords = set(your_stopwords_list)\n",
    "    # words = [word for word in words if word not in stopwords]\n",
    "    with open('baidu_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "        stopwords = set([line.strip() for line in f])\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# åº”ç”¨é¢„å¤„ç†å‡½æ•°\n",
    "df[\"content_cutted\"] = [preprocess_text(text) for text in df['å¾®åšæ­£æ–‡']]\n",
    "\n",
    "# ä½¿ç”¨TF-IDFè¿›è¡Œå‘é‡åŒ–\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words=stopwords)\n",
    "tfidf = tfidf_vectorizer.fit_transform(df[\"content_cutted\"])\n",
    "\n",
    "# ä½¿ç”¨NMFè¿›è¡Œä¸»é¢˜å»ºæ¨¡\n",
    "num_topics = 17\n",
    "nmf = NMF(n_components=num_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "\n",
    "\n",
    "# æ‰“å°æ¯ä¸ªä¸»é¢˜çš„å‰10ä¸ªå…³é”®è¯\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "print_top_words(nmf, tfidf_vectorizer.get_feature_names())  # ä¿®æ”¹è¿™é‡Œ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# å°†æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒä¿å­˜åˆ°Excel\n",
    "doc_topic = nmf.transform(tfidf)\n",
    "df['topic'] = doc_topic.argmax(axis=1)\n",
    "df.to_excel('/Users/laihuiqian/Documents/weibo_0918/total1-4_topics_nmf.xlsx', sheet_name='Sheet1')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386586f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
