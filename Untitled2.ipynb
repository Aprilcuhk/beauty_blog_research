{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13424659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files merged and saved as total1-4.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_csvs_in_directory(directory_path, output_filename):\n",
    "    # 列出目录中的所有文件\n",
    "    files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "    # 确保至少有一个CSV文件\n",
    "    if not files:\n",
    "        print(\"No CSV files found in the specified directory.\")\n",
    "        return\n",
    "\n",
    "    # 读取第一个CSV文件\n",
    "    df_total = pd.read_csv(os.path.join(directory_path, files[0]), encoding='utf-8')\n",
    "\n",
    "    # 读取并合并其余的CSV文件\n",
    "    for file in files[1:]:\n",
    "        df = pd.read_csv(os.path.join(directory_path, file), encoding='utf-8')\n",
    "        df_total = pd.concat([df_total, df], ignore_index=True)\n",
    "\n",
    "    # 保存合并后的CSV文件\n",
    "    df_total.to_csv(os.path.join(directory_path, output_filename), index=False, encoding='utf-8')\n",
    "    print(f\"CSV files merged and saved as {output_filename}.\")\n",
    "\n",
    "# 使用方法\n",
    "directory_path = '/Users/laihuiqian/Documents/weibo_0918/'  # 替换为你的文件夹路径\n",
    "output_filename = 'total1-4.csv'\n",
    "merge_csvs_in_directory(directory_path, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72b46426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files merged and saved as total1-4.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result['encoding']\n",
    "\n",
    "def merge_csvs_in_directory(directory_path, output_filename):\n",
    "    # 列出目录中的所有文件\n",
    "    files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "    # 确保至少有一个CSV文件\n",
    "    if not files:\n",
    "        print(\"No CSV files found in the specified directory.\")\n",
    "        return\n",
    "\n",
    "    # 读取第一个CSV文件\n",
    "    encoding = detect_encoding(os.path.join(directory_path, files[0]))\n",
    "    df_total = pd.read_csv(os.path.join(directory_path, files[0]), encoding=encoding)\n",
    "\n",
    "    # 读取并合并其余的CSV文件\n",
    "    for file in files[1:]:\n",
    "        encoding = detect_encoding(os.path.join(directory_path, file))\n",
    "        df = pd.read_csv(os.path.join(directory_path, file), encoding=encoding)\n",
    "        df_total = pd.concat([df_total, df], ignore_index=True)\n",
    "\n",
    "    # 保存合并后的CSV文件\n",
    "    df_total.to_csv(os.path.join(directory_path, output_filename), index=False, encoding='utf-8')\n",
    "    print(f\"CSV files merged and saved as {output_filename}.\")\n",
    "\n",
    "# 使用方法\n",
    "directory_path = '/Users/laihuiqian/Documents/weibo_0918/'  # 替换为你的文件夹路径\n",
    "output_filename = 'total1-4.csv'\n",
    "merge_csvs_in_directory(directory_path, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff7af80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files merged and saved as total1-4.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result['encoding']\n",
    "\n",
    "def convert_to_utf8(file_path):\n",
    "    encoding = detect_encoding(file_path)\n",
    "    df = pd.read_csv(file_path, encoding=encoding)\n",
    "    df.to_csv(file_path, index=False, encoding='utf-8')\n",
    "\n",
    "def merge_csvs_in_directory(directory_path, output_filename):\n",
    "    # 列出目录中的所有文件\n",
    "    files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "    # 确保至少有一个CSV文件\n",
    "    if not files:\n",
    "        print(\"No CSV files found in the specified directory.\")\n",
    "        return\n",
    "\n",
    "    # 转换每个CSV文件为utf-8编码\n",
    "    for file in files:\n",
    "        convert_to_utf8(os.path.join(directory_path, file))\n",
    "\n",
    "    # 读取第一个CSV文件\n",
    "    df_total = pd.read_csv(os.path.join(directory_path, files[0]), encoding='utf-8')\n",
    "\n",
    "    # 读取并合并其余的CSV文件\n",
    "    for file in files[1:]:\n",
    "        df = pd.read_csv(os.path.join(directory_path, file), encoding='utf-8')\n",
    "        df_total = pd.concat([df_total, df], ignore_index=True)\n",
    "\n",
    "    # 保存合并后的CSV文件\n",
    "    df_total.to_csv(os.path.join(directory_path, output_filename), index=False, encoding='utf-8')\n",
    "    print(f\"CSV files merged and saved as {output_filename}.\")\n",
    "\n",
    "# 使用方法\n",
    "directory_path = '/Users/laihuiqian/Documents/weibo_0918/'  # 替换为你的文件夹路径\n",
    "output_filename = 'total1-4.csv'\n",
    "merge_csvs_in_directory(directory_path, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7795fa6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 4: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-90e4de15067c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mfirst_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdf_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# 在Jupyter Notebook中打印DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8d in position 4: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result['encoding']\n",
    "\n",
    "# 指定你的文件夹路径\n",
    "directory_path = '/Users/laihuiqian/Documents/weibo_0918/'  # 替换为你的文件夹路径\n",
    "\n",
    "# 获取文件夹中的所有CSV文件\n",
    "files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "# 确保至少有一个CSV文件\n",
    "if not files:\n",
    "    print(\"No CSV files found in the specified directory.\")\n",
    "else:\n",
    "    # 读取第一个CSV文件\n",
    "    first_file_path = os.path.join(directory_path, files[0])\n",
    "    encoding = detect_encoding(first_file_path)\n",
    "    df_first = pd.read_csv(first_file_path, encoding=encoding)\n",
    "\n",
    "    # 在Jupyter Notebook中打印DataFrame\n",
    "    display(df_first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98daeb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>微博id</th>\n",
       "      <th>微博正文</th>\n",
       "      <th>头条文章url</th>\n",
       "      <th>原始图片url</th>\n",
       "      <th>微博视频url</th>\n",
       "      <th>发布位置</th>\n",
       "      <th>发布时间</th>\n",
       "      <th>发布工具</th>\n",
       "      <th>点赞数</th>\n",
       "      <th>转发数</th>\n",
       "      <th>评论数</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Njql4rGWs</td>\n",
       "      <td>身体护理好物分享 姐妹们，身体头皮的护理和我们面部一样重要哦！从头到脚都不能放过！ 最近一个...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/006tjdECly1hhxf7go...</td>\n",
       "      <td>无</td>\n",
       "      <td>无</td>\n",
       "      <td>2023-09-15 18:00</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>4258</td>\n",
       "      <td>204</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NjdSIx7mr</td>\n",
       "      <td>永远自由如风~#今天穿什么##ootd# #微博变美手册# #种草# 博尔塔拉·赛里木湖  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww1.sinaimg.cn/large/006tjdECly1hhvw7vp...</td>\n",
       "      <td>无</td>\n",
       "      <td>博尔塔拉·赛里木湖</td>\n",
       "      <td>2023-09-14 10:17</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>4911</td>\n",
       "      <td>104</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NjdQKvWzj</td>\n",
       "      <td>PIPIs SHARING | 早秋护肤分享秋天的第一份护肤品请查收～#好物分享##好物推荐...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>无</td>\n",
       "      <td>https://f.video.weibocdn.com/o0/ZJkVnEb8lx088A...</td>\n",
       "      <td>无</td>\n",
       "      <td>2023-09-14 10:13</td>\n",
       "      <td>微博视频号</td>\n",
       "      <td>6650</td>\n",
       "      <td>326</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NiMJzE4VZ</td>\n",
       "      <td>小bus也太可爱了吧！#好物推荐##好物分享#  [组图共9张] 原图</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww2.sinaimg.cn/large/006tjdECly1hhsjzmd...</td>\n",
       "      <td>无</td>\n",
       "      <td>无</td>\n",
       "      <td>2023-09-11 13:11</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>14711</td>\n",
       "      <td>107</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Niv5EnIuE</td>\n",
       "      <td>与生命力对话#OOTD##今天穿什么# #微博变美手册# #种草#  [组图共9张] 原图</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/006tjdECly1hhqeg01...</td>\n",
       "      <td>无</td>\n",
       "      <td>无</td>\n",
       "      <td>2023-09-09 16:16</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>14683</td>\n",
       "      <td>108</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>L9egkogzS</td>\n",
       "      <td>又是粉粉的一天💗#虎年开运穿搭#  [组图共9张] 原图</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/006tjdECly1gy1tx17...</td>\n",
       "      <td>无</td>\n",
       "      <td>无</td>\n",
       "      <td>2022-01-04 19:24</td>\n",
       "      <td>无</td>\n",
       "      <td>36022</td>\n",
       "      <td>107</td>\n",
       "      <td>832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>L8Wj6l13P</td>\n",
       "      <td>家人们，新年快乐！  [组图共6张] 原图</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww2.sinaimg.cn/large/006tjdECly1gxznsb2...</td>\n",
       "      <td>无</td>\n",
       "      <td>无</td>\n",
       "      <td>2022-01-02 21:41</td>\n",
       "      <td>无</td>\n",
       "      <td>30324</td>\n",
       "      <td>511</td>\n",
       "      <td>1181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>L8T3dhl9n</td>\n",
       "      <td>过年了🧨Thxs❤️  [组图共18张] 原图</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww2.sinaimg.cn/large/006tjdECly1gxz95ld...</td>\n",
       "      <td>无</td>\n",
       "      <td>无</td>\n",
       "      <td>2022-01-02 13:24</td>\n",
       "      <td>无</td>\n",
       "      <td>5879</td>\n",
       "      <td>310</td>\n",
       "      <td>931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>L8Nc9gDC5</td>\n",
       "      <td>12月的三亚，满足了我对海岛的一切幻想！前几天组团和姐妹一起去三亚过冬啦，每次来必逛的“景点...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww3.sinaimg.cn/large/0060cLBFly8gxyg3f1...</td>\n",
       "      <td>无</td>\n",
       "      <td>无</td>\n",
       "      <td>2022-01-01 22:30</td>\n",
       "      <td>微博  weibo.com</td>\n",
       "      <td>31299</td>\n",
       "      <td>203</td>\n",
       "      <td>1127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>L8HTBd99L</td>\n",
       "      <td>#陈大皮儿[超话]#新年快乐宝贝们！！！[话筒]十二月PIPI粉丝互动榜和超话粉群眼熟榜新鲜...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww2.sinaimg.cn/large/006tjdECly1gxxw2dk...</td>\n",
       "      <td>无</td>\n",
       "      <td>无</td>\n",
       "      <td>2022-01-01 09:00</td>\n",
       "      <td>陈大皮儿超话</td>\n",
       "      <td>33399</td>\n",
       "      <td>114</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>308 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          微博id                                               微博正文  头条文章url  \\\n",
       "0    Njql4rGWs  身体护理好物分享 姐妹们，身体头皮的护理和我们面部一样重要哦！从头到脚都不能放过！ 最近一个...      NaN   \n",
       "1    NjdSIx7mr  永远自由如风~#今天穿什么##ootd# #微博变美手册# #种草# 博尔塔拉·赛里木湖  ...      NaN   \n",
       "2    NjdQKvWzj  PIPIs SHARING | 早秋护肤分享秋天的第一份护肤品请查收～#好物分享##好物推荐...      NaN   \n",
       "3    NiMJzE4VZ               小bus也太可爱了吧！#好物推荐##好物分享#  [组图共9张] 原图       NaN   \n",
       "4    Niv5EnIuE     与生命力对话#OOTD##今天穿什么# #微博变美手册# #种草#  [组图共9张] 原图       NaN   \n",
       "..         ...                                                ...      ...   \n",
       "303  L9egkogzS                      又是粉粉的一天💗#虎年开运穿搭#  [组图共9张] 原图       NaN   \n",
       "304  L8Wj6l13P                             家人们，新年快乐！  [组图共6张] 原图       NaN   \n",
       "305  L8T3dhl9n                           过年了🧨Thxs❤️  [组图共18张] 原图       NaN   \n",
       "306  L8Nc9gDC5  12月的三亚，满足了我对海岛的一切幻想！前几天组团和姐妹一起去三亚过冬啦，每次来必逛的“景点...      NaN   \n",
       "307  L8HTBd99L  #陈大皮儿[超话]#新年快乐宝贝们！！！[话筒]十二月PIPI粉丝互动榜和超话粉群眼熟榜新鲜...      NaN   \n",
       "\n",
       "                                               原始图片url  \\\n",
       "0    http://ww4.sinaimg.cn/large/006tjdECly1hhxf7go...   \n",
       "1    http://ww1.sinaimg.cn/large/006tjdECly1hhvw7vp...   \n",
       "2                                                    无   \n",
       "3    http://ww2.sinaimg.cn/large/006tjdECly1hhsjzmd...   \n",
       "4    http://ww4.sinaimg.cn/large/006tjdECly1hhqeg01...   \n",
       "..                                                 ...   \n",
       "303  http://ww4.sinaimg.cn/large/006tjdECly1gy1tx17...   \n",
       "304  http://ww2.sinaimg.cn/large/006tjdECly1gxznsb2...   \n",
       "305  http://ww2.sinaimg.cn/large/006tjdECly1gxz95ld...   \n",
       "306  http://ww3.sinaimg.cn/large/0060cLBFly8gxyg3f1...   \n",
       "307  http://ww2.sinaimg.cn/large/006tjdECly1gxxw2dk...   \n",
       "\n",
       "                                               微博视频url       发布位置  \\\n",
       "0                                                    无          无   \n",
       "1                                                    无  博尔塔拉·赛里木湖   \n",
       "2    https://f.video.weibocdn.com/o0/ZJkVnEb8lx088A...          无   \n",
       "3                                                    无          无   \n",
       "4                                                    无          无   \n",
       "..                                                 ...        ...   \n",
       "303                                                  无          无   \n",
       "304                                                  无          无   \n",
       "305                                                  无          无   \n",
       "306                                                  无          无   \n",
       "307                                                  无          无   \n",
       "\n",
       "                 发布时间           发布工具    点赞数  转发数   评论数  \n",
       "0    2023-09-15 18:00      iPhone 13   4258  204   433  \n",
       "1    2023-09-14 10:17      iPhone 13   4911  104   347  \n",
       "2    2023-09-14 10:13          微博视频号   6650  326   421  \n",
       "3    2023-09-11 13:11      iPhone 13  14711  107   342  \n",
       "4    2023-09-09 16:16      iPhone 13  14683  108   341  \n",
       "..                ...            ...    ...  ...   ...  \n",
       "303  2022-01-04 19:24              无  36022  107   832  \n",
       "304  2022-01-02 21:41              无  30324  511  1181  \n",
       "305  2022-01-02 13:24              无   5879  310   931  \n",
       "306  2022-01-01 22:30  微博  weibo.com  31299  203  1127  \n",
       "307  2022-01-01 09:00         陈大皮儿超话  33399  114   630  \n",
       "\n",
       "[308 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 指定你的文件夹路径\n",
    "directory_path = '/Users/laihuiqian/Documents/weibo_0918/'  # 替换为你的文件夹路径\n",
    "\n",
    "# 获取文件夹中的所有CSV文件\n",
    "files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "# 确保至少有一个CSV文件\n",
    "if not files:\n",
    "    print(\"No CSV files found in the specified directory.\")\n",
    "else:\n",
    "    # 读取第一个CSV文件\n",
    "    first_file_path = os.path.join(directory_path, files[0])\n",
    "    \n",
    "    encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']\n",
    "    df_first = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            df_first = pd.read_csv(first_file_path, encoding=enc)\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    if df_first is not None:\n",
    "        # 在Jupyter Notebook中打印DataFrame\n",
    "        display(df_first)\n",
    "    else:\n",
    "        print(\"Failed to read the CSV file with the provided encodings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ef9b4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>微博id</th>\n",
       "      <th>微博正文</th>\n",
       "      <th>头条文章url</th>\n",
       "      <th>原始图片url</th>\n",
       "      <th>微博视频url</th>\n",
       "      <th>发布位置</th>\n",
       "      <th>发布时间</th>\n",
       "      <th>发布工具</th>\n",
       "      <th>点赞数</th>\n",
       "      <th>转发数</th>\n",
       "      <th>评论数</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Njql4rGWs</td>\n",
       "      <td>身体护理好物分享 姐妹们，身体头皮的护理和我们面部一样重要哦！从头到脚都不能放过！ 最近一个...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/006tjdECly1hhxf7go...</td>\n",
       "      <td>无</td>\n",
       "      <td>无</td>\n",
       "      <td>2023-09-15 18:00</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>4258</td>\n",
       "      <td>204</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NjdSIx7mr</td>\n",
       "      <td>永远自由如风~#今天穿什么##ootd# #微博变美手册# #种草# 博尔塔拉·赛里木湖  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww1.sinaimg.cn/large/006tjdECly1hhvw7vp...</td>\n",
       "      <td>无</td>\n",
       "      <td>博尔塔拉·赛里木湖</td>\n",
       "      <td>2023-09-14 10:17</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>4911</td>\n",
       "      <td>104</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NjdQKvWzj</td>\n",
       "      <td>PIPIs SHARING | 早秋护肤分享秋天的第一份护肤品请查收～#好物分享##好物推荐...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>无</td>\n",
       "      <td>https://f.video.weibocdn.com/o0/ZJkVnEb8lx088A...</td>\n",
       "      <td>无</td>\n",
       "      <td>2023-09-14 10:13</td>\n",
       "      <td>微博视频号</td>\n",
       "      <td>6650</td>\n",
       "      <td>326</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NiMJzE4VZ</td>\n",
       "      <td>小bus也太可爱了吧！#好物推荐##好物分享#  [组图共9张] 原图</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww2.sinaimg.cn/large/006tjdECly1hhsjzmd...</td>\n",
       "      <td>无</td>\n",
       "      <td>无</td>\n",
       "      <td>2023-09-11 13:11</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>14711</td>\n",
       "      <td>107</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Niv5EnIuE</td>\n",
       "      <td>与生命力对话#OOTD##今天穿什么# #微博变美手册# #种草#  [组图共9张] 原图</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/006tjdECly1hhqeg01...</td>\n",
       "      <td>无</td>\n",
       "      <td>无</td>\n",
       "      <td>2023-09-09 16:16</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>14683</td>\n",
       "      <td>108</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>LaejswP9y</td>\n",
       "      <td>#天天不想上班可能不是因为懒#❗️如何保持上班的激情？❗️上班单纯为了生计吗？❗️不想上班的...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>无</td>\n",
       "      <td>https://f.video.weibocdn.com/o0/YlgaIPJJlx07SS...</td>\n",
       "      <td>无</td>\n",
       "      <td>2022-01-11 09:22</td>\n",
       "      <td>微博视频号</td>\n",
       "      <td>5033</td>\n",
       "      <td>1156</td>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>L9QaVq586</td>\n",
       "      <td>#带着微博去旅行# 为期将近10天的旅行结束了🔚开心 遇到了老朋友 ，认识了新朋友…努力工作...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/001lWunQgy1gy6i6mz...</td>\n",
       "      <td>无</td>\n",
       "      <td>三亚·三亚凤凰国际机场</td>\n",
       "      <td>2022-01-08 19:55</td>\n",
       "      <td>iPhone 12 Pro</td>\n",
       "      <td>4970</td>\n",
       "      <td>1069</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>L91KHrjCW</td>\n",
       "      <td>#元旦假期最后一天# 飞龙在天…南山寺的天空犹如神来之笔旧岁辞云去，旦始迎朝阳。善行世间事，...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww1.sinaimg.cn/large/001lWunQgy1gy0bruw...</td>\n",
       "      <td>无</td>\n",
       "      <td>三亚·南山文化旅游区</td>\n",
       "      <td>2022-01-03 11:33</td>\n",
       "      <td>iPhone 12 Pro</td>\n",
       "      <td>4608</td>\n",
       "      <td>1197</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>L8WH2iHTj</td>\n",
       "      <td>#我的潮流关键词#  随拍日常… 三亚  显示地图 [组图共10张] 原图</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww1.sinaimg.cn/large/001lWunQgy1gxzpg9j...</td>\n",
       "      <td>无</td>\n",
       "      <td>三亚</td>\n",
       "      <td>2022-01-02 22:40</td>\n",
       "      <td>iPhone 12 Pro</td>\n",
       "      <td>4840</td>\n",
       "      <td>1097</td>\n",
       "      <td>1147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>L8MqwDJJm</td>\n",
       "      <td>#2022第一天##无数追梦人还在奋斗奉献#平安 健康 和开心 2022 真心的新年愿望就是...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://wx3.sinaimg.cn/large/001lWunQgy1gxyg3p...</td>\n",
       "      <td>无</td>\n",
       "      <td>三亚</td>\n",
       "      <td>2022-01-01 20:32</td>\n",
       "      <td>iPhone 12 mini</td>\n",
       "      <td>4223</td>\n",
       "      <td>1015</td>\n",
       "      <td>1082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1432 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           微博id                                               微博正文 头条文章url  \\\n",
       "0     Njql4rGWs  身体护理好物分享 姐妹们，身体头皮的护理和我们面部一样重要哦！从头到脚都不能放过！ 最近一个...     NaN   \n",
       "1     NjdSIx7mr  永远自由如风~#今天穿什么##ootd# #微博变美手册# #种草# 博尔塔拉·赛里木湖  ...     NaN   \n",
       "2     NjdQKvWzj  PIPIs SHARING | 早秋护肤分享秋天的第一份护肤品请查收～#好物分享##好物推荐...     NaN   \n",
       "3     NiMJzE4VZ               小bus也太可爱了吧！#好物推荐##好物分享#  [组图共9张] 原图      NaN   \n",
       "4     Niv5EnIuE     与生命力对话#OOTD##今天穿什么# #微博变美手册# #种草#  [组图共9张] 原图      NaN   \n",
       "...         ...                                                ...     ...   \n",
       "1427  LaejswP9y  #天天不想上班可能不是因为懒#❗️如何保持上班的激情？❗️上班单纯为了生计吗？❗️不想上班的...     NaN   \n",
       "1428  L9QaVq586  #带着微博去旅行# 为期将近10天的旅行结束了🔚开心 遇到了老朋友 ，认识了新朋友…努力工作...     NaN   \n",
       "1429  L91KHrjCW  #元旦假期最后一天# 飞龙在天…南山寺的天空犹如神来之笔旧岁辞云去，旦始迎朝阳。善行世间事，...     NaN   \n",
       "1430  L8WH2iHTj             #我的潮流关键词#  随拍日常… 三亚  显示地图 [组图共10张] 原图      NaN   \n",
       "1431  L8MqwDJJm  #2022第一天##无数追梦人还在奋斗奉献#平安 健康 和开心 2022 真心的新年愿望就是...     NaN   \n",
       "\n",
       "                                                原始图片url  \\\n",
       "0     http://ww4.sinaimg.cn/large/006tjdECly1hhxf7go...   \n",
       "1     http://ww1.sinaimg.cn/large/006tjdECly1hhvw7vp...   \n",
       "2                                                     无   \n",
       "3     http://ww2.sinaimg.cn/large/006tjdECly1hhsjzmd...   \n",
       "4     http://ww4.sinaimg.cn/large/006tjdECly1hhqeg01...   \n",
       "...                                                 ...   \n",
       "1427                                                  无   \n",
       "1428  http://ww4.sinaimg.cn/large/001lWunQgy1gy6i6mz...   \n",
       "1429  http://ww1.sinaimg.cn/large/001lWunQgy1gy0bruw...   \n",
       "1430  http://ww1.sinaimg.cn/large/001lWunQgy1gxzpg9j...   \n",
       "1431  https://wx3.sinaimg.cn/large/001lWunQgy1gxyg3p...   \n",
       "\n",
       "                                                微博视频url         发布位置  \\\n",
       "0                                                     无            无   \n",
       "1                                                     无    博尔塔拉·赛里木湖   \n",
       "2     https://f.video.weibocdn.com/o0/ZJkVnEb8lx088A...            无   \n",
       "3                                                     无            无   \n",
       "4                                                     无            无   \n",
       "...                                                 ...          ...   \n",
       "1427  https://f.video.weibocdn.com/o0/YlgaIPJJlx07SS...            无   \n",
       "1428                                                  无  三亚·三亚凤凰国际机场   \n",
       "1429                                                  无   三亚·南山文化旅游区   \n",
       "1430                                                  无           三亚   \n",
       "1431                                                  无           三亚   \n",
       "\n",
       "                  发布时间            发布工具    点赞数   转发数   评论数  \n",
       "0     2023-09-15 18:00       iPhone 13   4258   204   433  \n",
       "1     2023-09-14 10:17       iPhone 13   4911   104   347  \n",
       "2     2023-09-14 10:13           微博视频号   6650   326   421  \n",
       "3     2023-09-11 13:11       iPhone 13  14711   107   342  \n",
       "4     2023-09-09 16:16       iPhone 13  14683   108   341  \n",
       "...                ...             ...    ...   ...   ...  \n",
       "1427  2022-01-11 09:22           微博视频号   5033  1156  1177  \n",
       "1428  2022-01-08 19:55   iPhone 12 Pro   4970  1069  1142  \n",
       "1429  2022-01-03 11:33   iPhone 12 Pro   4608  1197  1225  \n",
       "1430  2022-01-02 22:40   iPhone 12 Pro   4840  1097  1147  \n",
       "1431  2022-01-01 20:32  iPhone 12 mini   4223  1015  1082  \n",
       "\n",
       "[1432 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the displayed DataFrame look correct? (yes/no): yes\n",
      "DataFrame saved as total1-4.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 指定你的文件夹路径\n",
    "directory_path = '/Users/laihuiqian/Documents/weibo_0918/'  # 替换为你的文件夹路径\n",
    "\n",
    "# 获取文件夹中的所有CSV文件\n",
    "files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "# 确保至少有一个CSV文件\n",
    "if not files:\n",
    "    print(\"No CSV files found in the specified directory.\")\n",
    "else:\n",
    "    encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']\n",
    "    dfs = []\n",
    "\n",
    "    # 尝试使用多种编码来读取每个CSV文件\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        df = None\n",
    "        for enc in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=enc)\n",
    "                dfs.append(df)\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "\n",
    "    # 合并所有的DataFrames\n",
    "    if dfs:\n",
    "        df_total = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # 在Jupyter Notebook中显示合并后的DataFrame\n",
    "        display(df_total)\n",
    "\n",
    "        # 询问用户是否保存为CSV\n",
    "        save = input(\"Does the displayed DataFrame look correct? (yes/no): \")\n",
    "        if save.lower() == 'yes':\n",
    "            df_total.to_csv(os.path.join(directory_path, 'total1-4.csv'), index=False, encoding='utf-8')\n",
    "            print(\"DataFrame saved as total1-4.csv.\")\n",
    "    else:\n",
    "        print(\"Failed to read the CSV files with the provided encodings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2778483f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>微博id</th>\n",
       "      <th>微博正文</th>\n",
       "      <th>头条文章url</th>\n",
       "      <th>原始图片url</th>\n",
       "      <th>微博视频url</th>\n",
       "      <th>发布位置</th>\n",
       "      <th>发布时间</th>\n",
       "      <th>发布工具</th>\n",
       "      <th>点赞数</th>\n",
       "      <th>转发数</th>\n",
       "      <th>评论数</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Njql4rGWs</td>\n",
       "      <td>身体护理好物分享 姐妹们，身体头皮的护理和我们面部一样重要哦！从头到脚都不能放过！ 最近一个...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/006tjdECly1hhxf7go...</td>\n",
       "      <td>无</td>\n",
       "      <td>无</td>\n",
       "      <td>2023-09-15 18:00</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>4258</td>\n",
       "      <td>204</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NjdSIx7mr</td>\n",
       "      <td>永远自由如风~#今天穿什么##ootd# #微博变美手册# #种草# 博尔塔拉·赛里木湖  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww1.sinaimg.cn/large/006tjdECly1hhvw7vp...</td>\n",
       "      <td>无</td>\n",
       "      <td>博尔塔拉·赛里木湖</td>\n",
       "      <td>2023-09-14 10:17</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>4911</td>\n",
       "      <td>104</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NjdQKvWzj</td>\n",
       "      <td>PIPIs SHARING | 早秋护肤分享秋天的第一份护肤品请查收～#好物分享##好物推荐...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>无</td>\n",
       "      <td>https://f.video.weibocdn.com/o0/ZJkVnEb8lx088A...</td>\n",
       "      <td>无</td>\n",
       "      <td>2023-09-14 10:13</td>\n",
       "      <td>微博视频号</td>\n",
       "      <td>6650</td>\n",
       "      <td>326</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NiMJzE4VZ</td>\n",
       "      <td>小bus也太可爱了吧！#好物推荐##好物分享#  [组图共9张] 原图</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww2.sinaimg.cn/large/006tjdECly1hhsjzmd...</td>\n",
       "      <td>无</td>\n",
       "      <td>无</td>\n",
       "      <td>2023-09-11 13:11</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>14711</td>\n",
       "      <td>107</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Niv5EnIuE</td>\n",
       "      <td>与生命力对话#OOTD##今天穿什么# #微博变美手册# #种草#  [组图共9张] 原图</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/006tjdECly1hhqeg01...</td>\n",
       "      <td>无</td>\n",
       "      <td>无</td>\n",
       "      <td>2023-09-09 16:16</td>\n",
       "      <td>iPhone 13</td>\n",
       "      <td>14683</td>\n",
       "      <td>108</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>LaejswP9y</td>\n",
       "      <td>#天天不想上班可能不是因为懒#❗️如何保持上班的激情？❗️上班单纯为了生计吗？❗️不想上班的...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>无</td>\n",
       "      <td>https://f.video.weibocdn.com/o0/YlgaIPJJlx07SS...</td>\n",
       "      <td>无</td>\n",
       "      <td>2022-01-11 09:22</td>\n",
       "      <td>微博视频号</td>\n",
       "      <td>5033</td>\n",
       "      <td>1156</td>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>L9QaVq586</td>\n",
       "      <td>#带着微博去旅行# 为期将近10天的旅行结束了🔚开心 遇到了老朋友 ，认识了新朋友…努力工作...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww4.sinaimg.cn/large/001lWunQgy1gy6i6mz...</td>\n",
       "      <td>无</td>\n",
       "      <td>三亚·三亚凤凰国际机场</td>\n",
       "      <td>2022-01-08 19:55</td>\n",
       "      <td>iPhone 12 Pro</td>\n",
       "      <td>4970</td>\n",
       "      <td>1069</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>L91KHrjCW</td>\n",
       "      <td>#元旦假期最后一天# 飞龙在天…南山寺的天空犹如神来之笔旧岁辞云去，旦始迎朝阳。善行世间事，...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww1.sinaimg.cn/large/001lWunQgy1gy0bruw...</td>\n",
       "      <td>无</td>\n",
       "      <td>三亚·南山文化旅游区</td>\n",
       "      <td>2022-01-03 11:33</td>\n",
       "      <td>iPhone 12 Pro</td>\n",
       "      <td>4608</td>\n",
       "      <td>1197</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862</th>\n",
       "      <td>L8WH2iHTj</td>\n",
       "      <td>#我的潮流关键词#  随拍日常… 三亚  显示地图 [组图共10张] 原图</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ww1.sinaimg.cn/large/001lWunQgy1gxzpg9j...</td>\n",
       "      <td>无</td>\n",
       "      <td>三亚</td>\n",
       "      <td>2022-01-02 22:40</td>\n",
       "      <td>iPhone 12 Pro</td>\n",
       "      <td>4840</td>\n",
       "      <td>1097</td>\n",
       "      <td>1147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863</th>\n",
       "      <td>L8MqwDJJm</td>\n",
       "      <td>#2022第一天##无数追梦人还在奋斗奉献#平安 健康 和开心 2022 真心的新年愿望就是...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://wx3.sinaimg.cn/large/001lWunQgy1gxyg3p...</td>\n",
       "      <td>无</td>\n",
       "      <td>三亚</td>\n",
       "      <td>2022-01-01 20:32</td>\n",
       "      <td>iPhone 12 mini</td>\n",
       "      <td>4223</td>\n",
       "      <td>1015</td>\n",
       "      <td>1082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2864 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           微博id                                               微博正文  头条文章url  \\\n",
       "0     Njql4rGWs  身体护理好物分享 姐妹们，身体头皮的护理和我们面部一样重要哦！从头到脚都不能放过！ 最近一个...      NaN   \n",
       "1     NjdSIx7mr  永远自由如风~#今天穿什么##ootd# #微博变美手册# #种草# 博尔塔拉·赛里木湖  ...      NaN   \n",
       "2     NjdQKvWzj  PIPIs SHARING | 早秋护肤分享秋天的第一份护肤品请查收～#好物分享##好物推荐...      NaN   \n",
       "3     NiMJzE4VZ               小bus也太可爱了吧！#好物推荐##好物分享#  [组图共9张] 原图       NaN   \n",
       "4     Niv5EnIuE     与生命力对话#OOTD##今天穿什么# #微博变美手册# #种草#  [组图共9张] 原图       NaN   \n",
       "...         ...                                                ...      ...   \n",
       "2859  LaejswP9y  #天天不想上班可能不是因为懒#❗️如何保持上班的激情？❗️上班单纯为了生计吗？❗️不想上班的...      NaN   \n",
       "2860  L9QaVq586  #带着微博去旅行# 为期将近10天的旅行结束了🔚开心 遇到了老朋友 ，认识了新朋友…努力工作...      NaN   \n",
       "2861  L91KHrjCW  #元旦假期最后一天# 飞龙在天…南山寺的天空犹如神来之笔旧岁辞云去，旦始迎朝阳。善行世间事，...      NaN   \n",
       "2862  L8WH2iHTj             #我的潮流关键词#  随拍日常… 三亚  显示地图 [组图共10张] 原图       NaN   \n",
       "2863  L8MqwDJJm  #2022第一天##无数追梦人还在奋斗奉献#平安 健康 和开心 2022 真心的新年愿望就是...      NaN   \n",
       "\n",
       "                                                原始图片url  \\\n",
       "0     http://ww4.sinaimg.cn/large/006tjdECly1hhxf7go...   \n",
       "1     http://ww1.sinaimg.cn/large/006tjdECly1hhvw7vp...   \n",
       "2                                                     无   \n",
       "3     http://ww2.sinaimg.cn/large/006tjdECly1hhsjzmd...   \n",
       "4     http://ww4.sinaimg.cn/large/006tjdECly1hhqeg01...   \n",
       "...                                                 ...   \n",
       "2859                                                  无   \n",
       "2860  http://ww4.sinaimg.cn/large/001lWunQgy1gy6i6mz...   \n",
       "2861  http://ww1.sinaimg.cn/large/001lWunQgy1gy0bruw...   \n",
       "2862  http://ww1.sinaimg.cn/large/001lWunQgy1gxzpg9j...   \n",
       "2863  https://wx3.sinaimg.cn/large/001lWunQgy1gxyg3p...   \n",
       "\n",
       "                                                微博视频url         发布位置  \\\n",
       "0                                                     无            无   \n",
       "1                                                     无    博尔塔拉·赛里木湖   \n",
       "2     https://f.video.weibocdn.com/o0/ZJkVnEb8lx088A...            无   \n",
       "3                                                     无            无   \n",
       "4                                                     无            无   \n",
       "...                                                 ...          ...   \n",
       "2859  https://f.video.weibocdn.com/o0/YlgaIPJJlx07SS...            无   \n",
       "2860                                                  无  三亚·三亚凤凰国际机场   \n",
       "2861                                                  无   三亚·南山文化旅游区   \n",
       "2862                                                  无           三亚   \n",
       "2863                                                  无           三亚   \n",
       "\n",
       "                  发布时间            发布工具    点赞数   转发数   评论数  \n",
       "0     2023-09-15 18:00       iPhone 13   4258   204   433  \n",
       "1     2023-09-14 10:17       iPhone 13   4911   104   347  \n",
       "2     2023-09-14 10:13           微博视频号   6650   326   421  \n",
       "3     2023-09-11 13:11       iPhone 13  14711   107   342  \n",
       "4     2023-09-09 16:16       iPhone 13  14683   108   341  \n",
       "...                ...             ...    ...   ...   ...  \n",
       "2859  2022-01-11 09:22           微博视频号   5033  1156  1177  \n",
       "2860  2022-01-08 19:55   iPhone 12 Pro   4970  1069  1142  \n",
       "2861  2022-01-03 11:33   iPhone 12 Pro   4608  1197  1225  \n",
       "2862  2022-01-02 22:40   iPhone 12 Pro   4840  1097  1147  \n",
       "2863  2022-01-01 20:32  iPhone 12 mini   4223  1015  1082  \n",
       "\n",
       "[2864 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the displayed DataFrame look correct? (yes/no): yes\n",
      "DataFrame saved as total1-4.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 指定你的文件夹路径\n",
    "directory_path = '/Users/laihuiqian/Documents/weibo_0918/'\n",
    "\n",
    "# 获取文件夹中的所有CSV文件\n",
    "files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "# 确保至少有一个CSV文件\n",
    "if not files:\n",
    "    print(\"No CSV files found in the specified directory.\")\n",
    "else:\n",
    "    encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']\n",
    "    dfs = []\n",
    "\n",
    "    # 尝试使用多种编码来读取每个CSV文件\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        df = None\n",
    "        for enc in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=enc)\n",
    "                dfs.append(df)\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "\n",
    "    # 合并所有的DataFrames\n",
    "    if dfs:\n",
    "        df_total = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # 在Jupyter Notebook中显示合并后的DataFrame\n",
    "        display(df_total)\n",
    "\n",
    "        # 询问用户是否保存为Excel文件\n",
    "        save = input(\"Does the displayed DataFrame look correct? (yes/no): \")\n",
    "        if save.lower() == 'yes':\n",
    "            df_total.to_excel(os.path.join(directory_path, 'total1-4.xlsx'), index=False, engine='openpyxl')\n",
    "            print(\"DataFrame saved as total1-4.xlsx.\")\n",
    "    else:\n",
    "        print(\"Failed to read the CSV files with the provided encodings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dc36e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/tx/327gmf1d3vd2fzt8k0972vz00000gn/T/jieba.cache\n",
      "Loading model cost 0.634 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "INFO:lda:n_documents: 2864\n",
      "INFO:lda:vocab_size: 1834\n",
      "INFO:lda:n_words: 74820\n",
      "INFO:lda:n_topics: 15\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -815870\n",
      "INFO:lda:<10> log likelihood: -564061\n",
      "INFO:lda:<20> log likelihood: -538229\n",
      "INFO:lda:<30> log likelihood: -528372\n",
      "INFO:lda:<40> log likelihood: -523706\n",
      "INFO:lda:<50> log likelihood: -519468\n",
      "INFO:lda:<60> log likelihood: -517493\n",
      "INFO:lda:<70> log likelihood: -515754\n",
      "INFO:lda:<80> log likelihood: -513831\n",
      "INFO:lda:<90> log likelihood: -512958\n",
      "INFO:lda:<99> log likelihood: -511896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model done\n",
      "Topic #0:\n",
      "京东 马锐 链接 网页 直播 00 11 一起 不见不散 健康\n",
      "Topic #1:\n",
      "分享 好物 视频 微博 开箱 快乐 你们 推荐 zjzj 彩妆\n",
      "Topic #2:\n",
      "妆容 今日 时尚 美妆 氛围 微博 风格 搭配 look 整个\n",
      "Topic #3:\n",
      "yoo 周小仙 日常 视频 微博 po 小仙 vlog 你们 plog\n",
      "Topic #4:\n",
      "视频 微博 张进 zjzj vlog 妆容 化妆 今天 大家 如何\n",
      "Topic #5:\n",
      "防晒 真的 可以 护肤 喜欢 感觉 还有 头发 保湿 一个\n",
      "Topic #6:\n",
      "马锐 微博 超话 视频 明星 笔记 化妆师 显示 一个 地图\n",
      "Topic #7:\n",
      "自己 我们 一个 没有 努力 真的 希望 可以 生活 就是\n",
      "Topic #8:\n",
      "皮肤 护肤 状态 自己 一定 不要 敏感 马锐 肌肤 所以\n",
      "Topic #9:\n",
      "视频 微博 皮儿 陈大 分享 pipis sharing 好物 双十 护肤\n",
      "Topic #10:\n",
      "超话 互动 大皮 粉丝 可以 近期 各位 生活 上榜 分享\n",
      "Topic #11:\n",
      "新年 氛围 礼盒 完美 计划 单品 心动 清新 限定 拿捏\n",
      "Topic #12:\n",
      "张进 热巴 化妆 迪丽 妆发 底妆 摄影 粉底液 宋佳 进发\n",
      "Topic #13:\n",
      "精华 皮肤 修护 肌肤 抗老 质地 真的 熬夜 面霜 雅诗兰黛\n",
      "Topic #14:\n",
      "一起 旅行 体验 免税 三亚 可以 味道 这次 国际 cdf\n",
      "\n",
      "Results saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import lda\n",
    "\n",
    "# 1. 读取数据\n",
    "df = pd.read_excel(\"/Users/laihuiqian/Documents/weibo_0918/total1-4.xlsx\")\n",
    "# print(df)\n",
    "texts = df['微博正文'].astype(str)  # 假设'B'列是微博正文\n",
    "\n",
    "# 2. 使用jieba进行中文分词\n",
    "texts_cut = [\" \".join(jieba.cut(text)) for text in texts]\n",
    "\n",
    "# 3. 文本向量化\n",
    "n_features = 5000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, stop_words='english', max_df=0.5, min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(texts_cut)\n",
    "\n",
    "# 4. LDA模型训练\n",
    "n_topics = 15\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=100, random_state=1)\n",
    "model.fit(tf)\n",
    "print('Model done')\n",
    "\n",
    "# 5. 打印每个主题的关键词\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# 6. 为每个文档分配主题\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = [topics.argmax() for topics in doc_topic]\n",
    "\n",
    "# 7. 保存结果\n",
    "df.to_excel('/Users/laihuiqian/Documents/weibo_0918/total1-4_with_topics.xlsx', index=False)\n",
    "print('Results saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01d0c7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 2864\n",
      "INFO:lda:vocab_size: 1799\n",
      "INFO:lda:n_words: 73526\n",
      "INFO:lda:n_topics: 15\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -831346\n",
      "INFO:lda:<10> log likelihood: -548199\n",
      "INFO:lda:<20> log likelihood: -524687\n",
      "INFO:lda:<30> log likelihood: -516162\n",
      "INFO:lda:<40> log likelihood: -511796\n",
      "INFO:lda:<50> log likelihood: -508639\n",
      "INFO:lda:<60> log likelihood: -506839\n",
      "INFO:lda:<70> log likelihood: -505462\n",
      "INFO:lda:<80> log likelihood: -503499\n",
      "INFO:lda:<90> log likelihood: -502360\n",
      "INFO:lda:<99> log likelihood: -501703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model done\n",
      "Topic #0:\n",
      "自己 一个 喜欢 没有 生活 时候 就是 时间 什么 一样\n",
      "Topic #1:\n",
      "超话 互动 大皮 粉丝 可以 近期 各位 幸运 生活 上榜\n",
      "Topic #2:\n",
      "皮肤 真的 肌肤 防晒 精华 护肤 成分 熬夜 敏感 就是\n",
      "Topic #3:\n",
      "分享 视频 微博 皮儿 陈大 好物 pipis sharing 推荐 今天\n",
      "Topic #4:\n",
      "自己 我们 可以 一个 健康 选择 努力 真的 就是 现在\n",
      "Topic #5:\n",
      "周小仙 yoo 视频 日常 微博 小仙 po 开箱 你们 分享\n",
      "Topic #6:\n",
      "新年 氛围 妆容 今天 ootd 今日 完美 夏天 口红 夏日\n",
      "Topic #7:\n",
      "微博 视频 vlog 大家 你们 快乐 zjzj 张进 一起 什么\n",
      "Topic #8:\n",
      "马锐 视频 微博 超话 明星 笔记 直播 化妆师 双十 职场\n",
      "Topic #9:\n",
      "京东 修护 精华 护肤 还有 链接 质地 网页 系列 面霜\n",
      "Topic #10:\n",
      "一起 旅行 体验 大家 可以 免税 开学 美丽 我们 你们\n",
      "Topic #11:\n",
      "张进 热巴 迪丽 化妆 妆发 显示 地图 摄影 造型 宋佳\n",
      "Topic #12:\n",
      "视频 微博 zjzj 张进 美妆 好物 化妆 彩妆 腮红 抽奖\n",
      "Topic #13:\n",
      "真的 礼盒 还有 这次 味道 高级 感觉 喜欢 玫瑰 头发\n",
      "Topic #14:\n",
      "不要 自己 没有 因为 一定 一个 可以 如果 我们 很多\n",
      "\n",
      "Results saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import lda\n",
    "\n",
    "# 1. 读取数据\n",
    "df = pd.read_excel(\"/Users/laihuiqian/Documents/weibo_0918/total1-4.xlsx\")\n",
    "# print(df)\n",
    "texts = df['微博正文'].astype(str)  # 假设'B'列是微博正文\n",
    "\n",
    "# 2. 使用jieba进行中文分词\n",
    "# texts_cut = [\" \".join(jieba.cut(text)) for text in texts]\n",
    "\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 使用jieba进行分词\n",
    "    words = jieba.cut(text)\n",
    "    \n",
    "    # 去除标点和特殊字符\n",
    "    words = [re.sub(r'[^a-zA-Z\\u4e00-\\u9fff]', '', word) for word in words]\n",
    "    \n",
    "    # 去除特定无用词\n",
    "    useless_terms = [\"显示地图\", \"原图\", \"\\[组图共\\d张\\]\"]\n",
    "    words = [word for word in words if word not in useless_terms and len(word) > 0]\n",
    "    \n",
    "    # 去除停用词 (这里需要一个中文的停用词列表)\n",
    "    # stopwords = set(your_stopwords_list)\n",
    "    # words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# 应用预处理函数\n",
    "texts_cut = [preprocess_text(text) for text in df['微博正文']]\n",
    "\n",
    "\n",
    "# 3. 文本向量化\n",
    "n_features = 5000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, stop_words='english', max_df=0.5, min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(texts_cut)\n",
    "\n",
    "# 4. LDA模型训练\n",
    "n_topics = 15\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=100, random_state=1)\n",
    "model.fit(tf)\n",
    "print('Model done')\n",
    "\n",
    "# 5. 打印每个主题的关键词\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# 6. 为每个文档分配主题\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = [topics.argmax() for topics in doc_topic]\n",
    "\n",
    "# 7. 保存结果\n",
    "df.to_excel('/Users/laihuiqian/Documents/weibo_0918/total1-4_with_topics.xlsx', index=False)\n",
    "print('Results saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcc6f1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 2864\n",
      "INFO:lda:vocab_size: 1799\n",
      "INFO:lda:n_words: 73526\n",
      "INFO:lda:n_topics: 15\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -831346\n",
      "INFO:lda:<10> log likelihood: -548199\n",
      "INFO:lda:<20> log likelihood: -524687\n",
      "INFO:lda:<30> log likelihood: -516162\n",
      "INFO:lda:<40> log likelihood: -511796\n",
      "INFO:lda:<50> log likelihood: -508639\n",
      "INFO:lda:<60> log likelihood: -506839\n",
      "INFO:lda:<70> log likelihood: -505462\n",
      "INFO:lda:<80> log likelihood: -503499\n",
      "INFO:lda:<90> log likelihood: -502360\n",
      "INFO:lda:<99> log likelihood: -501703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model done\n",
      "Topic #0:\n",
      "自己 一个 喜欢 没有 生活 时候 就是 时间 什么 一样\n",
      "Topic #1:\n",
      "超话 互动 大皮 粉丝 可以 近期 各位 幸运 生活 上榜\n",
      "Topic #2:\n",
      "皮肤 真的 肌肤 防晒 精华 护肤 成分 熬夜 敏感 就是\n",
      "Topic #3:\n",
      "分享 视频 微博 皮儿 陈大 好物 pipis sharing 推荐 今天\n",
      "Topic #4:\n",
      "自己 我们 可以 一个 健康 选择 努力 真的 就是 现在\n",
      "Topic #5:\n",
      "周小仙 yoo 视频 日常 微博 小仙 po 开箱 你们 分享\n",
      "Topic #6:\n",
      "新年 氛围 妆容 今天 ootd 今日 完美 夏天 口红 夏日\n",
      "Topic #7:\n",
      "微博 视频 vlog 大家 你们 快乐 zjzj 张进 一起 什么\n",
      "Topic #8:\n",
      "马锐 视频 微博 超话 明星 笔记 直播 化妆师 双十 职场\n",
      "Topic #9:\n",
      "京东 修护 精华 护肤 还有 链接 质地 网页 系列 面霜\n",
      "Topic #10:\n",
      "一起 旅行 体验 大家 可以 免税 开学 美丽 我们 你们\n",
      "Topic #11:\n",
      "张进 热巴 迪丽 化妆 妆发 显示 地图 摄影 造型 宋佳\n",
      "Topic #12:\n",
      "视频 微博 zjzj 张进 美妆 好物 化妆 彩妆 腮红 抽奖\n",
      "Topic #13:\n",
      "真的 礼盒 还有 这次 味道 高级 感觉 喜欢 玫瑰 头发\n",
      "Topic #14:\n",
      "不要 自己 没有 因为 一定 一个 可以 如果 我们 很多\n",
      "\n",
      "Results saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import lda\n",
    "\n",
    "# 1. 读取数据\n",
    "df = pd.read_excel(\"/Users/laihuiqian/Documents/weibo_0918/total1-4.xlsx\")\n",
    "# print(df)\n",
    "texts = df['微博正文'].astype(str)  # 假设'B'列是微博正文\n",
    "\n",
    "# 2. 使用jieba进行中文分词\n",
    "# texts_cut = [\" \".join(jieba.cut(text)) for text in texts]\n",
    "\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 使用jieba进行分词\n",
    "    words = jieba.cut(text)\n",
    "    \n",
    "    # 去除标点和特殊字符\n",
    "    words = [re.sub(r'[^a-zA-Z\\u4e00-\\u9fff]', '', word) for word in words]\n",
    "    \n",
    "    # 去除特定无用词\n",
    "    useless_terms = [\"显示地图\", \"原图\", \"\\[组图共\\d张\\]\"]\n",
    "    words = [word for word in words if word not in useless_terms and len(word) > 0]\n",
    "    \n",
    "    # 去除停用词 (这里需要一个中文的停用词列表)\n",
    "    # stopwords = set(your_stopwords_list)\n",
    "    # words = [word for word in words if word not in stopwords]\n",
    "    with open('baidu_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "        stopwords = set([line.strip() for line in f])\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# 应用预处理函数\n",
    "texts_cut = [preprocess_text(text) for text in df['微博正文']]\n",
    "\n",
    "\n",
    "# 3. 文本向量化\n",
    "n_features = 5000\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features, stop_words='english', max_df=0.5, min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(texts_cut)\n",
    "\n",
    "# 4. LDA模型训练\n",
    "n_topics = 15\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=100, random_state=1)\n",
    "model.fit(tf)\n",
    "print('Model done')\n",
    "\n",
    "# 5. 打印每个主题的关键词\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# 6. 为每个文档分配主题\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = [topics.argmax() for topics in doc_topic]\n",
    "\n",
    "# 7. 保存结果\n",
    "df.to_excel('/Users/laihuiqian/Documents/weibo_0918/total1-4_with_topics_2.xlsx', index=False)\n",
    "print('Results saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5709777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "INFO:lda:n_documents: 2864\n",
      "INFO:lda:vocab_size: 1724\n",
      "INFO:lda:n_words: 66408\n",
      "INFO:lda:n_topics: 17\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -760839\n",
      "INFO:lda:<10> log likelihood: -495089\n",
      "INFO:lda:<20> log likelihood: -472508\n",
      "INFO:lda:<30> log likelihood: -464254\n",
      "INFO:lda:<40> log likelihood: -459505\n",
      "INFO:lda:<50> log likelihood: -456637\n",
      "INFO:lda:<60> log likelihood: -454355\n",
      "INFO:lda:<70> log likelihood: -452068\n",
      "INFO:lda:<80> log likelihood: -451091\n",
      "INFO:lda:<90> log likelihood: -450118\n",
      "INFO:lda:<99> log likelihood: -449195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model done\n",
      "Topic #0:\n",
      "vlog 微博 日常 生活 视频 显示 地图 plog 快乐 日记 zjzj 博主 一天 终于 杭州 美出 春天 张进 碎片 po\n",
      "Topic #1:\n",
      "视频 微博 分享 皮儿 陈大 好物 pipis sharing 双十 推荐 大会 护肤 生活 身体 一个 马锐 一种 姐妹 真的 今天\n",
      "Topic #2:\n",
      "一个 努力 希望 没有 不要 生活 工作 时间 选择 遇到 真的 喜欢 成为 电影 人生 开心 可能 马锐 超话 永远\n",
      "Topic #3:\n",
      "氛围 高级 腮红 真的 口红 搭配 拿捏 整个 设计 眼影 单品 get 一点 夏天 礼盒 宝子们 限定 简直 好看 分享\n",
      "Topic #4:\n",
      "周小仙 yoo 视频 微博 日常 今日 小仙 po 今天 ootd 妆容 攻略 开春 喜欢 宠粉 好颜 记得 一下 新疆 辣妹\n",
      "Topic #5:\n",
      "新年 完美 健康 计划 快乐 礼盒 2022 礼物 2023 一年 虎年 过年 祝福 品牌 加油 七夕 幸福 一起 520 可爱\n",
      "Topic #6:\n",
      "微博 明星 马锐 视频 化妆师 笔记 到底 头发 美妆 选择 美妆红人 百大 头皮 抽奖 超话 学习 微博美妆 准备 修容 适合\n",
      "Topic #7:\n",
      "超话 大皮 互动 粉丝 近期 幸运 上榜 生活 分享 时间 发布 日常 加入 私信 自拍 领取 兑奖 没有 眼熟 宝子\n",
      "Topic #8:\n",
      "妆容 微博 视频 化妆 张进 zjzj 造型 今日 今天 粉底液 美妆 心动 打造 起来 丝绒 眼妆 底妆 立体 一起 奢光\n",
      "Topic #9:\n",
      "没有 一定 朋友 一个 真的 现在 品牌 觉得 特别 最近 运动 尝试 国货 中国 一起 非常 最后 知道 看到 其实\n",
      "Topic #10:\n",
      "张进 热巴 迪丽 化妆 妆发 摄影 宋佳 进发 封面 代言人 年度 品牌 彩妆 大片 雪亮 李志辉 全球 春日 张婧仪 直播\n",
      "Topic #11:\n",
      "修护 真的 防晒 精华 质地 熬夜 面霜 效果 下来 皮肤 抗老 现在 面膜 整个 直接 细腻 保湿 一个 喜欢 吸收\n",
      "Topic #12:\n",
      "一起 夏日 喜欢 味道 玫瑰 华为 自然 真的 风格 感受 香水 美好 ootd 清新 感觉 时尚 look 不同 香氛 解锁\n",
      "Topic #13:\n",
      "视频 分享 微博 好物 开箱 zjzj 张进 yoo 春夏 周小仙 抽奖 护肤 一起 详情 快乐 种草 今天 夏天 看看 夏日\n",
      "Topic #14:\n",
      "马锐 视频 超话 微博 直播 职场 00 不见不散 开学 变美 卫视 简单 今天 21 20 今晚 焦虑 天津 10 哈哈哈\n",
      "Topic #15:\n",
      "皮肤 护肤 肌肤 精华 状态 敏感 抗老 成分 体验 坚持 里面 起来 产品 美白 护肤品 一直 健康 容易 提升 透亮\n",
      "Topic #16:\n",
      "京东 链接 一起 网页 超级 免税 生活 开启 三亚 活动 11 美妆 红人节 惊喜 天猫 cdf 超多 国际 618 享受\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 加载停用词\n",
    "def load_stopwords():\n",
    "    # 这里使用了我之前提供的停用词链接，你可以替换为自己的链接或本地路径\n",
    "    url = \"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\"\n",
    "    return pd.read_csv(url, header=None, sep=\"\\t\", quoting=3, names=['stopword'], encoding='utf-8').stopword.tolist()\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# 分词并去除停用词\n",
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join([word for word in jieba.cut(mytext) if word not in stopwords])\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_excel(\"/Users/laihuiqian/Documents/weibo_0918/total1-4.xlsx\")\n",
    "df[\"content_cutted\"] = df['微博正文'].astype(str).apply(chinese_word_cut)\n",
    "\n",
    "n_features = 5000\n",
    "\n",
    "tf_vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords,\n",
    "                                max_df=0.5,\n",
    "                                min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(df.content_cutted)\n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA建模\n",
    "model = lda.LDA(n_topics=17, n_iter=100, random_state=1)\n",
    "model.fit(tf)\n",
    "print('model done')\n",
    "\n",
    "# 打印每个主题的前20个关键词\n",
    "def print_top_words(model, feature_names, n_top_words=20):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "print_top_words(model, vocab)\n",
    "\n",
    "# 将文档-主题分布保存到Excel\n",
    "doc_topic = model.doc_topic_\n",
    "df['topic'] = doc_topic.argmax(axis=1)\n",
    "df.to_excel('/Users/laihuiqian/Documents/weibo_0918/total1-4_topics.xlsx', sheet_name='Sheet1')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68c0ccae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:collected 13222 word types from a corpus of 193660 raw words and 2864 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 13222 unique words (100.00% of original 13222, drops 0)', 'datetime': '2023-09-18T14:34:11.377585', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 193660 word corpus (100.00% of original 193660, drops 0)', 'datetime': '2023-09-18T14:34:11.378314', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 13222 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 19 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 139445.79549075436 word corpus (72.0%% of prior 193660)', 'datetime': '2023-09-18T14:34:11.443229', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 13222 words and 100 dimensions: 17188600 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-09-18T14:34:11.551766', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 4 workers on 13222 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-09-18T14:34:11.552503', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 193660 raw words (139530 effective words) took 0.1s, 2276904 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 193660 raw words (139424 effective words) took 0.1s, 2398712 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 193660 raw words (139510 effective words) took 0.1s, 2406548 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 193660 raw words (139399 effective words) took 0.1s, 2434729 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 193660 raw words (139517 effective words) took 0.1s, 2307762 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 968300 raw words (697380 effective words) took 0.3s, 2241517 effective words/s', 'datetime': '2023-09-18T14:34:11.864024', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=13222, vector_size=100, alpha=0.025>', 'datetime': '2023-09-18T14:34:11.864442', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 加载停用词\n",
    "def load_stopwords():\n",
    "    url = \"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\"\n",
    "    return pd.read_csv(url, header=None, sep=\"\\t\", quoting=3, names=['stopword'], encoding='utf-8').stopword.tolist()\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# 分词并去除停用词\n",
    "def chinese_word_cut(mytext):\n",
    "    return [word for word in jieba.cut(mytext) if word not in stopwords]\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_excel(\"/Users/laihuiqian/Documents/weibo_0918/total1-4.xlsx\")\n",
    "df[\"content_cutted\"] = df['微博正文'].astype(str).apply(chinese_word_cut)\n",
    "\n",
    "# 使用Word2Vec生成嵌入向量\n",
    "model_w2v = Word2Vec(df[\"content_cutted\"], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "# 将微博正文转换为向量\n",
    "def document_vector(doc):\n",
    "    return np.mean([model_w2v.wv[word] for word in doc if word in model_w2v.wv.key_to_index], axis=0)\n",
    "\n",
    "df['vector'] = df[\"content_cutted\"].apply(document_vector)\n",
    "\n",
    "\n",
    "# 使用KMeans进行聚类\n",
    "num_clusters = 17\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=1)\n",
    "df['topic'] = kmeans.fit_predict(list(df['vector']))\n",
    "\n",
    "# 保存到Excel\n",
    "df.to_excel('/Users/laihuiqian/Documents/weibo_0918/total1-4_topics_w2v.xlsx', sheet_name='Sheet1')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76fb91a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:collected 13222 word types from a corpus of 193660 raw words and 2864 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 13222 unique words (100.00% of original 13222, drops 0)', 'datetime': '2023-09-18T14:36:06.551803', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 193660 word corpus (100.00% of original 193660, drops 0)', 'datetime': '2023-09-18T14:36:06.552465', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 13222 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 19 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 139445.79549075436 word corpus (72.0%% of prior 193660)', 'datetime': '2023-09-18T14:36:06.618854', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 13222 words and 100 dimensions: 17188600 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-09-18T14:36:06.723475', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 4 workers on 13222 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-09-18T14:36:06.724135', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 193660 raw words (139439 effective words) took 0.1s, 2291894 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 193660 raw words (139441 effective words) took 0.1s, 2225543 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 193660 raw words (139380 effective words) took 0.1s, 2231632 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 193660 raw words (139420 effective words) took 0.1s, 2155766 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 193660 raw words (139419 effective words) took 0.1s, 2197305 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 968300 raw words (697099 effective words) took 0.3s, 2106905 effective words/s', 'datetime': '2023-09-18T14:36:07.055400', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=13222, vector_size=100, alpha=0.025>', 'datetime': '2023-09-18T14:36:07.055829', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:   组图 深海 [ 化妆 共 ] 妆发 张 原图\n",
      "Topic 1: 不见不散   求职 … 王心凌 妆发 抖音 ～   甜\n",
      "Topic 2: 求职 ～ 甜 抖音 不见不散 🌌 发型 高手 找马锐 …\n",
      "Topic 3:   妆发 化妆 深海 组图 [ 赵丽颖 共 ] 摄影\n",
      "Topic 4: ~ 之夜 ～ 快乐 贴 戛纳 直播 高手 录 冰雪\n",
      "Topic 5: 赵丽颖 [ … 妆发   化妆 组图 深海 季 快乐\n",
      "Topic 6: 赵丽颖 妆发 化妆 … 摄影   [ 组图 深海 时装周\n",
      "Topic 7:   [ 组图 深海 化妆 妆发 共 ] 赵丽颖 姜通\n",
      "Topic 8: 直播 … 张进 快乐 ～ 带 不见不散 求职 抖音 季\n",
      "Topic 9: [ 赵丽颖 组图   化妆 深海 妆发 … 季 共\n",
      "Topic 10: 张婧仪 发型 大片 北京 甜 加油 电影 11 高手 生日快乐\n",
      "Topic 11: 张进 … 直播 不见不散 赵丽颖 微博 抖音 求职 ～ 快乐\n",
      "Topic 12: 直播 ～ 快乐 求职 ~ … 抖音 带 录 之夜\n",
      "Topic 13: 妆发   化妆 深海 组图 [ 赵丽颖 摄影 共 ]\n",
      "Topic 14:   妆发   深海 不见不散 … 化妆 [ 组图 王心凌\n",
      "Topic 15: 不见不散 求职 ～ 抖音 … 甜 王心凌 妆发 直播 🌌\n",
      "Topic 16: 赵丽颖 摄影 … 快乐 想见 妆发 TOMFORDBEAUTY 时装周 化妆 甜\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 加载停用词\n",
    "def load_stopwords():\n",
    "    url = \"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\"\n",
    "    return pd.read_csv(url, header=None, sep=\"\\t\", quoting=3, names=['stopword'], encoding='utf-8').stopword.tolist()\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# 分词并去除停用词\n",
    "def chinese_word_cut(mytext):\n",
    "    return [word for word in jieba.cut(mytext) if word not in stopwords]\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_excel(\"/Users/laihuiqian/Documents/weibo_0918/total1-4.xlsx\")\n",
    "df[\"content_cutted\"] = df['微博正文'].astype(str).apply(chinese_word_cut)\n",
    "\n",
    "# 使用Word2Vec生成嵌入向量\n",
    "model_w2v = Word2Vec(sentences=df[\"content_cutted\"], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 将微博正文转换为向量\n",
    "def document_vector(doc):\n",
    "    return np.mean([model_w2v.wv[word] for word in doc if word in model_w2v.wv.key_to_index], axis=0)\n",
    "\n",
    "df['vector'] = df[\"content_cutted\"].apply(document_vector)\n",
    "\n",
    "# 使用KMeans进行聚类\n",
    "num_clusters = 17\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=1)\n",
    "df['topic'] = kmeans.fit_predict(list(df['vector']))\n",
    "\n",
    "# 打印每个主题的描述\n",
    "for i in range(num_clusters):\n",
    "    center = kmeans.cluster_centers_[i]\n",
    "    most_similar = model_w2v.wv.most_similar(positive=[center], topn=10)\n",
    "    words = [word[0] for word in most_similar]\n",
    "    print(f\"Topic {i}: {' '.join(words)}\")\n",
    "\n",
    "# 保存到Excel\n",
    "df.to_excel('/Users/laihuiqian/Documents/weibo_0918/total1-4_topics_w2v.xlsx', sheet_name='Sheet1')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "672f66a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "周小仙 yoo 视频 微博 超话 周密 红人节 开箱 超级 随手\n",
      "Topic #1:\n",
      "组图 美好 美出 最近 快乐 生日快乐 春天 可爱 大会 超话\n",
      "Topic #2:\n",
      "热巴 迪丽 张进 化妆 李志辉 代言人 直播间 直播 妆发 进发\n",
      "Topic #3:\n",
      "马锐 超话 直播 明星 视频 笔记 微博 化妆师 变美 不见不散\n",
      "Topic #4:\n",
      "po 日常 小仙 日记 博主 氛围 plog 感出 随机 喜欢\n",
      "Topic #5:\n",
      "好物 分享 推荐 开箱 最近 购物 收到 详情 抽奖 宝贝\n",
      "Topic #6:\n",
      "妆发 张进 宋佳 品牌 张婧仪 代言人 摄影 组图 全球 优雅\n",
      "Topic #7:\n",
      "zjzj 张进 视频 微博 详情 抽奖 进宝 美妆 化妆 教程\n",
      "Topic #8:\n",
      "皮肤 护肤 精华 真的 肌肤 修护 京东 抗老 一个 状态\n",
      "Topic #9:\n",
      "vlog 快乐 生活 一天 一起 工作 一个 记录 大赛 希望\n",
      "Topic #10:\n",
      "显示 地图 杭州 广州 三亚 北京 国际 法国 潮流 组图\n",
      "Topic #11:\n",
      "新年 完美 计划 开箱 好颜 开春 新年好 攻略 虎年 开运\n",
      "Topic #12:\n",
      "皮儿 陈大 pipis sharing 视频 微博 美出 变美 分享 姐妹\n",
      "Topic #13:\n",
      "妆容 今日 美妆 腮红 时尚 冰雪 复古 motd 今天 底妆\n",
      "Topic #14:\n",
      "ootd 今天 夏日 辣妹 一下 夏天 look 怒放 春天 出街\n",
      "Topic #15:\n",
      "plog 一条 告别 碎片 闪光 寻找 组图 日常 博主 日记\n",
      "Topic #16:\n",
      "化妆 封面 进发 摄影 雪亮 大片 宋佳 李志辉 组图 发型\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# 加载停用词\n",
    "def load_stopwords():\n",
    "    url = \"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\"\n",
    "    return pd.read_csv(url, header=None, sep=\"\\t\", quoting=3, names=['stopword'], encoding='utf-8').stopword.tolist()\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# 分词并去除停用词\n",
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join([word for word in jieba.cut(mytext) if word not in stopwords])\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_excel(\"/Users/laihuiqian/Documents/weibo_0918/total1-4.xlsx\")\n",
    "# df[\"content_cutted\"] = df['微博正文'].astype(str).apply(chinese_word_cut)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 使用jieba进行分词\n",
    "    words = jieba.cut(text)\n",
    "    \n",
    "    # 去除标点和特殊字符\n",
    "    words = [re.sub(r'[^a-zA-Z\\u4e00-\\u9fff]', '', word) for word in words]\n",
    "    \n",
    "    # 去除特定无用词\n",
    "    useless_terms = [\"显示地图\", \"原图\", \"\\[组图共\\d张\\]\"]\n",
    "    words = [word for word in words if word not in useless_terms and len(word) > 0]\n",
    "    \n",
    "    # 去除停用词 (这里需要一个中文的停用词列表)\n",
    "    # stopwords = set(your_stopwords_list)\n",
    "    # words = [word for word in words if word not in stopwords]\n",
    "    with open('baidu_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "        stopwords = set([line.strip() for line in f])\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# 应用预处理函数\n",
    "df[\"content_cutted\"] = [preprocess_text(text) for text in df['微博正文']]\n",
    "\n",
    "# 使用TF-IDF进行向量化\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words=stopwords)\n",
    "tfidf = tfidf_vectorizer.fit_transform(df[\"content_cutted\"])\n",
    "\n",
    "# 使用NMF进行主题建模\n",
    "num_topics = 17\n",
    "nmf = NMF(n_components=num_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "\n",
    "\n",
    "# 打印每个主题的前10个关键词\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "print_top_words(nmf, tfidf_vectorizer.get_feature_names())  # 修改这里\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 将文档-主题分布保存到Excel\n",
    "doc_topic = nmf.transform(tfidf)\n",
    "df['topic'] = doc_topic.argmax(axis=1)\n",
    "df.to_excel('/Users/laihuiqian/Documents/weibo_0918/total1-4_topics_nmf.xlsx', sheet_name='Sheet1')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386586f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
